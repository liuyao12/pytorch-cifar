{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf6nUgErY6Bh"
   },
   "source": [
    "# Cifar10 with PDE\n",
    "\n",
    "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
    "\n",
    "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning only includes bits of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
    "\n",
    "* Code based off of https://github.com/kuangliu/pytorch-cifar, and the [official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) \n",
    "\n",
    "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPeChzrK7iYC"
   },
   "source": [
    "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [notebook](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations) on observable.\n",
    "\n",
    "neural network | heat equation\n",
    ":----:|:-------:\n",
    "input layer | initial condition\n",
    "feed forward | solving the equation\n",
    "hidden layers | solution at intermediate times\n",
    "output layer | solution at final time\n",
    "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
    "weights | coefficients\n",
    "boundary handling (padding) | boundary condition\n",
    "multiple channels | system of (coupled) PDEs\n",
    "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
    "16×16×1×1 kernel | 16×16 matrix of constants\n",
    "\n",
    "\n",
    "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply making it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should theoretically enable the neural net to learn more ways to deform than diffusion and translation (e.g., rotation and scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lmUdhteH5N9s",
    "outputId": "fdb9e1a1-a178-4b65-a778-468c9263b8c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Testing on a random input:\n",
      "INPUT  torch.Size([1, 3, 32, 32])\n",
      "OUTPUT torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "ResNet in PyTorch  https://github.com/kuangliu/pytorch-cifar\n",
    "Reference:\n",
    "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, iterations=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.iterations = iterations\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.shortcut(x) + self.bn2(x2)\n",
    "        x3 = F.relu(x3)\n",
    "        return x3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = F.relu(self.bn2(x2))\n",
    "        x4 = self.conv3(x3)\n",
    "        x4 = self.bn3(x4)\n",
    "        x4 += self.shortcut(x)\n",
    "        x4 = F.relu(x4)\n",
    "        return x4\n",
    "\n",
    "\n",
    "class PDE_Block(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, act=True):\n",
    "        super(PDE_Block, self).__init__()\n",
    "        self.act = act\n",
    "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.convx = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.convy = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x1 += self.twist(x, x1.shape)\n",
    "        x2 = self.bn(x1)\n",
    "        x2 += self.shortcut(x)\n",
    "        if self.act:\n",
    "            x2 = F.relu(x2)\n",
    "        return x2\n",
    "\n",
    "    def twist(self, x, out_shape):\n",
    "        if self.XX is None:\n",
    "            _, _, h, w = list(out_shape)\n",
    "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
    "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
    "        self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3))/2\n",
    "        self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3))/2\n",
    "        return self.XX * self.convx(x) + self.YY * self.convy(x)\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 8\n",
    "        self.num_blocks = num_blocks\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(128 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for idx, stride in enumerate(strides):\n",
    "            # layers.append(block(self.in_channels, channels, stride))\n",
    "            if block == PDE_Block and idx % 3 == 2:\n",
    "                layers.append(block(self.in_channels, channels, stride, False))\n",
    "            else:\n",
    "                layers.append(block(self.in_channels, channels, stride))\n",
    "            self.in_channels = channels * block.expansion\n",
    "                \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "\n",
    "net = ResNet(PDE_Block, [8,8,8,8])\n",
    "epoch = 0 \n",
    "lr = 0.1\n",
    "checkpoint = {'acc': 0, 'epoch': 0}\n",
    "history = [{'acc': 0, 'epoch': 0}]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.to(device)\n",
    "print('Testing on a random input:')\n",
    "test = torch.randn(1,3,32,32).to(device)\n",
    "print('INPUT ', test.shape)\n",
    "print('OUTPUT', net(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ozKUGgKwcruw",
    "outputId": "f35c1191-0bfb-4436-be1f-2f2f5622f6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1RzI4cWWHVlg",
    "outputId": "182f6eb5-1750-463d-a07d-c6dcd15ed0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0 2.621119260787964 [0.26603174209594727, 0.2705247402191162, 0.2586051821708679]\n",
      "100 2.254976987838745 [0.42982226610183716, 0.3049733340740204, 0.2759949564933777]\n",
      "200 2.0854451656341553 [0.3846457302570343, 0.2742742598056793, 0.25108227133750916]\n",
      "300 1.8720414638519287 [0.35999801754951477, 0.24873992800712585, 0.22601842880249023]\n",
      "train loss: 2.154 | acc: 21.532 (10766/50000)\n",
      "test  loss: 2.099 | acc: 27.36  ( 2736/10000) (up by 27.36)\n",
      "Epoch 1\n",
      "0 1.7926530838012695 [0.3287651538848877, 0.050074175000190735, 0.05373675748705864]\n",
      "100 1.8145718574523926 [0.3062690496444702, 0.04554499313235283, 0.05205224081873894]\n",
      "200 1.7525367736816406 [0.27903982996940613, 0.04170724377036095, 0.04629070311784744]\n",
      "300 1.5481927394866943 [0.25506502389907837, 0.03975250944495201, 0.04279080405831337]\n",
      "train loss: 1.716 | acc: 36.148 (18074/50000)\n",
      "test  loss: 1.649 | acc: 39.36  ( 3936/10000) (up by 12.00)\n",
      "Epoch 2\n",
      "0 1.6551792621612549 [0.23592537641525269, 0.03381640836596489, 0.03564313054084778]\n",
      "100 1.6073989868164062 [0.21802988648414612, 0.030590979382395744, 0.03258959949016571]\n",
      "200 1.5457457304000854 [0.2014678716659546, 0.028303569182753563, 0.02991868555545807]\n",
      "300 1.4741215705871582 [0.18351884186267853, 0.028039416298270226, 0.029089810326695442]\n",
      "train loss: 1.542 | acc: 43.556 (21778/50000)\n",
      "test  loss: 1.388 | acc: 49.07  ( 4907/10000) (up by 9.71)\n",
      "Epoch 3\n",
      "0 1.4722875356674194 [0.16891254484653473, 0.02356334961950779, 0.023947082459926605]\n",
      "100 1.408515453338623 [0.15661285817623138, 0.021887624636292458, 0.022573860362172127]\n",
      "200 1.4099278450012207 [0.14899130165576935, 0.02081621252000332, 0.021074403077363968]\n",
      "300 1.2173975706100464 [0.13636514544487, 0.018689770251512527, 0.020667873322963715]\n",
      "train loss: 1.342 | acc: 51.386 (25693/50000)\n",
      "test  loss: 1.165 | acc: 58.26  ( 5826/10000) (up by 9.19)\n",
      "Epoch 4\n",
      "0 1.0967116355895996 [0.12543456256389618, 0.016545912250876427, 0.01609426736831665]\n",
      "100 1.271786093711853 [0.11952362209558487, 0.016045624390244484, 0.015882117673754692]\n",
      "200 1.0910441875457764 [0.107864610850811, 0.014051532372832298, 0.01666884310543537]\n",
      "300 1.0496647357940674 [0.10055993497371674, 0.01362516824156046, 0.01673836074769497]\n",
      "train loss: 1.141 | acc: 59.178 (29589/50000)\n",
      "test  loss: 1.053 | acc: 62.25  ( 6225/10000) (up by 3.99)\n",
      "Epoch 5\n",
      "0 0.9837118983268738 [0.09578729420900345, 0.011447949334979057, 0.011372730135917664]\n",
      "100 1.2326228618621826 [0.09331369400024414, 0.011189975775778294, 0.011843233369290829]\n",
      "200 1.0309128761291504 [0.08509473502635956, 0.010381724685430527, 0.011754694394767284]\n",
      "300 0.9894830584526062 [0.08113653212785721, 0.010403386317193508, 0.011076067574322224]\n",
      "train loss: 0.971 | acc: 65.746 (32873/50000)\n",
      "test  loss: 0.964 | acc: 66.17  ( 6617/10000) (up by 3.92)\n",
      "Epoch 6\n",
      "0 0.8326080441474915 [0.0797128900885582, 0.00809361319988966, 0.00866953656077385]\n",
      "100 0.750185489654541 [0.07662449032068253, 0.007795590441673994, 0.008849717676639557]\n",
      "200 0.857291579246521 [0.07244972884654999, 0.007603741716593504, 0.008618605323135853]\n",
      "300 0.8809669017791748 [0.07047116756439209, 0.00763727817684412, 0.008405564352869987]\n",
      "train loss: 0.848 | acc: 70.496 (35248/50000)\n",
      "test  loss: 0.873 | acc: 70.31  ( 7031/10000) (up by 4.14)\n",
      "Epoch 7\n",
      "0 0.7373442649841309 [0.06899230927228928, 0.00551387807354331, 0.006813514977693558]\n",
      "100 0.6217749118804932 [0.06734363734722137, 0.005940136965364218, 0.006800603587180376]\n",
      "200 0.7710713148117065 [0.0629371851682663, 0.006196440197527409, 0.0062228054739534855]\n",
      "300 0.7756189107894897 [0.05930357798933983, 0.005948706530034542, 0.006183491554111242]\n",
      "train loss: 0.768 | acc: 73.280 (36640/50000)\n",
      "test  loss: 0.808 | acc: 71.63  ( 7163/10000) (up by 1.32)\n",
      "Epoch 8\n",
      "0 0.6367263793945312 [0.0634787380695343, 0.004075705073773861, 0.004636476747691631]\n",
      "100 0.6603671312332153 [0.06014504283666611, 0.004563168156892061, 0.0047307624481618404]\n",
      "200 0.7781868577003479 [0.057970307767391205, 0.004919718950986862, 0.0049325753934681416]\n",
      "300 0.705282986164093 [0.059774626046419144, 0.005173731129616499, 0.004949675407260656]\n",
      "train loss: 0.727 | acc: 74.784 (37392/50000)\n",
      "test  loss: 0.835 | acc: 71.26  ( 7126/10000)\n",
      "Epoch 9\n",
      "0 0.821495532989502 [0.05386703088879585, 0.0031618846114724874, 0.00385942030698061]\n",
      "100 0.6395696401596069 [0.05484951660037041, 0.00356987863779068, 0.004071575589478016]\n",
      "200 0.7047990560531616 [0.05478256940841675, 0.0035065209958702326, 0.004435803275555372]\n",
      "300 0.6227520704269409 [0.06113689765334129, 0.0036613463889807463, 0.004983896855264902]\n",
      "train loss: 0.690 | acc: 76.152 (38076/50000)\n",
      "test  loss: 0.743 | acc: 74.01  ( 7401/10000) (up by 2.38)\n",
      "Epoch 10\n",
      "0 0.6524322628974915 [0.06015241518616676, 0.00234387069940567, 0.002677065320312977]\n",
      "100 0.7959877252578735 [0.04989059641957283, 0.002704371465370059, 0.0029601873829960823]\n",
      "200 0.6667340993881226 [0.05647270753979683, 0.002908294554799795, 0.003725789487361908]\n",
      "300 0.5552160739898682 [0.05999758094549179, 0.0033248066902160645, 0.0036801774986088276]\n",
      "train loss: 0.664 | acc: 76.962 (38481/50000)\n",
      "test  loss: 0.699 | acc: 75.68  ( 7568/10000) (up by 1.67)\n",
      "Epoch 11\n",
      "0 0.6124935150146484 [0.05286315828561783, 0.001787680434063077, 0.002196138259023428]\n",
      "100 0.5593794584274292 [0.05535097047686577, 0.002350566675886512, 0.002621018560603261]\n",
      "200 0.6112636923789978 [0.06079644337296486, 0.002437226241454482, 0.0031775229144841433]\n",
      "300 0.7060325741767883 [0.06503511965274811, 0.0026158629916608334, 0.003274122718721628]\n",
      "train loss: 0.642 | acc: 77.798 (38899/50000)\n",
      "test  loss: 0.671 | acc: 77.88  ( 7788/10000) (up by 2.20)\n",
      "Epoch 12\n",
      "0 0.5671856999397278 [0.053112633526325226, 0.0016492303693667054, 0.0016899004112929106]\n",
      "100 0.8028589487075806 [0.058606430888175964, 0.002148205181583762, 0.002583221299573779]\n",
      "200 0.7078747749328613 [0.0661679059267044, 0.0023980222176760435, 0.002936778822913766]\n",
      "300 0.7361138463020325 [0.06724027544260025, 0.0025441283360123634, 0.003360505448654294]\n",
      "train loss: 0.625 | acc: 78.498 (39249/50000)\n",
      "test  loss: 0.689 | acc: 76.39  ( 7639/10000)\n",
      "Epoch 13\n",
      "0 0.5795217752456665 [0.06246402859687805, 0.001400264329276979, 0.001525834552012384]\n",
      "100 0.49045276641845703 [0.06706884503364563, 0.001629925100132823, 0.0021621347405016422]\n",
      "200 0.5748311281204224 [0.06324514001607895, 0.0016336964908987284, 0.002696465700864792]\n",
      "300 0.762611985206604 [0.05855536833405495, 0.0019936533644795418, 0.002864411799237132]\n",
      "train loss: 0.612 | acc: 78.834 (39417/50000)\n",
      "test  loss: 0.827 | acc: 72.43  ( 7243/10000)\n",
      "Epoch 14\n",
      "0 0.7087926268577576 [0.061606451869010925, 0.0011254997225478292, 0.0014663315378129482]\n",
      "100 0.5292233824729919 [0.0632525160908699, 0.001496379030868411, 0.00186459394171834]\n",
      "200 0.4298955798149109 [0.061568327248096466, 0.0015840386040508747, 0.0021163925994187593]\n",
      "300 0.5969714522361755 [0.06302324682474136, 0.0023774837609380484, 0.00301670515909791]\n",
      "train loss: 0.594 | acc: 79.376 (39688/50000)\n",
      "test  loss: 0.801 | acc: 74.00  ( 7400/10000)\n",
      "Epoch 15\n",
      "0 0.5618478059768677 [0.07155274599790573, 0.0011874245246872306, 0.001118074986152351]\n",
      "100 0.5480302572250366 [0.06203592196106911, 0.0017587925540283322, 0.0021967010106891394]\n",
      "200 0.6433638334274292 [0.06660068780183792, 0.0019366046180948615, 0.00257623428478837]\n",
      "300 0.6450009942054749 [0.07058137655258179, 0.0018794368952512741, 0.003103778464719653]\n",
      "train loss: 0.584 | acc: 79.708 (39854/50000)\n",
      "test  loss: 0.719 | acc: 75.70  ( 7570/10000)\n",
      "Epoch 16\n",
      "0 0.6712473034858704 [0.06707490235567093, 0.0011275477008894086, 0.0012170214904472232]\n",
      "100 0.585109531879425 [0.06815187633037567, 0.0013112756423652172, 0.002226504497230053]\n",
      "200 0.7156715989112854 [0.07183531671762466, 0.0017862303648144007, 0.0030945735052227974]\n",
      "300 0.5220960974693298 [0.07535594701766968, 0.0020249751396477222, 0.0036292618606239557]\n",
      "train loss: 0.577 | acc: 80.038 (40019/50000)\n",
      "test  loss: 0.831 | acc: 73.10  ( 7310/10000)\n",
      "Epoch 17\n",
      "0 0.470130980014801 [0.0749717429280281, 0.0008686336223036051, 0.0014807234983891249]\n",
      "100 0.612008273601532 [0.08607970178127289, 0.0013438883470371366, 0.0026316680014133453]\n",
      "200 0.5085964202880859 [0.08271756023168564, 0.0014152334770187736, 0.002454883884638548]\n",
      "300 0.655642569065094 [0.08660878241062164, 0.002189688151702285, 0.0025540979113429785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.575 | acc: 80.270 (40135/50000)\n",
      "test  loss: 0.775 | acc: 73.44  ( 7344/10000)\n",
      "Epoch 18\n",
      "0 0.6972066760063171 [0.08370932936668396, 0.0009309304878115654, 0.0012791709741577506]\n",
      "100 0.5550433993339539 [0.07841374725103378, 0.0014296907465904951, 0.002110864967107773]\n",
      "200 0.6731474995613098 [0.07262791693210602, 0.0014554635854437947, 0.0028041889891028404]\n",
      "300 0.4247148633003235 [0.06934884935617447, 0.0020732718985527754, 0.0031631195452064276]\n",
      "train loss: 0.569 | acc: 80.294 (40147/50000)\n",
      "test  loss: 0.714 | acc: 76.77  ( 7677/10000)\n",
      "Epoch 19\n",
      "0 0.5979697704315186 [0.07526425272226334, 0.000888223119545728, 0.0014423412503674626]\n",
      "100 0.4077020287513733 [0.08384042233228683, 0.0014802508521825075, 0.0022622165270149708]\n",
      "200 0.40080755949020386 [0.07797499001026154, 0.001807714463211596, 0.002396781463176012]\n",
      "300 0.5369938611984253 [0.07168252766132355, 0.002119452226907015, 0.002372283022850752]\n",
      "train loss: 0.550 | acc: 80.882 (40441/50000)\n",
      "test  loss: 0.699 | acc: 76.68  ( 7668/10000)\n",
      "Epoch 20\n",
      "0 0.4396136403083801 [0.06712508201599121, 0.0007047596736811101, 0.00133550725877285]\n",
      "100 0.54690021276474 [0.07058008760213852, 0.0011775944149121642, 0.001707532093860209]\n",
      "200 0.6370701193809509 [0.06789530813694, 0.001639010151848197, 0.0023084613494575024]\n",
      "300 0.6204195618629456 [0.06805931031703949, 0.001744558452628553, 0.0028668646700680256]\n",
      "train loss: 0.547 | acc: 81.252 (40626/50000)\n",
      "test  loss: 0.794 | acc: 74.08  ( 7408/10000)\n",
      "Epoch 21\n",
      "0 0.4996739327907562 [0.05311958119273186, 0.0007405129144899547, 0.0013517825864255428]\n",
      "100 0.5998306274414062 [0.05746850371360779, 0.0011275988072156906, 0.0019406437641009688]\n",
      "200 0.6435964703559875 [0.056035127490758896, 0.0015328862937167287, 0.0030022943392395973]\n",
      "300 0.38035690784454346 [0.05905093625187874, 0.0022996431216597557, 0.003041562158614397]\n",
      "train loss: 0.546 | acc: 81.182 (40591/50000)\n",
      "test  loss: 0.668 | acc: 77.15  ( 7715/10000)\n",
      "Epoch 22\n",
      "0 0.6430630087852478 [0.06094741076231003, 0.0007043331279419363, 0.0014415740733966231]\n",
      "100 0.4457031786441803 [0.05830373615026474, 0.0013623317936435342, 0.0019523355877026916]\n",
      "200 0.45959368348121643 [0.06370516121387482, 0.001503571169450879, 0.002242335584014654]\n",
      "300 0.47504958510398865 [0.05702463164925575, 0.0018077361164614558, 0.002488469472154975]\n",
      "train loss: 0.540 | acc: 81.350 (40675/50000)\n",
      "test  loss: 0.808 | acc: 73.14  ( 7314/10000)\n",
      "Epoch 23\n",
      "0 0.5546631813049316 [0.057718534022569656, 0.0007497052429243922, 0.0013767154887318611]\n",
      "100 0.5915810465812683 [0.05945466831326485, 0.0011260530445724726, 0.0023787319660186768]\n",
      "200 0.7522731423377991 [0.06739024072885513, 0.0016588261350989342, 0.0031459119636565447]\n",
      "300 0.5322773456573486 [0.06444653868675232, 0.0020502556581050158, 0.003216156968846917]\n",
      "train loss: 0.538 | acc: 81.432 (40716/50000)\n",
      "test  loss: 0.731 | acc: 75.54  ( 7554/10000)\n",
      "Epoch 24\n",
      "0 0.5139076709747314 [0.07053282856941223, 0.0006913202232681215, 0.0014594080857932568]\n",
      "100 0.3744247555732727 [0.06199723482131958, 0.001487226807512343, 0.0019611131865531206]\n",
      "200 0.620211124420166 [0.05803931504487991, 0.0016125282272696495, 0.00267305807210505]\n",
      "300 0.4554760456085205 [0.06777919828891754, 0.0022892362903803587, 0.0028883691411465406]\n",
      "train loss: 0.535 | acc: 81.508 (40754/50000)\n",
      "test  loss: 0.673 | acc: 77.80  ( 7780/10000)\n",
      "Epoch 25\n",
      "0 0.5876484513282776 [0.06512028723955154, 0.0008774117450229824, 0.0012344899587333202]\n",
      "100 0.6114681959152222 [0.08611519634723663, 0.0019944459199905396, 0.0019244933500885963]\n",
      "200 0.5971342921257019 [0.07516888529062271, 0.002053367905318737, 0.002574570244178176]\n",
      "300 0.4510037899017334 [0.08257777243852615, 0.0022719677072018385, 0.0034591653384268284]\n",
      "train loss: 0.527 | acc: 81.510 (40755/50000)\n",
      "test  loss: 0.561 | acc: 81.16  ( 8116/10000) (up by 3.28)\n",
      "Epoch 26\n",
      "0 0.48159876465797424 [0.0862811878323555, 0.0007989848381839693, 0.0010526604019105434]\n",
      "100 0.6748314499855042 [0.07798744738101959, 0.001391068333759904, 0.0016109395073726773]\n",
      "200 0.6898614764213562 [0.0726381316781044, 0.0017696985742077231, 0.0026442245580255985]\n",
      "300 0.5020349621772766 [0.06649401038885117, 0.0020924953278154135, 0.0028660797979682684]\n",
      "train loss: 0.525 | acc: 81.972 (40986/50000)\n",
      "test  loss: 0.783 | acc: 74.97  ( 7497/10000)\n",
      "Epoch 27\n",
      "0 0.5532066226005554 [0.08374956995248795, 0.0009915527189150453, 0.0009892162634059787]\n",
      "100 0.5020102858543396 [0.0821385383605957, 0.0012187740067020059, 0.001577644725330174]\n",
      "200 0.6939582228660583 [0.08849586546421051, 0.001770205795764923, 0.0029184995219111443]\n",
      "300 0.5134783387184143 [0.08549141138792038, 0.0020992523059248924, 0.003244858467951417]\n",
      "train loss: 0.530 | acc: 81.722 (40861/50000)\n",
      "test  loss: 0.746 | acc: 75.28  ( 7528/10000)\n",
      "Epoch 28\n",
      "0 0.504217803478241 [0.09458959847688675, 0.0009199902997352183, 0.00139095529448241]\n",
      "100 0.4755910336971283 [0.0818391889333725, 0.0016246514860540628, 0.0024719664361327887]\n",
      "200 0.5372328758239746 [0.0918007344007492, 0.001884167897514999, 0.002344879088923335]\n",
      "300 0.720378041267395 [0.09371794760227203, 0.002416173694655299, 0.003684084862470627]\n",
      "train loss: 0.522 | acc: 82.126 (41063/50000)\n",
      "test  loss: 0.620 | acc: 79.20  ( 7920/10000)\n",
      "Epoch 29\n",
      "0 0.4992712736129761 [0.09167798608541489, 0.0007948270649649203, 0.00163669278845191]\n",
      "100 0.4790521562099457 [0.08673645555973053, 0.0013189249439164996, 0.001973601756617427]\n",
      "200 0.44647854566574097 [0.06258301436901093, 0.0015932396054267883, 0.002632386749610305]\n",
      "300 0.4494207203388214 [0.06773412972688675, 0.00230002892203629, 0.003732776502147317]\n",
      "train loss: 0.517 | acc: 82.074 (41037/50000)\n",
      "test  loss: 0.712 | acc: 75.50  ( 7550/10000)\n",
      "Epoch 30\n",
      "0 0.4749722182750702 [0.06805802136659622, 0.0008605847251601517, 0.0019096912583336234]\n",
      "100 0.490801066160202 [0.0690082311630249, 0.0014913913328200579, 0.0021741846576333046]\n",
      "200 0.503419816493988 [0.06800612062215805, 0.0019847587682306767, 0.0027886878233402967]\n",
      "300 0.48502078652381897 [0.07371281087398529, 0.0024229204282164574, 0.0027343600522726774]\n",
      "train loss: 0.515 | acc: 82.270 (41135/50000)\n",
      "test  loss: 0.651 | acc: 78.12  ( 7812/10000)\n",
      "Epoch 31\n",
      "0 0.6205223798751831 [0.07304230332374573, 0.0008919365354813635, 0.0011307576205581427]\n",
      "100 0.5469428896903992 [0.0724743977189064, 0.0017107963794842362, 0.0018889360362663865]\n",
      "200 0.6087266802787781 [0.07638503611087799, 0.0017348548863083124, 0.002852772828191519]\n",
      "300 0.5802647471427917 [0.06493168324232101, 0.0025303226429969072, 0.0024737799540162086]\n",
      "train loss: 0.518 | acc: 82.136 (41068/50000)\n",
      "test  loss: 0.658 | acc: 77.74  ( 7774/10000)\n",
      "Epoch 32\n",
      "0 0.42031943798065186 [0.06757599115371704, 0.0011271641124039888, 0.0010029030963778496]\n",
      "100 0.513664722442627 [0.064641572535038, 0.0014600638533011079, 0.0020702481269836426]\n",
      "200 0.5447732210159302 [0.0685541033744812, 0.0020064350683242083, 0.0033406822476536036]\n",
      "300 0.6036234498023987 [0.07232644408941269, 0.002330473158508539, 0.0036411897744983435]\n",
      "train loss: 0.513 | acc: 82.410 (41205/50000)\n",
      "test  loss: 0.640 | acc: 79.17  ( 7917/10000)\n",
      "Epoch 33\n",
      "0 0.4744320213794708 [0.07136841863393784, 0.0008400612277910113, 0.001634961343370378]\n",
      "100 0.45521998405456543 [0.06490802019834518, 0.0013618936063721776, 0.0020584561862051487]\n",
      "200 0.6605811715126038 [0.06955824047327042, 0.002170936670154333, 0.0022529298439621925]\n",
      "300 0.5325851440429688 [0.07804325968027115, 0.0023454411420971155, 0.003352390369400382]\n",
      "train loss: 0.509 | acc: 82.500 (41250/50000)\n",
      "test  loss: 0.660 | acc: 78.24  ( 7824/10000)\n",
      "Epoch 34\n",
      "0 0.6436187624931335 [0.0722479298710823, 0.0007336746202781796, 0.001758249243721366]\n",
      "100 0.3535095453262329 [0.07459922879934311, 0.0012769910972565413, 0.00250628381036222]\n",
      "200 0.4660533368587494 [0.07879680395126343, 0.002322076354175806, 0.0034989751875400543]\n",
      "300 0.5433499813079834 [0.0688549056649208, 0.0020783143118023872, 0.0031363656744360924]\n",
      "train loss: 0.507 | acc: 82.652 (41326/50000)\n",
      "test  loss: 0.637 | acc: 78.13  ( 7813/10000)\n",
      "Epoch 35\n",
      "0 0.4507559835910797 [0.07376301288604736, 0.0007454821025021374, 0.0012433972442522645]\n",
      "100 0.4417932629585266 [0.072389617562294, 0.0018795765936374664, 0.0018547156360000372]\n",
      "200 0.47633281350135803 [0.07601285725831985, 0.0022846225183457136, 0.0022461791522800922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.3880174160003662 [0.0838351845741272, 0.0030637714080512524, 0.00335335242561996]\n",
      "train loss: 0.504 | acc: 82.714 (41357/50000)\n",
      "test  loss: 0.842 | acc: 72.61  ( 7261/10000)\n",
      "Epoch 36\n",
      "0 0.5108032822608948 [0.09085855633020401, 0.0009114841232076287, 0.0013477579923346639]\n",
      "100 0.4734669327735901 [0.08414360135793686, 0.001761490129865706, 0.002367902547121048]\n",
      "200 0.46316665410995483 [0.08655106276273727, 0.0019181885290890932, 0.003252035938203335]\n",
      "300 0.48596546053886414 [0.09223815053701401, 0.0021711874287575483, 0.0030762297101318836]\n",
      "train loss: 0.505 | acc: 82.678 (41339/50000)\n",
      "test  loss: 0.715 | acc: 76.60  ( 7660/10000)\n",
      "Epoch 37\n",
      "0 0.5684757828712463 [0.09389279782772064, 0.0009987927041947842, 0.0012832869542762637]\n",
      "100 0.6188259720802307 [0.08870349824428558, 0.0015754939522594213, 0.0021625813096761703]\n",
      "200 0.40221819281578064 [0.07853559404611588, 0.0017384688835591078, 0.0030544812325388193]\n",
      "300 0.3984885811805725 [0.09229308366775513, 0.0020795317832380533, 0.003413093276321888]\n",
      "train loss: 0.503 | acc: 82.628 (41314/50000)\n",
      "test  loss: 0.656 | acc: 77.64  ( 7764/10000)\n",
      "Epoch 38\n",
      "0 0.5470336079597473 [0.08307455480098724, 0.0006850669742561877, 0.0011665519559755921]\n",
      "100 0.37806832790374756 [0.10404790192842484, 0.001391284866258502, 0.0022084147203713655]\n",
      "200 0.3810672163963318 [0.0964391902089119, 0.0016949662240222096, 0.002317054197192192]\n",
      "300 0.5184782147407532 [0.0927019789814949, 0.0020079074893146753, 0.002862045541405678]\n",
      "train loss: 0.500 | acc: 82.778 (41389/50000)\n",
      "test  loss: 0.761 | acc: 75.17  ( 7517/10000)\n",
      "Epoch 39\n",
      "0 0.545973539352417 [0.08807551115751266, 0.0007713931845501065, 0.0014928075252100825]\n",
      "100 0.4604043662548065 [0.1027027890086174, 0.0012058293214067817, 0.0017403725069016218]\n",
      "200 0.566506028175354 [0.0930408239364624, 0.001655785832554102, 0.0025067515671253204]\n",
      "300 0.4936012625694275 [0.0952112004160881, 0.0018480448052287102, 0.0032385634258389473]\n",
      "train loss: 0.502 | acc: 82.686 (41343/50000)\n",
      "test  loss: 0.568 | acc: 81.00  ( 8100/10000)\n",
      "Epoch 40\n",
      "0 0.5388445258140564 [0.0992288887500763, 0.0007745494949631393, 0.0015471279621124268]\n",
      "100 0.5887433886528015 [0.09866471588611603, 0.0018316684290766716, 0.002361420774832368]\n",
      "200 0.5881658792495728 [0.09092046320438385, 0.002635358367115259, 0.003591392654925585]\n",
      "300 0.42901161313056946 [0.08318745344877243, 0.002860067179426551, 0.0042860182002186775]\n",
      "train loss: 0.503 | acc: 82.736 (41368/50000)\n",
      "test  loss: 0.815 | acc: 72.81  ( 7281/10000)\n",
      "Epoch 41\n",
      "0 0.517666757106781 [0.08151978254318237, 0.000804716139100492, 0.0017008554423227906]\n",
      "100 0.4974479377269745 [0.08723132312297821, 0.0015754326013848186, 0.0031239455565810204]\n",
      "200 0.5267091393470764 [0.09507926553487778, 0.00218910607509315, 0.0034359267447143793]\n",
      "300 0.4399256706237793 [0.09139895439147949, 0.002224372001364827, 0.004354783799499273]\n",
      "train loss: 0.497 | acc: 82.866 (41433/50000)\n",
      "test  loss: 0.906 | acc: 70.99  ( 7099/10000)\n",
      "Epoch 42\n",
      "0 0.5569319725036621 [0.10165317356586456, 0.0008912543999031186, 0.0019948503468185663]\n",
      "100 0.4730689227581024 [0.11237476766109467, 0.0019115661270916462, 0.0037460043095052242]\n",
      "200 0.5940907001495361 [0.11290456354618073, 0.002103951992467046, 0.004075613338500261]\n",
      "300 0.4968445599079132 [0.10613017529249191, 0.0023433503229171038, 0.004032314755022526]\n",
      "train loss: 0.498 | acc: 82.802 (41401/50000)\n",
      "test  loss: 0.790 | acc: 74.21  ( 7421/10000)\n",
      "Epoch 43\n",
      "0 0.39930224418640137 [0.12082568556070328, 0.0008066610898822546, 0.0015406449092552066]\n",
      "100 0.4827245771884918 [0.12691299617290497, 0.0015438082627952099, 0.0025407092180103064]\n",
      "200 0.4781999886035919 [0.10880574584007263, 0.0023185524623841047, 0.003304236102849245]\n",
      "300 0.5760941505432129 [0.09769786894321442, 0.002501832554116845, 0.0036736910697072744]\n",
      "train loss: 0.493 | acc: 83.058 (41529/50000)\n",
      "test  loss: 0.726 | acc: 76.30  ( 7630/10000)\n",
      "Epoch 44\n",
      "0 0.5556657314300537 [0.10344462841749191, 0.000961323210503906, 0.001505061169154942]\n",
      "100 0.562805712223053 [0.08357717841863632, 0.0015779506647959352, 0.002130628563463688]\n",
      "200 0.4566272795200348 [0.09492165595293045, 0.0025969636626541615, 0.003570648841559887]\n",
      "300 0.6142566800117493 [0.10216012597084045, 0.0027318671345710754, 0.0040066917426884174]\n",
      "train loss: 0.492 | acc: 83.034 (41517/50000)\n",
      "test  loss: 0.614 | acc: 79.43  ( 7943/10000)\n",
      "learning rate downgraded to 0.010000000000000002 at epoch 45\n",
      "loading state_dict from Epoch 25 (acc = 81.16)\n",
      "Epoch 45\n",
      "0 0.6751161813735962 [0.10885871201753616, 0.002500342670828104, 0.0029899124056100845]\n",
      "100 0.40744054317474365 [0.10666576772928238, 0.0026446504052728415, 0.003042789874598384]\n",
      "200 0.396873414516449 [0.10523684322834015, 0.002823718125000596, 0.0029092500917613506]\n",
      "300 0.33776769042015076 [0.10468900203704834, 0.0027826239820569754, 0.003167468588799238]\n",
      "train loss: 0.340 | acc: 88.348 (44174/50000)\n",
      "test  loss: 0.349 | acc: 88.02  ( 8802/10000) (up by 6.86)\n",
      "Epoch 46\n",
      "0 0.2884925305843353 [0.10377209633588791, 0.0009773154743015766, 0.000953510869294405]\n",
      "100 0.33811843395233154 [0.10195460915565491, 0.0009912432869896293, 0.0009393770014867187]\n",
      "200 0.4293775260448456 [0.10169364511966705, 0.0010059844935312867, 0.0008920387481339276]\n",
      "300 0.34773772954940796 [0.10098358988761902, 0.0010150880552828312, 0.0009280859958380461]\n",
      "train loss: 0.296 | acc: 89.808 (44904/50000)\n",
      "test  loss: 0.336 | acc: 88.55  ( 8855/10000) (up by 0.53)\n",
      "Epoch 47\n",
      "0 0.1944652944803238 [0.09928026050329208, 0.0009233007440343499, 0.0008167788037098944]\n",
      "100 0.31497782468795776 [0.1007038950920105, 0.0009181126370094717, 0.0008323128568008542]\n",
      "200 0.3335438370704651 [0.09920049458742142, 0.0009069813531823456, 0.000868062605150044]\n",
      "300 0.24808543920516968 [0.09727337956428528, 0.0008792025619186461, 0.0008458121446892619]\n",
      "train loss: 0.279 | acc: 90.494 (45247/50000)\n",
      "test  loss: 0.330 | acc: 88.66  ( 8866/10000) (up by 0.11)\n",
      "Epoch 48\n",
      "0 0.27212560176849365 [0.09716271609067917, 0.0008335246820934117, 0.0008063889690674841]\n",
      "100 0.2712382376194 [0.09600052982568741, 0.0008331462158821523, 0.000839217274915427]\n",
      "200 0.2754722833633423 [0.09479141980409622, 0.0008114125812426209, 0.0008845861302688718]\n",
      "300 0.30068573355674744 [0.09634248167276382, 0.00084836216410622, 0.0009452710510231555]\n",
      "train loss: 0.268 | acc: 90.816 (45408/50000)\n",
      "test  loss: 0.330 | acc: 88.95  ( 8895/10000) (up by 0.29)\n",
      "Epoch 49\n",
      "0 0.21207565069198608 [0.09612463414669037, 0.0008282389026135206, 0.0008259599562734365]\n",
      "100 0.21553252637386322 [0.09563599526882172, 0.0008422365062870085, 0.0008449145825579762]\n",
      "200 0.3503299355506897 [0.09455735236406326, 0.0008906190632842481, 0.0009041425073519349]\n",
      "300 0.271038293838501 [0.09418214112520218, 0.0009079488809220493, 0.0009527050424367189]\n",
      "train loss: 0.256 | acc: 91.230 (45615/50000)\n",
      "test  loss: 0.322 | acc: 89.03  ( 8903/10000) (up by 0.08)\n",
      "Epoch 50\n",
      "0 0.264914870262146 [0.09282834082841873, 0.00083943922072649, 0.0008573609520681202]\n",
      "100 0.1502390205860138 [0.09316558390855789, 0.00085553148528561, 0.0009401023271493614]\n",
      "200 0.2785463333129883 [0.09282869845628738, 0.0008567380718886852, 0.0009765948634594679]\n",
      "300 0.1771751046180725 [0.09192486107349396, 0.0008346274262294173, 0.0009549115784466267]\n",
      "train loss: 0.249 | acc: 91.504 (45752/50000)\n",
      "test  loss: 0.322 | acc: 89.01  ( 8901/10000)\n",
      "Epoch 51\n",
      "0 0.21183808147907257 [0.09164302051067352, 0.0007906325627118349, 0.0008975726668722928]\n",
      "100 0.21198564767837524 [0.09044789522886276, 0.0008200377342291176, 0.00092059385497123]\n",
      "200 0.3258720636367798 [0.09060872346162796, 0.0008042952395044267, 0.0009383512078784406]\n",
      "300 0.2135077714920044 [0.08727424591779709, 0.0008257364970631897, 0.0009589368710294366]\n",
      "train loss: 0.242 | acc: 91.676 (45838/50000)\n",
      "test  loss: 0.312 | acc: 89.32  ( 8932/10000) (up by 0.29)\n",
      "Epoch 52\n",
      "0 0.19012568891048431 [0.08548057824373245, 0.0008048102026805282, 0.0008784891106188297]\n",
      "100 0.2971648573875427 [0.08518373966217041, 0.0008100061677396297, 0.0008880597306415439]\n",
      "200 0.25470831990242004 [0.08601360023021698, 0.0008170762448571622, 0.0009636268368922174]\n",
      "300 0.16970346868038177 [0.08698240667581558, 0.0008810997824184597, 0.0009433912928216159]\n",
      "train loss: 0.236 | acc: 91.700 (45850/50000)\n",
      "test  loss: 0.313 | acc: 89.56  ( 8956/10000) (up by 0.24)\n",
      "Epoch 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.15532353520393372 [0.0869075208902359, 0.0008254848653450608, 0.0008913796627894044]\n",
      "100 0.18233615159988403 [0.08691118657588959, 0.0007987382705323398, 0.0009884374449029565]\n",
      "200 0.33762088418006897 [0.08588048070669174, 0.0008053504279814661, 0.0009811898926272988]\n",
      "300 0.19287925958633423 [0.08547412604093552, 0.000810656463727355, 0.0011303172213956714]\n",
      "train loss: 0.228 | acc: 92.154 (46077/50000)\n",
      "test  loss: 0.322 | acc: 89.24  ( 8924/10000)\n",
      "Epoch 54\n",
      "0 0.24385741353034973 [0.08596447110176086, 0.0007617035298608243, 0.0009427473996765912]\n",
      "100 0.2659147083759308 [0.08669768273830414, 0.0007924020173959434, 0.0010589573066681623]\n",
      "200 0.09842280298471451 [0.08726697415113449, 0.0007809321396052837, 0.0010691372444853187]\n",
      "300 0.2494063824415207 [0.08785205334424973, 0.0008091972558759153, 0.0011279555037617683]\n",
      "train loss: 0.222 | acc: 92.406 (46203/50000)\n",
      "test  loss: 0.311 | acc: 89.40  ( 8940/10000)\n",
      "Epoch 55\n",
      "0 0.24449476599693298 [0.08817902952432632, 0.0007038518670015037, 0.0009429794154129922]\n",
      "100 0.21118733286857605 [0.08715532720088959, 0.0007111713057383895, 0.000941696809604764]\n",
      "200 0.28657281398773193 [0.08723180741071701, 0.0007205697475001216, 0.0009940983727574348]\n",
      "300 0.23570257425308228 [0.08732573688030243, 0.0007722773589193821, 0.0011183245806023479]\n",
      "train loss: 0.219 | acc: 92.436 (46218/50000)\n",
      "test  loss: 0.313 | acc: 89.51  ( 8951/10000)\n",
      "Epoch 56\n",
      "0 0.24898791313171387 [0.08517783135175705, 0.000710480788256973, 0.0008677534642629325]\n",
      "100 0.2005336880683899 [0.08299347758293152, 0.0007578265503980219, 0.0008394711767323315]\n",
      "200 0.26098838448524475 [0.08378353714942932, 0.0007526271510869265, 0.0008697983575984836]\n",
      "300 0.293899804353714 [0.08293547481298447, 0.000765734410379082, 0.000864146277308464]\n",
      "train loss: 0.218 | acc: 92.364 (46182/50000)\n",
      "test  loss: 0.319 | acc: 89.38  ( 8938/10000)\n",
      "Epoch 57\n",
      "0 0.14511865377426147 [0.0823393315076828, 0.0007242204737849534, 0.000775613880250603]\n",
      "100 0.2357896864414215 [0.08289104700088501, 0.0007480659987777472, 0.0008286177180707455]\n",
      "200 0.21150606870651245 [0.0825464278459549, 0.0007688476471230388, 0.0008917161612771451]\n",
      "300 0.2684723734855652 [0.08142942935228348, 0.000766446755733341, 0.0010895819868892431]\n",
      "train loss: 0.209 | acc: 92.706 (46353/50000)\n",
      "test  loss: 0.320 | acc: 89.28  ( 8928/10000)\n",
      "Epoch 58\n",
      "0 0.20099933445453644 [0.08109285682439804, 0.0007019232143647969, 0.0008367609116248786]\n",
      "100 0.21366474032402039 [0.0798647552728653, 0.0006604524096474051, 0.0008687864756211638]\n",
      "200 0.1812220811843872 [0.08058439195156097, 0.0007347746286541224, 0.0008474658825434744]\n",
      "300 0.22165381908416748 [0.08114396035671234, 0.0007181623368524015, 0.0008707302040420473]\n",
      "train loss: 0.208 | acc: 92.736 (46368/50000)\n",
      "test  loss: 0.312 | acc: 89.63  ( 8963/10000) (up by 0.07)\n",
      "Epoch 59\n",
      "0 0.24682526290416718 [0.08239080756902695, 0.0005888650193810463, 0.0008115076925605536]\n",
      "100 0.1717904955148697 [0.08216449618339539, 0.0006659466307610273, 0.000843927264213562]\n",
      "200 0.16152901947498322 [0.0822153091430664, 0.0006646865513175726, 0.0008629789808765054]\n",
      "300 0.19848540425300598 [0.08168421685695648, 0.0007140074740163982, 0.0010780894663184881]\n",
      "train loss: 0.204 | acc: 93.018 (46509/50000)\n",
      "test  loss: 0.329 | acc: 89.02  ( 8902/10000)\n",
      "Epoch 60\n",
      "0 0.1664162576198578 [0.08119934797286987, 0.0005663252668455243, 0.0008673685370013118]\n",
      "100 0.22451931238174438 [0.08211773633956909, 0.0006210512365214527, 0.000934783776756376]\n",
      "200 0.22068628668785095 [0.08209087699651718, 0.0006026749033480883, 0.0009541637264192104]\n",
      "300 0.12904614210128784 [0.0797186866402626, 0.0006233938620425761, 0.0009604570805095136]\n",
      "train loss: 0.200 | acc: 93.062 (46531/50000)\n",
      "test  loss: 0.337 | acc: 88.89  ( 8889/10000)\n",
      "Epoch 61\n",
      "0 0.18324217200279236 [0.07851129025220871, 0.0005428515723906457, 0.0008815611945465207]\n",
      "100 0.14490513503551483 [0.08005093038082123, 0.0005593128153122962, 0.0008610067307017744]\n",
      "200 0.18249578773975372 [0.0835287868976593, 0.000563976529520005, 0.0009116788860410452]\n",
      "300 0.2798539400100708 [0.08161888271570206, 0.0006236953777261078, 0.0008474417263641953]\n",
      "train loss: 0.198 | acc: 93.168 (46584/50000)\n",
      "test  loss: 0.317 | acc: 89.39  ( 8939/10000)\n",
      "Epoch 62\n",
      "0 0.19331727921962738 [0.07841315120458603, 0.0005100706475786865, 0.0007033760193735361]\n",
      "100 0.28397896885871887 [0.07974035292863846, 0.0005551444482989609, 0.000808490440249443]\n",
      "200 0.21020804345607758 [0.0772191733121872, 0.0005788434064015746, 0.0007750372751615942]\n",
      "300 0.21963563561439514 [0.07286449521780014, 0.000714659399818629, 0.0008804020471870899]\n",
      "train loss: 0.200 | acc: 93.120 (46560/50000)\n",
      "test  loss: 0.329 | acc: 89.56  ( 8956/10000)\n",
      "Epoch 63\n",
      "0 0.2004951536655426 [0.0714394822716713, 0.0004888306721113622, 0.0007391058607026935]\n",
      "100 0.18097738921642303 [0.07098738104104996, 0.0005423637921921909, 0.0008644445915706456]\n",
      "200 0.13358741998672485 [0.07227656245231628, 0.0005993178929202259, 0.0009670807048678398]\n",
      "300 0.2163754254579544 [0.07370606809854507, 0.0006516234134323895, 0.0007856328156776726]\n",
      "train loss: 0.193 | acc: 93.342 (46671/50000)\n",
      "test  loss: 0.330 | acc: 89.09  ( 8909/10000)\n",
      "Epoch 64\n",
      "0 0.2032962143421173 [0.07214623689651489, 0.0004800150927621871, 0.0007041508215479553]\n",
      "100 0.13865308463573456 [0.07491932064294815, 0.0005215178825892508, 0.0006937938742339611]\n",
      "200 0.1939767748117447 [0.07496288418769836, 0.0005622523021884263, 0.000749052211176604]\n",
      "300 0.19073578715324402 [0.07292602211236954, 0.0005989644560031593, 0.0007482648943550885]\n",
      "train loss: 0.194 | acc: 93.098 (46549/50000)\n",
      "test  loss: 0.331 | acc: 89.04  ( 8904/10000)\n",
      "Epoch 65\n",
      "0 0.10775652527809143 [0.0719107910990715, 0.0005131061770953238, 0.0006772091146558523]\n",
      "100 0.17478665709495544 [0.07231836020946503, 0.0005578931304626167, 0.0007274168892763555]\n",
      "200 0.1621265411376953 [0.06868866831064224, 0.0006549765821546316, 0.0007163779810070992]\n",
      "300 0.238279327750206 [0.06852459907531738, 0.0006645804969593883, 0.0007632551132701337]\n",
      "train loss: 0.191 | acc: 93.372 (46686/50000)\n",
      "test  loss: 0.351 | acc: 88.79  ( 8879/10000)\n",
      "Epoch 66\n",
      "0 0.1892332136631012 [0.06876315921545029, 0.0004917992628179491, 0.0006513175321742892]\n",
      "100 0.2588045001029968 [0.06824254989624023, 0.0005778158083558083, 0.0007552540046162903]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(loss_func, opt):\n",
    "    global history\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = net(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        energy = get_energy(net)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(batch_idx, loss.item(), energy)\n",
    "        if epoch < 0:\n",
    "            energy_sum = energy[1] + energy[2]\n",
    "            loss += energy_sum * 100\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = pred.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
    "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
    "\n",
    "def get_energy(net):\n",
    "    energy = [0, 0, 0]\n",
    "    m = net.module\n",
    "    layers = [m.layer1, m.layer2, m.layer3, m.layer4]\n",
    "    for n, layer in enumerate(layers):\n",
    "        for i in range(m.num_blocks[n]-1):\n",
    "            A = layer[i].conv.weight\n",
    "            B = layer[i+1].conv.weight\n",
    "            if A.shape == B.shape:\n",
    "                diff = A - B\n",
    "                energy[0] += torch.sum(diff * diff, dim=(2,3)).min(0)[0].max()\n",
    "                A = layer[i].convx.weight\n",
    "                B = layer[i+1].convx.weight\n",
    "                diff = A - B\n",
    "                energy[1] += torch.sum(diff * diff, dim=(2,3)).min(0)[0].max()\n",
    "                A = layer[i].convy.weight\n",
    "                B = layer[i+1].convy.weight\n",
    "                diff = A - B\n",
    "                energy[2] += torch.sum(diff * diff, dim=(2,3)).min(0)[0].max()\n",
    "    return [e.item() for e in energy]\n",
    "    \n",
    "def test(loss_func):\n",
    "    global checkpoint, history\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = pred.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    history[-1]['loss'] = test_loss\n",
    "    history[-1]['acc'] = acc\n",
    "    if acc > checkpoint['acc']:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
    "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
    "               acc - checkpoint['acc']))\n",
    "        # print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        checkpoint = state\n",
    "        # if not os.path.isdir('checkpoint'):\n",
    "        #     os.mkdir('checkpoint')\n",
    "        # torch.save(state, './checkpoint/ckpt.pth')\n",
    "    else:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
    "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
    "\n",
    "for _ in range(200):\n",
    "    global epoch, checkpoint, history\n",
    "    if epoch - checkpoint['epoch'] >= 20:\n",
    "        lr *= 0.1\n",
    "        print('learning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
    "        print('loading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        checkpoint['epoch'] = epoch\n",
    "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
    "        opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    train(loss_func, opt)\n",
    "    test(loss_func)\n",
    "    epoch += 1\n",
    "print('finish at lr =', lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3R7ZAEEyMUx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': 0, 'epoch': 0},\n",
       " {'epoch': 0,\n",
       "  'train_loss': 1169.85959982872,\n",
       "  'train_acc': 16.206,\n",
       "  'loss': 202.5710642337799,\n",
       "  'acc': 20.63},\n",
       " {'epoch': 1,\n",
       "  'train_loss': 857.1845973730087,\n",
       "  'train_acc': 27.856,\n",
       "  'loss': 173.35470294952393,\n",
       "  'acc': 35.48},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 737.6731110811234,\n",
       "  'train_acc': 37.112,\n",
       "  'loss': 156.45672821998596,\n",
       "  'acc': 41.09},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 658.7332085371017,\n",
       "  'train_acc': 43.226,\n",
       "  'loss': 147.26791775226593,\n",
       "  'acc': 46.12},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 579.6091915369034,\n",
       "  'train_acc': 49.828,\n",
       "  'loss': 133.8121293783188,\n",
       "  'acc': 52.62},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 497.3395382165909,\n",
       "  'train_acc': 56.332,\n",
       "  'loss': 118.1007451415062,\n",
       "  'acc': 58.48},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 436.36036986112595,\n",
       "  'train_acc': 62.228,\n",
       "  'loss': 117.68855303525925,\n",
       "  'acc': 57.54},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 399.8831261396408,\n",
       "  'train_acc': 65.47,\n",
       "  'loss': 96.41954684257507,\n",
       "  'acc': 66.25},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 369.3739624619484,\n",
       "  'train_acc': 68.352,\n",
       "  'loss': 108.94800209999084,\n",
       "  'acc': 63.45},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 341.4793377518654,\n",
       "  'train_acc': 70.76,\n",
       "  'loss': 87.1739552617073,\n",
       "  'acc': 70.07},\n",
       " {'epoch': 10,\n",
       "  'train_loss': 319.1727582216263,\n",
       "  'train_acc': 72.6,\n",
       "  'loss': 87.5623824596405,\n",
       "  'acc': 70.17},\n",
       " {'epoch': 11,\n",
       "  'train_loss': 301.5655908584595,\n",
       "  'train_acc': 74.058,\n",
       "  'loss': 109.78194165229797,\n",
       "  'acc': 64.02},\n",
       " {'epoch': 12,\n",
       "  'train_loss': 289.19023114442825,\n",
       "  'train_acc': 75.194,\n",
       "  'loss': 75.90550881624222,\n",
       "  'acc': 73.57},\n",
       " {'epoch': 13,\n",
       "  'train_loss': 274.6771573126316,\n",
       "  'train_acc': 76.248,\n",
       "  'loss': 76.71270775794983,\n",
       "  'acc': 73.73},\n",
       " {'epoch': 14,\n",
       "  'train_loss': 267.43907621502876,\n",
       "  'train_acc': 77.166,\n",
       "  'loss': 75.81004777550697,\n",
       "  'acc': 75.03},\n",
       " {'epoch': 15,\n",
       "  'train_loss': 257.9490969777107,\n",
       "  'train_acc': 77.932,\n",
       "  'loss': 86.10497891902924,\n",
       "  'acc': 70.78},\n",
       " {'epoch': 16,\n",
       "  'train_loss': 250.64166241884232,\n",
       "  'train_acc': 78.708,\n",
       "  'loss': 84.88227480649948,\n",
       "  'acc': 71.57},\n",
       " {'epoch': 17,\n",
       "  'train_loss': 242.4497573673725,\n",
       "  'train_acc': 79.564,\n",
       "  'loss': 91.76187282800674,\n",
       "  'acc': 72.16},\n",
       " {'epoch': 18,\n",
       "  'train_loss': 239.05664694309235,\n",
       "  'train_acc': 79.94,\n",
       "  'loss': 85.64357253909111,\n",
       "  'acc': 72.39},\n",
       " {'epoch': 19,\n",
       "  'train_loss': 231.32896727323532,\n",
       "  'train_acc': 80.544,\n",
       "  'loss': 83.89407008886337,\n",
       "  'acc': 71.88},\n",
       " {'epoch': 20,\n",
       "  'train_loss': 228.28858131170273,\n",
       "  'train_acc': 80.782,\n",
       "  'loss': 121.27243798971176,\n",
       "  'acc': 62.65},\n",
       " {'epoch': 21,\n",
       "  'train_loss': 221.21289530396461,\n",
       "  'train_acc': 81.414,\n",
       "  'loss': 66.09986627101898,\n",
       "  'acc': 77.75},\n",
       " {'epoch': 22,\n",
       "  'train_loss': 223.4912121295929,\n",
       "  'train_acc': 81.356,\n",
       "  'loss': 65.19429203867912,\n",
       "  'acc': 77.86},\n",
       " {'epoch': 23,\n",
       "  'train_loss': 216.9015080332756,\n",
       "  'train_acc': 81.95,\n",
       "  'loss': 74.5354452431202,\n",
       "  'acc': 75.17},\n",
       " {'epoch': 24,\n",
       "  'train_loss': 213.33773365616798,\n",
       "  'train_acc': 82.326,\n",
       "  'loss': 88.41438698768616,\n",
       "  'acc': 71.95},\n",
       " {'epoch': 25,\n",
       "  'train_loss': 210.65859213471413,\n",
       "  'train_acc': 82.298,\n",
       "  'loss': 85.24597126245499,\n",
       "  'acc': 71.94},\n",
       " {'epoch': 26,\n",
       "  'train_loss': 209.81532853841782,\n",
       "  'train_acc': 82.576,\n",
       "  'loss': 56.869165897369385,\n",
       "  'acc': 80.43},\n",
       " {'epoch': 27,\n",
       "  'train_loss': 205.70964297652245,\n",
       "  'train_acc': 82.794,\n",
       "  'loss': 74.30298471450806,\n",
       "  'acc': 75.24},\n",
       " {'epoch': 28,\n",
       "  'train_loss': 206.61119484901428,\n",
       "  'train_acc': 82.838,\n",
       "  'loss': 62.37618213891983,\n",
       "  'acc': 79.03},\n",
       " {'epoch': 29,\n",
       "  'train_loss': 202.49031937122345,\n",
       "  'train_acc': 82.968,\n",
       "  'loss': 85.17155766487122,\n",
       "  'acc': 72.37},\n",
       " {'epoch': 30,\n",
       "  'train_loss': 200.37366512417793,\n",
       "  'train_acc': 83.286,\n",
       "  'loss': 71.59363630414009,\n",
       "  'acc': 76.41},\n",
       " {'epoch': 31,\n",
       "  'train_loss': 199.44287741184235,\n",
       "  'train_acc': 83.278,\n",
       "  'loss': 51.58026750385761,\n",
       "  'acc': 82.53},\n",
       " {'epoch': 32,\n",
       "  'train_loss': 196.77932316064835,\n",
       "  'train_acc': 83.608,\n",
       "  'loss': 59.17319396138191,\n",
       "  'acc': 80.17},\n",
       " {'epoch': 33,\n",
       "  'train_loss': 196.01418942213058,\n",
       "  'train_acc': 83.658,\n",
       "  'loss': 69.2007414996624,\n",
       "  'acc': 76.97},\n",
       " {'epoch': 34,\n",
       "  'train_loss': 196.96658137440681,\n",
       "  'train_acc': 83.76,\n",
       "  'loss': 62.01026403903961,\n",
       "  'acc': 79.08},\n",
       " {'epoch': 35,\n",
       "  'train_loss': 195.57917839288712,\n",
       "  'train_acc': 84.03,\n",
       "  'loss': 65.02097603678703,\n",
       "  'acc': 78.64},\n",
       " {'epoch': 36,\n",
       "  'train_loss': 191.28743746876717,\n",
       "  'train_acc': 84.198,\n",
       "  'loss': 62.32709497213364,\n",
       "  'acc': 79.75},\n",
       " {'epoch': 37,\n",
       "  'train_loss': 195.462970495224,\n",
       "  'train_acc': 83.932,\n",
       "  'loss': 71.35214990377426,\n",
       "  'acc': 76.7},\n",
       " {'epoch': 38,\n",
       "  'train_loss': 190.74733319878578,\n",
       "  'train_acc': 84.382,\n",
       "  'loss': 65.81543350219727,\n",
       "  'acc': 78.56},\n",
       " {'epoch': 39,\n",
       "  'train_loss': 190.77017711102962,\n",
       "  'train_acc': 84.368,\n",
       "  'loss': 69.54837813973427,\n",
       "  'acc': 77.99},\n",
       " {'epoch': 40,\n",
       "  'train_loss': 191.06389766931534,\n",
       "  'train_acc': 84.328,\n",
       "  'loss': 79.22480171918869,\n",
       "  'acc': 75.16},\n",
       " {'epoch': 41,\n",
       "  'train_loss': 184.7060750424862,\n",
       "  'train_acc': 84.72,\n",
       "  'loss': 63.55613136291504,\n",
       "  'acc': 79.35},\n",
       " {'epoch': 42,\n",
       "  'train_loss': 185.59151747822762,\n",
       "  'train_acc': 84.666,\n",
       "  'loss': 152.1135311126709,\n",
       "  'acc': 66.05},\n",
       " {'epoch': 43,\n",
       "  'train_loss': 182.77813518047333,\n",
       "  'train_acc': 84.818,\n",
       "  'loss': 64.12067496776581,\n",
       "  'acc': 79.42},\n",
       " {'epoch': 44,\n",
       "  'train_loss': 185.09644004702568,\n",
       "  'train_acc': 84.71,\n",
       "  'loss': 54.198883920907974,\n",
       "  'acc': 81.89},\n",
       " {'epoch': 45,\n",
       "  'train_loss': 180.74623093008995,\n",
       "  'train_acc': 85.118,\n",
       "  'loss': 86.220234811306,\n",
       "  'acc': 73.8},\n",
       " {'epoch': 46,\n",
       "  'train_loss': 180.76884996891022,\n",
       "  'train_acc': 85.072,\n",
       "  'loss': 61.798248678445816,\n",
       "  'acc': 79.3},\n",
       " {'epoch': 47,\n",
       "  'train_loss': 181.1861135661602,\n",
       "  'train_acc': 84.992,\n",
       "  'loss': 54.5048853456974,\n",
       "  'acc': 81.83},\n",
       " {'epoch': 48,\n",
       "  'train_loss': 183.66939121484756,\n",
       "  'train_acc': 84.722,\n",
       "  'loss': 63.86251896619797,\n",
       "  'acc': 79.6},\n",
       " {'epoch': 49,\n",
       "  'train_loss': 179.62184777855873,\n",
       "  'train_acc': 85.24,\n",
       "  'loss': 56.70989987254143,\n",
       "  'acc': 80.53},\n",
       " {'epoch': 50,\n",
       "  'train_loss': 170.10098353028297,\n",
       "  'train_acc': 84.984,\n",
       "  'loss': 51.731533616781235,\n",
       "  'acc': 82.81},\n",
       " {'epoch': 51,\n",
       "  'train_loss': 167.37356477975845,\n",
       "  'train_acc': 85.45,\n",
       "  'loss': 71.46831664443016,\n",
       "  'acc': 77.52},\n",
       " {'epoch': 52,\n",
       "  'train_loss': 168.59600499272346,\n",
       "  'train_acc': 85.086,\n",
       "  'loss': 58.90515246987343,\n",
       "  'acc': 81.06},\n",
       " {'epoch': 53,\n",
       "  'train_loss': 165.342099994421,\n",
       "  'train_acc': 85.604,\n",
       "  'loss': 69.06263899803162,\n",
       "  'acc': 77.96},\n",
       " {'epoch': 54,\n",
       "  'train_loss': 166.66428472101688,\n",
       "  'train_acc': 85.478,\n",
       "  'loss': 53.36568573117256,\n",
       "  'acc': 81.64},\n",
       " {'epoch': 55,\n",
       "  'train_loss': 165.9652664065361,\n",
       "  'train_acc': 85.472,\n",
       "  'loss': 69.37680488824844,\n",
       "  'acc': 76.83},\n",
       " {'epoch': 56,\n",
       "  'train_loss': 164.62831319868565,\n",
       "  'train_acc': 85.7,\n",
       "  'loss': 51.7511830329895,\n",
       "  'acc': 82.99},\n",
       " {'epoch': 57,\n",
       "  'train_loss': 165.1286272853613,\n",
       "  'train_acc': 85.662,\n",
       "  'loss': 58.92176687717438,\n",
       "  'acc': 80.49},\n",
       " {'epoch': 58,\n",
       "  'train_loss': 164.81097348034382,\n",
       "  'train_acc': 85.658,\n",
       "  'loss': 85.41012451052666,\n",
       "  'acc': 73.88},\n",
       " {'epoch': 59,\n",
       "  'train_loss': 163.43078093230724,\n",
       "  'train_acc': 85.532,\n",
       "  'loss': 51.94078150391579,\n",
       "  'acc': 82.61},\n",
       " {'epoch': 60,\n",
       "  'train_loss': 163.09822538495064,\n",
       "  'train_acc': 85.69,\n",
       "  'loss': 58.1815260052681,\n",
       "  'acc': 80.95},\n",
       " {'epoch': 61,\n",
       "  'train_loss': 163.82425671815872,\n",
       "  'train_acc': 85.664,\n",
       "  'loss': 69.30604428052902,\n",
       "  'acc': 76.96},\n",
       " {'epoch': 62,\n",
       "  'train_loss': 162.77723470330238,\n",
       "  'train_acc': 85.622,\n",
       "  'loss': 87.43840751051903,\n",
       "  'acc': 74.23},\n",
       " {'epoch': 63,\n",
       "  'train_loss': 160.87206542491913,\n",
       "  'train_acc': 85.936,\n",
       "  'loss': 50.60292509198189,\n",
       "  'acc': 82.79},\n",
       " {'epoch': 64,\n",
       "  'train_loss': 161.49064576625824,\n",
       "  'train_acc': 85.802,\n",
       "  'loss': 51.85096526145935,\n",
       "  'acc': 82.7},\n",
       " {'epoch': 65,\n",
       "  'train_loss': 160.23605020344257,\n",
       "  'train_acc': 85.91,\n",
       "  'loss': 74.2531915307045,\n",
       "  'acc': 76.05},\n",
       " {'epoch': 66,\n",
       "  'train_loss': 160.27759820222855,\n",
       "  'train_acc': 85.928,\n",
       "  'loss': 55.21869480609894,\n",
       "  'acc': 81.35},\n",
       " {'epoch': 67,\n",
       "  'train_loss': 160.93317575752735,\n",
       "  'train_acc': 85.788,\n",
       "  'loss': 64.55892005562782,\n",
       "  'acc': 79.01},\n",
       " {'epoch': 68,\n",
       "  'train_loss': 160.22825537621975,\n",
       "  'train_acc': 85.962,\n",
       "  'loss': 63.312017381191254,\n",
       "  'acc': 79.45},\n",
       " {'epoch': 69,\n",
       "  'train_loss': 159.92867267131805,\n",
       "  'train_acc': 85.998,\n",
       "  'loss': 53.7062871158123,\n",
       "  'acc': 82.51},\n",
       " {'epoch': 70,\n",
       "  'train_loss': 160.3967742919922,\n",
       "  'train_acc': 85.924,\n",
       "  'loss': 59.99211835861206,\n",
       "  'acc': 80.68},\n",
       " {'epoch': 71,\n",
       "  'train_loss': 155.45646372437477,\n",
       "  'train_acc': 86.452,\n",
       "  'loss': 53.82484424114227,\n",
       "  'acc': 81.55},\n",
       " {'epoch': 72,\n",
       "  'train_loss': 160.17826166749,\n",
       "  'train_acc': 86.056,\n",
       "  'loss': 52.30372670292854,\n",
       "  'acc': 81.85},\n",
       " {'epoch': 73,\n",
       "  'train_loss': 160.7699390500784,\n",
       "  'train_acc': 85.898,\n",
       "  'loss': 63.09913620352745,\n",
       "  'acc': 79.11},\n",
       " {'epoch': 74,\n",
       "  'train_loss': 157.17609614133835,\n",
       "  'train_acc': 86.342,\n",
       "  'loss': 57.93686258792877,\n",
       "  'acc': 81.37},\n",
       " {'epoch': 75,\n",
       "  'train_loss': 158.09422753751278,\n",
       "  'train_acc': 86.178,\n",
       "  'loss': 57.815342634916306,\n",
       "  'acc': 80.81},\n",
       " {'epoch': 76, 'acc': 82.99},\n",
       " {'epoch': 76,\n",
       "  'train_loss': 92.94845060259104,\n",
       "  'train_acc': 91.928,\n",
       "  'loss': 24.076794981956482,\n",
       "  'acc': 91.8},\n",
       " {'epoch': 77,\n",
       "  'train_loss': 72.96850749105215,\n",
       "  'train_acc': 93.646,\n",
       "  'loss': 22.806699603796005,\n",
       "  'acc': 92.44},\n",
       " {'epoch': 78,\n",
       "  'train_loss': 64.52061127126217,\n",
       "  'train_acc': 94.372,\n",
       "  'loss': 21.284162543714046,\n",
       "  'acc': 92.95},\n",
       " {'epoch': 79,\n",
       "  'train_loss': 59.4445065446198,\n",
       "  'train_acc': 94.778,\n",
       "  'loss': 21.548901170492172,\n",
       "  'acc': 92.7},\n",
       " {'epoch': 80,\n",
       "  'train_loss': 54.089535031467676,\n",
       "  'train_acc': 95.332,\n",
       "  'loss': 21.41466149687767,\n",
       "  'acc': 92.84},\n",
       " {'epoch': 81,\n",
       "  'train_loss': 50.67570510134101,\n",
       "  'train_acc': 95.476,\n",
       "  'loss': 21.499699294567108,\n",
       "  'acc': 93.04},\n",
       " {'epoch': 82,\n",
       "  'train_loss': 48.18327272310853,\n",
       "  'train_acc': 95.758,\n",
       "  'loss': 21.890938356518745,\n",
       "  'acc': 92.62},\n",
       " {'epoch': 83,\n",
       "  'train_loss': 45.14835059642792,\n",
       "  'train_acc': 96.118,\n",
       "  'loss': 21.909174256026745,\n",
       "  'acc': 92.9},\n",
       " {'epoch': 84,\n",
       "  'train_loss': 43.120540343225,\n",
       "  'train_acc': 96.192,\n",
       "  'loss': 21.558182761073112,\n",
       "  'acc': 93.21},\n",
       " {'epoch': 85,\n",
       "  'train_loss': 41.82209110632539,\n",
       "  'train_acc': 96.316,\n",
       "  'loss': 21.515664227306843,\n",
       "  'acc': 93.17},\n",
       " {'epoch': 86,\n",
       "  'train_loss': 38.63207344152033,\n",
       "  'train_acc': 96.604,\n",
       "  'loss': 22.55472870916128,\n",
       "  'acc': 92.99},\n",
       " {'epoch': 87,\n",
       "  'train_loss': 37.27994812838733,\n",
       "  'train_acc': 96.722,\n",
       "  'loss': 22.997493393719196,\n",
       "  'acc': 92.92},\n",
       " {'epoch': 88,\n",
       "  'train_loss': 36.352415561676025,\n",
       "  'train_acc': 96.824,\n",
       "  'loss': 22.861956737935543,\n",
       "  'acc': 92.75},\n",
       " {'epoch': 89,\n",
       "  'train_loss': 36.462844740599394,\n",
       "  'train_acc': 96.754,\n",
       "  'loss': 23.47056308761239,\n",
       "  'acc': 92.96},\n",
       " {'epoch': 90,\n",
       "  'train_loss': 34.056194404140115,\n",
       "  'train_acc': 97.1,\n",
       "  'loss': 24.041933488100767,\n",
       "  'acc': 92.84},\n",
       " {'epoch': 91,\n",
       "  'train_loss': 33.16341090016067,\n",
       "  'train_acc': 97.116,\n",
       "  'loss': 28.464240230619907,\n",
       "  'acc': 91.88},\n",
       " {'epoch': 92,\n",
       "  'train_loss': 34.04665352590382,\n",
       "  'train_acc': 97.026,\n",
       "  'loss': 23.711731284856796,\n",
       "  'acc': 92.79},\n",
       " {'epoch': 93,\n",
       "  'train_loss': 32.89912450872362,\n",
       "  'train_acc': 97.088,\n",
       "  'loss': 26.819623559713364,\n",
       "  'acc': 91.98},\n",
       " {'epoch': 94,\n",
       "  'train_loss': 32.85907197371125,\n",
       "  'train_acc': 97.18,\n",
       "  'loss': 27.041739784181118,\n",
       "  'acc': 92.1},\n",
       " {'epoch': 95,\n",
       "  'train_loss': 31.748160902410746,\n",
       "  'train_acc': 97.154,\n",
       "  'loss': 27.28680206090212,\n",
       "  'acc': 92.42},\n",
       " {'epoch': 96,\n",
       "  'train_loss': 32.83281492255628,\n",
       "  'train_acc': 97.076,\n",
       "  'loss': 26.932992845773697,\n",
       "  'acc': 92.03},\n",
       " {'epoch': 97,\n",
       "  'train_loss': 32.80289541371167,\n",
       "  'train_acc': 97.118,\n",
       "  'loss': 24.315778516232967,\n",
       "  'acc': 92.6},\n",
       " {'epoch': 98,\n",
       "  'train_loss': 32.116547303274274,\n",
       "  'train_acc': 97.134,\n",
       "  'loss': 26.359690122306347,\n",
       "  'acc': 92.16},\n",
       " {'epoch': 99,\n",
       "  'train_loss': 33.07935921475291,\n",
       "  'train_acc': 97.116,\n",
       "  'loss': 30.411604687571526,\n",
       "  'acc': 90.97},\n",
       " {'epoch': 100,\n",
       "  'train_loss': 29.394342055544257,\n",
       "  'train_acc': 97.53,\n",
       "  'loss': 24.835947766900063,\n",
       "  'acc': 92.77},\n",
       " {'epoch': 101,\n",
       "  'train_loss': 31.588163720443845,\n",
       "  'train_acc': 97.154,\n",
       "  'loss': 23.83422916010022,\n",
       "  'acc': 92.94},\n",
       " {'epoch': 102,\n",
       "  'train_loss': 32.39410556666553,\n",
       "  'train_acc': 97.12,\n",
       "  'loss': 29.14827996492386,\n",
       "  'acc': 91.98},\n",
       " {'epoch': 103,\n",
       "  'train_loss': 32.14331957884133,\n",
       "  'train_acc': 97.17,\n",
       "  'loss': 29.24009270220995,\n",
       "  'acc': 91.42},\n",
       " {'epoch': 104, 'acc': 93.21},\n",
       " {'epoch': 104,\n",
       "  'train_loss': 19.35893750563264,\n",
       "  'train_acc': 98.422,\n",
       "  'loss': 20.490536492317915,\n",
       "  'acc': 93.83},\n",
       " {'epoch': 105,\n",
       "  'train_loss': 13.858345981687307,\n",
       "  'train_acc': 98.96,\n",
       "  'loss': 20.13600681722164,\n",
       "  'acc': 94.04},\n",
       " {'epoch': 106,\n",
       "  'train_loss': 11.284441472962499,\n",
       "  'train_acc': 99.17,\n",
       "  'loss': 19.97987286001444,\n",
       "  'acc': 94.18},\n",
       " {'epoch': 107,\n",
       "  'train_loss': 10.648957390338182,\n",
       "  'train_acc': 99.232,\n",
       "  'loss': 20.186589051038027,\n",
       "  'acc': 94.1},\n",
       " {'epoch': 108,\n",
       "  'train_loss': 9.621584787033498,\n",
       "  'train_acc': 99.324,\n",
       "  'loss': 20.27405183017254,\n",
       "  'acc': 94.18},\n",
       " {'epoch': 109,\n",
       "  'train_loss': 8.572867350652814,\n",
       "  'train_acc': 99.386,\n",
       "  'loss': 20.102864053100348,\n",
       "  'acc': 94.27},\n",
       " {'epoch': 110,\n",
       "  'train_loss': 7.72719064168632,\n",
       "  'train_acc': 99.496,\n",
       "  'loss': 20.40591885894537,\n",
       "  'acc': 94.3},\n",
       " {'epoch': 111,\n",
       "  'train_loss': 7.608695140108466,\n",
       "  'train_acc': 99.48,\n",
       "  'loss': 21.030658531934023,\n",
       "  'acc': 94.25},\n",
       " {'epoch': 112,\n",
       "  'train_loss': 7.104832767508924,\n",
       "  'train_acc': 99.492,\n",
       "  'loss': 20.752700209617615,\n",
       "  'acc': 94.26},\n",
       " {'epoch': 113,\n",
       "  'train_loss': 6.791161710396409,\n",
       "  'train_acc': 99.56,\n",
       "  'loss': 20.951135996729136,\n",
       "  'acc': 94.27},\n",
       " {'epoch': 114,\n",
       "  'train_loss': 6.492431275546551,\n",
       "  'train_acc': 99.582,\n",
       "  'loss': 20.97178315743804,\n",
       "  'acc': 94.3},\n",
       " {'epoch': 115,\n",
       "  'train_loss': 5.988715713843703,\n",
       "  'train_acc': 99.594,\n",
       "  'loss': 21.57286812365055,\n",
       "  'acc': 94.31},\n",
       " {'epoch': 116,\n",
       "  'train_loss': 6.0024256985634565,\n",
       "  'train_acc': 99.614,\n",
       "  'loss': 21.749677930027246,\n",
       "  'acc': 94.23},\n",
       " {'epoch': 117,\n",
       "  'train_loss': 5.676754651591182,\n",
       "  'train_acc': 99.634,\n",
       "  'loss': 21.23311462625861,\n",
       "  'acc': 94.22},\n",
       " {'epoch': 118,\n",
       "  'train_loss': 5.604470659047365,\n",
       "  'train_acc': 99.668,\n",
       "  'loss': 21.370468627661467,\n",
       "  'acc': 94.17},\n",
       " {'epoch': 119,\n",
       "  'train_loss': 5.383636672981083,\n",
       "  'train_acc': 99.682,\n",
       "  'loss': 21.721104584634304,\n",
       "  'acc': 94.18},\n",
       " {'epoch': 120,\n",
       "  'train_loss': 5.1360627096146345,\n",
       "  'train_acc': 99.684,\n",
       "  'loss': 22.276627875864506,\n",
       "  'acc': 94.1}]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIDu_XmJErwi"
   },
   "outputs": [],
   "source": [
    "ResNet18 = {'run0': [(0, 40.77), (1, 50.38), (2, 60.33), (3, 67.54), (4, 72.53), (5, 74.63), (8, 77.29), (10, 80.53), (14, 81.13), (16, 84.1), (20, 84.58), (29, 85.29), (42, 87.32), (50, 92.76), (51, 93.15), (52, 93.52), (53, 93.64), (54, 93.65), (58, 93.92), (75, 94.05), (76, 94.35), (82, 94.37), (85, 94.38), (86, 94.39), (92, 94.47)],\n",
    "        'run1': [(0, 46.66), (1, 59.49), (2, 64.82), (3, 67.94), (4, 73.07), (5, 76.9), (7, 80.42), (10, 82.33), (12, 83.24), (15, 83.56), (22, 84.14), (31, 84.36), (32, 86.76), (36, 87.09), (50, 92.81), (51, 92.97), (52, 93.31), (53, 93.33), (54, 93.58), (55, 93.61), (58, 93.7), (60, 93.71), (62, 93.72), (64, 93.88), (75, 94.32), (76, 94.47), (77, 94.52), (78, 94.55), (79, 94.56), (80, 94.59), (81, 94.68), (83, 94.74), (85, 94.76), (87, 94.77), (88, 94.82), (99, 94.89)], \n",
    "        'run2': [(0, 53.2), (1, 59.44), (2, 74.87), (4, 77.51), (5, 80.48), (8, 81.88), (13, 82.32), (16, 82.91), (18, 82.99), (19, 83.11), (24, 83.8), (25, 85.36), (28, 86.4), (50, 92.47), (51, 93.07), (52, 93.63), (56, 93.71), (61, 93.77), (63, 93.94), (75, 94.26), (76, 94.41), (77, 94.43), (78, 94.67), (81, 94.81), (84, 94.84)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7B52zeW88ef"
   },
   "source": [
    "### Results on ResNet18: \n",
    "Following kuangliu, I manually change the learning rate as follows:\n",
    "\n",
    "* `lr=0.1` for Epoch [0:50] \n",
    "* `lr=0.01` for Epoch [50:75]\n",
    "* `lr=0.001` for Epoch [75:100]\n",
    "\n",
    "epoch | 0 | 5 |  10 | 15 | 20 | 25 | 30 | 45 | 50 | 55 | 65 | 75 | 80 | 90 | 99\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "without twist ('run0') | 40.77 | 74.63 | 80.53 | 81.13 | 84.58 | - | 85.29 | 87.32 | 92.76 | 93.65 | - | 94.05 | 94.35 | 94.39 | 94.47\n",
    "with twist | 42.39 | 74.86 | 81.51 | 82.88 | - | 84.34 | 86.98 | 87.85 | 92.34 | 93.65 | - | 93.73 | 94.22\n",
    "with twist ('run1') | 46.66 | 76.90 | 82.33 | 83.56 | - | 84.14 | - | 87.09 | 92.81 | 93.61 | 93.88 | 94.32 | 94.59 | 94.82 | 94.89\n",
    "with twist | 50.56 | 80.40 | 82.81 | 85.15 | - | - | - | 87.36 | 93.20 | 93.84 | 94.02\n",
    "deep twist (3) | 41.55 | 72.37 | 78.81 | 79.49 | 82.38 | 82.38 | 83.14 | 83.36 \n",
    "\n",
    "Not a significant improvement as I initially thought based on the reported accuracy at https://github.com/kuangliu/pytorch-cifar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMfk2BX43F1X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10_with_PDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

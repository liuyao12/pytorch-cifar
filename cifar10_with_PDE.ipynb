{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf6nUgErY6Bh"
   },
   "source": [
    "# ResNet with a \"twist\"\n",
    "\n",
    "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
    "\n",
    "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning typically includes basics of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
    "\n",
    "* Code based off of https://github.com/kuangliu/pytorch-cifar\n",
    "\n",
    "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPeChzrK7iYC"
   },
   "source": [
    "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [blog post on Observable](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations).\n",
    "\n",
    "neural network | heat equation\n",
    ":----:|:-------:\n",
    "input layer | initial condition\n",
    "feed forward | solving the equation\n",
    "hidden layers | solution at intermediate times\n",
    "output layer | solution at final time\n",
    "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
    "weights | coefficients\n",
    "boundary handling (padding) | boundary condition\n",
    "multiple channels/filters/feature maps | system of (coupled) PDEs\n",
    "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
    "16×16×1×1 kernel | 16×16 matrix of constants\n",
    "\n",
    "\n",
    "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply trying to make it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should (in theory) enable the neural net to learn more ways to deform the input than diffusion and translation (e.g., rotation and scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lmUdhteH5N9s",
    "outputId": "fdb9e1a1-a178-4b65-a778-468c9263b8c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Testing on a random input:\n",
      "INPUT  torch.Size([1, 3, 32, 32])\n",
      "OUTPUT torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "ResNet in PyTorch, forked from https://github.com/kuangliu/pytorch-cifar\n",
    "Reference:\n",
    "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.match = stride == 1 and in_channels == self.expansion * channels\n",
    "        self.twist = False\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.conv1x = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv1y = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if not self.match:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x1 = self.conv1(x)\n",
    "        if self.twist:\n",
    "            _, c, h, w = tuple(x1.shape)\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv1x.weight.data = (self.conv1x.weight - self.conv1x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv1y.weight.data = (self.conv1y.weight - self.conv1y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv1y.weight.data = self.conv1x.weight.transpose(2,3).flip(2)\n",
    "            if self.XX is None:\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "                # print(\"twist initialized, self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "            x1 = self.conv1(x) + self.XX * self.conv1x(x) + self.YY * self.conv1y(x)\n",
    "            # print(\"twist initialized, outside self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "        \n",
    "        x2 = F.relu(self.bn2(x1))\n",
    "        if self.twist:\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(2)\n",
    "            x3 = self.conv2(x2) + self.XX * self.conv2x(x2) + self.YY * self.conv2y(x2)\n",
    "        else:\n",
    "            x3 = self.conv2(x2)\n",
    "        x3 += self.shortcut(x)\n",
    "        return x3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.twist = False\n",
    "        self.channels = channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        if self.twist: \n",
    "            # symmetrize the kernels (force it to be a 1st-order diff op, i.e. a vector field)\n",
    "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
    "            self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            # make y-vector perpendicular to x-vector\n",
    "            # self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(3)\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.twist:\n",
    "            if self.XX is None: # initialize self.XY\n",
    "                _, c, h, w = tuple(x2.shape)\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "            x2 += self.XX * self.conv2x(x1) + self.YY * self.conv2y(x1)\n",
    "        x3 = F.relu(self.bn2(x2))\n",
    "        x4 = self.conv3(x3)\n",
    "        x4 = self.bn3(x4)\n",
    "        x4 += self.shortcut(x)\n",
    "        x4 = F.relu(x4)\n",
    "        return x4\n",
    "\n",
    "\n",
    "class PDEBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(PDEBlock, self).__init__()\n",
    "        self.twist = False\n",
    "        self.match = in_channels == channels and stride == 1\n",
    "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.convx = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.convy = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if not self.match:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                self.conv,\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.XX is None:\n",
    "            _, _, h, w = tuple(x.shape)\n",
    "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
    "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
    "            # self.conv.weight.data = self.orthogonal(self.conv.weight)\n",
    "            # self.convx.weight.data = self.orthogonal(self.convx.weight)\n",
    "        \n",
    "        if self.twist and self.match:\n",
    "            # symmetrize kernels\n",
    "            self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3)) / 2\n",
    "            # self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3)) / 2\n",
    "            self.convy.weight.data = self.convx.weight.transpose(2,3).flip(2)\n",
    "            for i in range(2):\n",
    "                x = self.Euler_step(x)\n",
    "        else:\n",
    "            x = self.Euler_step(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def Euler_step(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        if self.twist and self.match:\n",
    "            x1 += self.XX * self.convx(x) + self.YY * self.convy(x)\n",
    "        x1 += self.shortcut(x)\n",
    "        return x1\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        channels = [self.in_channels * i for i in [1, 2, 4, 4]]\n",
    "        self.num_blocks = num_blocks\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, channels[3], num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(channels[3] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for idx, stride in enumerate(strides):\n",
    "            # twist = twist and idx < 3\n",
    "            layers.append(block(self.in_channels, channels, stride))\n",
    "            self.in_channels = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x = self.conv1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "net = ResNet50()\n",
    "# net = ResNet(BasicBlock, [3,6,4,3])\n",
    "epoch = 0 \n",
    "lr = 0.1\n",
    "checkpoint = {'acc': 0, 'epoch': 0}\n",
    "history = [{'acc': 0, 'epoch': 0}]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.to(device)\n",
    "print('Testing on a random input:')\n",
    "test = torch.randn(1,3,32,32).to(device)\n",
    "print('INPUT ', test.shape)\n",
    "print('OUTPUT', net(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ozKUGgKwcruw",
    "outputId": "f35c1191-0bfb-4436-be1f-2f2f5622f6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.3,0.3), scale=(0.8,1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1RzI4cWWHVlg",
    "outputId": "182f6eb5-1750-463d-a07d-c6dcd15ed0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 (lr=0.0085)\n",
      "train loss: 0.217 | acc: 92.452 (46226/50000)\n",
      "test  loss: 0.255 | acc: 92.16  ( 9216/10000) (up by 0.42)\n",
      "Epoch 51 (lr=0.0078)\n",
      "train loss: 0.209 | acc: 92.772 (46386/50000)\n",
      "test  loss: 0.241 | acc: 92.23  ( 9223/10000) (up by 0.07)\n",
      "Epoch 52 (lr=0.0072)\n",
      "train loss: 0.200 | acc: 93.026 (46513/50000)\n",
      "test  loss: 0.245 | acc: 92.12  ( 9212/10000)\n",
      "Epoch 53 (lr=0.0066)\n",
      "train loss: 0.192 | acc: 93.370 (46685/50000)\n",
      "test  loss: 0.232 | acc: 92.73  ( 9273/10000) (up by 0.50)\n",
      "Epoch 54 (lr=0.0061)\n",
      "train loss: 0.187 | acc: 93.434 (46717/50000)\n",
      "test  loss: 0.250 | acc: 92.63  ( 9263/10000)\n",
      "Epoch 55 (lr=0.0056)\n",
      "train loss: 0.175 | acc: 93.930 (46965/50000)\n",
      "test  loss: 0.216 | acc: 93.31  ( 9331/10000) (up by 0.58)\n",
      "Epoch 56 (lr=0.0052)\n",
      "train loss: 0.170 | acc: 94.144 (47072/50000)\n",
      "test  loss: 0.216 | acc: 93.16  ( 9316/10000)\n",
      "Epoch 57 (lr=0.0048)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(loss_func, opt):\n",
    "    global history\n",
    "    print('Epoch {} (lr={:.4f})'.format(epoch, lr))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = net(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = pred.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
    "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
    "    \n",
    "def test(loss_func):\n",
    "    global checkpoint, history\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = pred.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    history[-1]['loss'] = test_loss\n",
    "    history[-1]['acc'] = acc\n",
    "    if acc > checkpoint['acc']:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
    "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
    "               acc - checkpoint['acc']))\n",
    "        # print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'lr': lr,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        checkpoint = state\n",
    "        # if not os.path.isdir('checkpoint'):\n",
    "        #     os.mkdir('checkpoint')\n",
    "        # torch.save(state, './checkpoint/ckpt.pth')\n",
    "    else:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
    "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
    "\n",
    "\n",
    "from math import exp, e\n",
    "\n",
    "def lr_schedule(x, lr):\n",
    "    x0 = 10\n",
    "    y0 = 0.1\n",
    "    return 0. + exp(- x / x0) * x * e * y0 / x0\n",
    "#     if x < 10:\n",
    "#         return 0.01 if x == 0 else lr + 0.01\n",
    "#     elif 10 <= x < 25:\n",
    "#         return lr - 0.003\n",
    "#     elif 25 <= x:\n",
    "#         return lr * 0.8\n",
    "\n",
    "for _ in range(20):\n",
    "    global epoch, checkpoint, history\n",
    "    if history[-1].get('train_acc', 0) > 99.99:\n",
    "        break\n",
    "    if epoch == 0:\n",
    "        m = net.module\n",
    "        for layer in [m.layer1, m.layer2, m.layer3]: #, m.layer4]:\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].twist = True\n",
    "        print(\"twist on\")\n",
    "        print('testing on random initial weights:')\n",
    "        test(loss_func)\n",
    "    if epoch < 100:\n",
    "        lr = lr_schedule(epoch + 1, lr)\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epoch - checkpoint['epoch'] >= 10:\n",
    "        lr = checkpoint['lr'] * 0.1\n",
    "        print('\\nlearning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
    "        print('loading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        checkpoint['epoch'] = epoch\n",
    "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
    "        opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "    train(loss_func, opt)\n",
    "    test(loss_func)\n",
    "    epoch += 1\n",
    "print('finish at lr = {}, acc = {}'.format(lr, checkpoint['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00010978416685247794 0.10759923607110977 torch.Size([64, 3, 3, 3])\n",
      "\n",
      "-0.0008422630489803851 0.018080590292811394 0.0032747811637818813\n",
      "-0.001270839711651206 0.021102633327245712 0.003429204924032092\n",
      "-0.0014117248356342316 0.023197108879685402 0.00417900737375021\n",
      "\n",
      "-0.004404743667691946 0.0251794271171093 0.024030175060033798\n",
      "-0.0007541141239926219 0.0175800658762455 0.0034878645092248917\n",
      "-0.0008282261551357806 0.016995558515191078 0.003441042033955455\n",
      "\n",
      "-0.0027684776578098536 0.015212113969027996 0.017009109258651733\n",
      "\n",
      "-0.00012686442642007023 0.004946237895637751 0.012030222453176975\n"
     ]
    }
   ],
   "source": [
    "m = net.module\n",
    "print(m.conv1.weight.mean().item(), m.conv1.weight.std().item(), m.conv1.weight.shape)\n",
    "for layer in [m.layer1, m.layer2, m.layer3, m.layer4]:\n",
    "    print()\n",
    "    for i in range(len(layer)):\n",
    "        print(layer[i].conv.weight.mean().item(), layer[i].conv.weight.std().item(), layer[i].convx.weight.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 27])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    " [2.8104, 2.6607, 2.0765, 1.7392, 1.5272, 1.3853, 1.1833, 1.1261, 0.8286,\n",
    "        0.7381, 0.6732, 0.5530, 0.4448, 0.3987, 0.3280, 0.2829, 0.2381, 0.2129,\n",
    "        0.1887, 0.1716, 0.1538, 0.1227, 0.1100, 0.0991, 0.0793, 0.0575, 0.0470],\n",
    " [2.5849, 2.4312, 2.0399, 1.4542, 1.3491, 1.2283, 1.0040, 0.7782, 0.7403,\n",
    "        0.6404, 0.4244, 0.4096, 0.3201, 0.2533, 0.2170, 0.1899, 0.1393, 0.1194,\n",
    "        0.1111, 0.0810, 0.0702, 0.0599, 0.0491, 0.0427, 0.0316, 0.0281, 0.0211],\n",
    " [2.3489, 2.1176, 1.8689, 1.3103, 1.1440, 1.1354, 0.8471, 0.7960, 0.6468,\n",
    "        0.4969, 0.4144, 0.3684, 0.2797, 0.2498, 0.1899, 0.1561, 0.1421, 0.1118,\n",
    "        0.0815, 0.0706, 0.0602, 0.0536, 0.0389, 0.0341, 0.0209, 0.0138, 0.0093],\n",
    " [2.3597, 2.0739, 1.8342, 1.2758, 1.1094, 1.0668, 0.9078, 0.8299, 0.6184,\n",
    "        0.5731, 0.4374, 0.4200, 0.2464, 0.2361, 0.2333, 0.1684, 0.1497, 0.1077,\n",
    "        0.0909, 0.0606, 0.0542, 0.0475, 0.0420, 0.0290, 0.0157, 0.0121, 0.0053],\n",
    " [2.4256, 2.0896, 1.8365, 1.3078, 1.1393, 1.1217, 1.0316, 0.8114, 0.6341,\n",
    "        0.5491, 0.4557, 0.3869, 0.2779, 0.2309, 0.1898, 0.1831, 0.1766, 0.1271,\n",
    "        0.0781, 0.0603, 0.0509, 0.0406, 0.0363, 0.0292, 0.0185, 0.0120, 0.0060],\n",
    " [2.4849, 2.0532, 1.8571, 1.3805, 1.1323, 1.1066, 1.0433, 0.8157, 0.6629,\n",
    "        0.5746, 0.4429, 0.3829, 0.2724, 0.2594, 0.1690, 0.1577, 0.1473, 0.1279,\n",
    "        0.0809, 0.0602, 0.0458, 0.0376, 0.0331, 0.0263, 0.0196, 0.0110, 0.0061],\n",
    " [2.4709, 2.0658, 1.8636, 1.4249, 1.1889, 1.1478, 0.9837, 0.7942, 0.6960,\n",
    "        0.5268, 0.4503, 0.4186, 0.2792, 0.2359, 0.1945, 0.1636, 0.1547, 0.1163,\n",
    "        0.0766, 0.0685, 0.0539, 0.0411, 0.0342, 0.0238, 0.0154, 0.0120, 0.0039],\n",
    " [2.5026, 2.0597, 1.8135, 1.4707, 1.2017, 1.0554, 0.9883, 0.8208, 0.7253,\n",
    "        0.5223, 0.4767, 0.4090, 0.3169, 0.2375, 0.1904, 0.1654, 0.1372, 0.1143,\n",
    "        0.0842, 0.0540, 0.0447, 0.0322, 0.0279, 0.0192, 0.0152, 0.0099, 0.0069],\n",
    " [2.5069, 2.0750, 1.7927, 1.5006, 1.2215, 1.1006, 0.9533, 0.8030, 0.7206,\n",
    "        0.5561, 0.4817, 0.4372, 0.3454, 0.2270, 0.1583, 0.1296, 0.1175, 0.0945,\n",
    "        0.0714, 0.0528, 0.0451, 0.0355, 0.0255, 0.0217, 0.0172, 0.0125, 0.0101],\n",
    " [2.5199, 2.0277, 1.7792, 1.4563, 1.2592, 1.0659, 0.9456, 0.8021, 0.7113,\n",
    "        0.5484, 0.4876, 0.4590, 0.3279, 0.1928, 0.1862, 0.1847, 0.1571, 0.1049,\n",
    "        0.0790, 0.0580, 0.0396, 0.0352, 0.0295, 0.0217, 0.0183, 0.0103, 0.0057],\n",
    " [2.3893, 1.9121, 1.6831, 1.3817, 1.1941, 1.0182, 0.9111, 0.7532, 0.6538,\n",
    "        0.5459, 0.4672, 0.4358, 0.3111, 0.1830, 0.1812, 0.1646, 0.1533, 0.0977,\n",
    "        0.0770, 0.0575, 0.0403, 0.0324, 0.0275, 0.0221, 0.0167, 0.0096, 0.0064],\n",
    " [2.2690, 1.8165, 1.5975, 1.3160, 1.1309, 0.9703, 0.8707, 0.7099, 0.6191,\n",
    "        0.5291, 0.4453, 0.4059, 0.2917, 0.1800, 0.1698, 0.1525, 0.1403, 0.0821,\n",
    "        0.0720, 0.0552, 0.0420, 0.0310, 0.0275, 0.0211, 0.0167, 0.0098, 0.0062],\n",
    " [2.1601, 1.7282, 1.5242, 1.2564, 1.0730, 0.9203, 0.8337, 0.6863, 0.5854,\n",
    "        0.5140, 0.4252, 0.3822, 0.2758, 0.1659, 0.1575, 0.1521, 0.1334, 0.0783,\n",
    "        0.0741, 0.0565, 0.0421, 0.0305, 0.0250, 0.0212, 0.0163, 0.0090, 0.0055],\n",
    " [2.0608, 1.6436, 1.4555, 1.2012, 1.0342, 0.8702, 0.7983, 0.6782, 0.5550,\n",
    "        0.4913, 0.4171, 0.3638, 0.2737, 0.1557, 0.1502, 0.1329, 0.1208, 0.0756,\n",
    "        0.0704, 0.0500, 0.0414, 0.0295, 0.0238, 0.0203, 0.0151, 0.0089, 0.0044],\n",
    " [2.0513, 1.6338, 1.4452, 1.1936, 1.0283, 0.8667, 0.7949, 0.6695, 0.5496,\n",
    "        0.4944, 0.4150, 0.3623, 0.2676, 0.1548, 0.1484, 0.1310, 0.1200, 0.0749,\n",
    "        0.0701, 0.0505, 0.0418, 0.0292, 0.0238, 0.0202, 0.0152, 0.0090, 0.0041]\n",
    "]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7B52zeW88ef"
   },
   "source": [
    "### Results: \n",
    "\n",
    "Run with lr=[0.1, 0.01, 0.001], downgrading if plateaued for 20 epochs.\n",
    "\n",
    "Baseline (classic ResNet)\n",
    "\n",
    "* ResNet50: 94.29 Bottleneck 32, [3,4,6,3], twist=[F,F,F,F]\n",
    "\n",
    "With \"twist\":\n",
    "* 94.52 BasicBlock 64, [2,2,2,2], twist=[T,T,T,T], rotation_aug=10\n",
    "\n",
    "* 94.15 Bottleneck 32, [3,4,6,3], twist=[T,T,T,T]\n",
    "* 94.59 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.84 Bottleneck 32, [8,8,8,3], twist=[T,T,T,F]\n",
    "* 94.22 Bottleneck 32, [8,8,16,3], twist=[T,T,T,F]\n",
    "\n",
    "Run with lr= x e^(-x), i.e., rise linearly at first, then falls off exponentially.\n",
    "\n",
    "* ResNet34\n",
    "* ResNet50\n",
    "* 94.53 (train_acc = 96.13) , BasicBlock 64, [3,4,6,3], twist=[T,T,T,F]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': 9.99, 'epoch': 0, 'loss': 230.3608274459839},\n",
       " {'epoch': 0,\n",
       "  'train_loss': 666.2691235542297,\n",
       "  'train_acc': 36.246,\n",
       "  'loss': 153.22612977027893,\n",
       "  'acc': 47.38},\n",
       " {'epoch': 1,\n",
       "  'train_loss': 517.6593613028526,\n",
       "  'train_acc': 52.334,\n",
       "  'loss': 119.58472263813019,\n",
       "  'acc': 59.18},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 427.99107599258423,\n",
       "  'train_acc': 61.348,\n",
       "  'loss': 97.8248980641365,\n",
       "  'acc': 65.57},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 368.5976007580757,\n",
       "  'train_acc': 67.25,\n",
       "  'loss': 81.78509759902954,\n",
       "  'acc': 71.68},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 330.26688754558563,\n",
       "  'train_acc': 70.686,\n",
       "  'loss': 72.03951048851013,\n",
       "  'acc': 74.96},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 306.86695009469986,\n",
       "  'train_acc': 72.84,\n",
       "  'loss': 68.68046641349792,\n",
       "  'acc': 76.34},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 285.6267733871937,\n",
       "  'train_acc': 74.75,\n",
       "  'loss': 58.28057482838631,\n",
       "  'acc': 79.66},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 267.80362460017204,\n",
       "  'train_acc': 76.364,\n",
       "  'loss': 67.58784228563309,\n",
       "  'acc': 76.89},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 252.18021336197853,\n",
       "  'train_acc': 77.654,\n",
       "  'loss': 68.10494154691696,\n",
       "  'acc': 77.48},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 238.69603729248047,\n",
       "  'train_acc': 78.764,\n",
       "  'loss': 84.76842737197876,\n",
       "  'acc': 73.44},\n",
       " {'epoch': 10,\n",
       "  'train_loss': 228.9899226129055,\n",
       "  'train_acc': 79.614,\n",
       "  'loss': 82.20864635705948,\n",
       "  'acc': 73.4},\n",
       " {'epoch': 11,\n",
       "  'train_loss': 217.36649563908577,\n",
       "  'train_acc': 80.736,\n",
       "  'loss': 47.315788477659225,\n",
       "  'acc': 84.03},\n",
       " {'epoch': 12,\n",
       "  'train_loss': 203.7947068810463,\n",
       "  'train_acc': 81.89,\n",
       "  'loss': 44.541913241147995,\n",
       "  'acc': 84.97},\n",
       " {'epoch': 13,\n",
       "  'train_loss': 194.03989931941032,\n",
       "  'train_acc': 83.068,\n",
       "  'loss': 45.258047088980675,\n",
       "  'acc': 85.2},\n",
       " {'epoch': 14,\n",
       "  'train_loss': 181.23214638233185,\n",
       "  'train_acc': 84.23,\n",
       "  'loss': 40.886669129133224,\n",
       "  'acc': 86.04},\n",
       " {'epoch': 15,\n",
       "  'train_loss': 168.81505770981312,\n",
       "  'train_acc': 85.19,\n",
       "  'loss': 37.453239381313324,\n",
       "  'acc': 87.51},\n",
       " {'epoch': 16,\n",
       "  'train_loss': 162.0363976508379,\n",
       "  'train_acc': 85.826,\n",
       "  'loss': 36.78310486674309,\n",
       "  'acc': 87.41},\n",
       " {'epoch': 17,\n",
       "  'train_loss': 150.48107224702835,\n",
       "  'train_acc': 86.666,\n",
       "  'loss': 34.74047985672951,\n",
       "  'acc': 88.41},\n",
       " {'epoch': 18,\n",
       "  'train_loss': 142.46023334562778,\n",
       "  'train_acc': 87.422,\n",
       "  'loss': 35.233622685074806,\n",
       "  'acc': 88.3},\n",
       " {'epoch': 19,\n",
       "  'train_loss': 132.53982338309288,\n",
       "  'train_acc': 88.366,\n",
       "  'loss': 30.806242659687996,\n",
       "  'acc': 89.47},\n",
       " {'epoch': 20,\n",
       "  'train_loss': 126.59526006877422,\n",
       "  'train_acc': 88.872,\n",
       "  'loss': 28.859239548444748,\n",
       "  'acc': 90.37},\n",
       " {'epoch': 21,\n",
       "  'train_loss': 119.04347951710224,\n",
       "  'train_acc': 89.576,\n",
       "  'loss': 28.31999883055687,\n",
       "  'acc': 90.53},\n",
       " {'epoch': 22,\n",
       "  'train_loss': 111.3269245326519,\n",
       "  'train_acc': 90.18,\n",
       "  'loss': 24.928075410425663,\n",
       "  'acc': 91.62},\n",
       " {'epoch': 23,\n",
       "  'train_loss': 101.77035357058048,\n",
       "  'train_acc': 90.956,\n",
       "  'loss': 25.20368816703558,\n",
       "  'acc': 91.57},\n",
       " {'epoch': 24,\n",
       "  'train_loss': 95.90005028992891,\n",
       "  'train_acc': 91.432,\n",
       "  'loss': 26.213478684425354,\n",
       "  'acc': 91.46},\n",
       " {'epoch': 25,\n",
       "  'train_loss': 90.6054368764162,\n",
       "  'train_acc': 91.928,\n",
       "  'loss': 24.320430725812912,\n",
       "  'acc': 92.23},\n",
       " {'epoch': 26,\n",
       "  'train_loss': 85.15921375155449,\n",
       "  'train_acc': 92.464,\n",
       "  'loss': 24.086441233754158,\n",
       "  'acc': 92.39},\n",
       " {'epoch': 27,\n",
       "  'train_loss': 79.72178784012794,\n",
       "  'train_acc': 92.904,\n",
       "  'loss': 22.39763568341732,\n",
       "  'acc': 92.73},\n",
       " {'epoch': 28,\n",
       "  'train_loss': 73.89659570902586,\n",
       "  'train_acc': 93.456,\n",
       "  'loss': 21.08917561173439,\n",
       "  'acc': 93.19},\n",
       " {'epoch': 29,\n",
       "  'train_loss': 71.56228206679225,\n",
       "  'train_acc': 93.558,\n",
       "  'loss': 21.0093397423625,\n",
       "  'acc': 93.41},\n",
       " {'epoch': 30,\n",
       "  'train_loss': 67.14232063293457,\n",
       "  'train_acc': 93.982,\n",
       "  'loss': 21.146784633398056,\n",
       "  'acc': 93.37},\n",
       " {'epoch': 31,\n",
       "  'train_loss': 64.26483689621091,\n",
       "  'train_acc': 94.368,\n",
       "  'loss': 19.98834800720215,\n",
       "  'acc': 93.56},\n",
       " {'epoch': 32,\n",
       "  'train_loss': 62.14677308499813,\n",
       "  'train_acc': 94.53,\n",
       "  'loss': 20.032008912414312,\n",
       "  'acc': 93.81},\n",
       " {'epoch': 33,\n",
       "  'train_loss': 58.70496967434883,\n",
       "  'train_acc': 94.934,\n",
       "  'loss': 19.96943348646164,\n",
       "  'acc': 93.79},\n",
       " {'epoch': 34,\n",
       "  'train_loss': 56.87298882007599,\n",
       "  'train_acc': 95.07,\n",
       "  'loss': 19.904628328979015,\n",
       "  'acc': 93.89},\n",
       " {'epoch': 35,\n",
       "  'train_loss': 54.848214242607355,\n",
       "  'train_acc': 95.108,\n",
       "  'loss': 19.48341093957424,\n",
       "  'acc': 93.83},\n",
       " {'epoch': 36,\n",
       "  'train_loss': 52.75905891880393,\n",
       "  'train_acc': 95.356,\n",
       "  'loss': 19.157636672258377,\n",
       "  'acc': 93.92},\n",
       " {'epoch': 37,\n",
       "  'train_loss': 51.54561901465058,\n",
       "  'train_acc': 95.472,\n",
       "  'loss': 19.505841329693794,\n",
       "  'acc': 93.88},\n",
       " {'epoch': 38,\n",
       "  'train_loss': 50.86273618042469,\n",
       "  'train_acc': 95.526,\n",
       "  'loss': 19.33512682840228,\n",
       "  'acc': 93.9},\n",
       " {'epoch': 39,\n",
       "  'train_loss': 49.01752393692732,\n",
       "  'train_acc': 95.71,\n",
       "  'loss': 18.960935905575752,\n",
       "  'acc': 93.9},\n",
       " {'epoch': 40,\n",
       "  'train_loss': 47.95500773563981,\n",
       "  'train_acc': 95.824,\n",
       "  'loss': 19.063250362873077,\n",
       "  'acc': 94.11},\n",
       " {'epoch': 41,\n",
       "  'train_loss': 47.164143435657024,\n",
       "  'train_acc': 95.814,\n",
       "  'loss': 18.976284131407738,\n",
       "  'acc': 94.17},\n",
       " {'epoch': 42,\n",
       "  'train_loss': 47.616665713489056,\n",
       "  'train_acc': 95.782,\n",
       "  'loss': 18.87744726985693,\n",
       "  'acc': 94.25},\n",
       " {'epoch': 43,\n",
       "  'train_loss': 46.95778476819396,\n",
       "  'train_acc': 95.93,\n",
       "  'loss': 18.875757686793804,\n",
       "  'acc': 94.29},\n",
       " {'epoch': 44,\n",
       "  'train_loss': 45.82076712325215,\n",
       "  'train_acc': 96.006,\n",
       "  'loss': 19.12398014217615,\n",
       "  'acc': 94.14},\n",
       " {'epoch': 45,\n",
       "  'train_loss': 45.53530374541879,\n",
       "  'train_acc': 96.076,\n",
       "  'loss': 19.120268251746893,\n",
       "  'acc': 94.17},\n",
       " {'epoch': 46,\n",
       "  'train_loss': 44.88455733470619,\n",
       "  'train_acc': 96.084,\n",
       "  'loss': 19.120254833251238,\n",
       "  'acc': 94.25},\n",
       " {'epoch': 47,\n",
       "  'train_loss': 45.3449318446219,\n",
       "  'train_acc': 95.924,\n",
       "  'loss': 18.868264995515347,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 48,\n",
       "  'train_loss': 45.33034622296691,\n",
       "  'train_acc': 95.976,\n",
       "  'loss': 18.71583379432559,\n",
       "  'acc': 94.22},\n",
       " {'epoch': 49,\n",
       "  'train_loss': 44.47442372888327,\n",
       "  'train_acc': 96.022,\n",
       "  'loss': 18.78823348134756,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 50,\n",
       "  'train_loss': 43.62526988238096,\n",
       "  'train_acc': 96.156,\n",
       "  'loss': 18.82933197915554,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 51,\n",
       "  'train_loss': 44.28954343125224,\n",
       "  'train_acc': 96.05,\n",
       "  'loss': 19.10630153864622,\n",
       "  'acc': 94.33},\n",
       " {'epoch': 52,\n",
       "  'train_loss': 44.69549234956503,\n",
       "  'train_acc': 96.078,\n",
       "  'loss': 19.107800144702196,\n",
       "  'acc': 94.34},\n",
       " {'epoch': 53,\n",
       "  'train_loss': 43.42231479845941,\n",
       "  'train_acc': 96.172,\n",
       "  'loss': 18.85310187190771,\n",
       "  'acc': 94.27},\n",
       " {'epoch': 54,\n",
       "  'train_loss': 44.65778976678848,\n",
       "  'train_acc': 96.076,\n",
       "  'loss': 18.760366581380367,\n",
       "  'acc': 94.4},\n",
       " {'epoch': 55,\n",
       "  'train_loss': 43.25567268393934,\n",
       "  'train_acc': 96.24,\n",
       "  'loss': 18.97530959919095,\n",
       "  'acc': 94.4},\n",
       " {'epoch': 56,\n",
       "  'train_loss': 43.876108054071665,\n",
       "  'train_acc': 96.118,\n",
       "  'loss': 18.891455341130495,\n",
       "  'acc': 94.4},\n",
       " {'epoch': 57,\n",
       "  'train_loss': 43.86184210702777,\n",
       "  'train_acc': 96.14,\n",
       "  'loss': 19.090667251497507,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 58,\n",
       "  'train_loss': 44.11509374529123,\n",
       "  'train_acc': 96.106,\n",
       "  'loss': 18.94907058775425,\n",
       "  'acc': 94.35},\n",
       " {'epoch': 59,\n",
       "  'train_loss': 44.53637481480837,\n",
       "  'train_acc': 96.186,\n",
       "  'loss': 19.019448667764664,\n",
       "  'acc': 94.36},\n",
       " {'epoch': 60,\n",
       "  'train_loss': 43.10530900768936,\n",
       "  'train_acc': 96.238,\n",
       "  'loss': 18.681437011808157,\n",
       "  'acc': 94.4},\n",
       " {'epoch': 61,\n",
       "  'train_loss': 42.33747152797878,\n",
       "  'train_acc': 96.276,\n",
       "  'loss': 18.838364113122225,\n",
       "  'acc': 94.47},\n",
       " {'epoch': 62,\n",
       "  'train_loss': 42.953318793326616,\n",
       "  'train_acc': 96.25,\n",
       "  'loss': 18.720218770205975,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 63,\n",
       "  'train_loss': 43.623546715825796,\n",
       "  'train_acc': 96.202,\n",
       "  'loss': 18.89309049025178,\n",
       "  'acc': 94.35},\n",
       " {'epoch': 64,\n",
       "  'train_loss': 43.237705904990435,\n",
       "  'train_acc': 96.136,\n",
       "  'loss': 18.8153104968369,\n",
       "  'acc': 94.48},\n",
       " {'epoch': 65,\n",
       "  'train_loss': 43.02971560508013,\n",
       "  'train_acc': 96.252,\n",
       "  'loss': 18.779968835413456,\n",
       "  'acc': 94.41},\n",
       " {'epoch': 66,\n",
       "  'train_loss': 43.35066430270672,\n",
       "  'train_acc': 96.198,\n",
       "  'loss': 18.65889985859394,\n",
       "  'acc': 94.39},\n",
       " {'epoch': 67,\n",
       "  'train_loss': 43.122365940362215,\n",
       "  'train_acc': 96.134,\n",
       "  'loss': 18.677190728485584,\n",
       "  'acc': 94.53},\n",
       " {'epoch': 68,\n",
       "  'train_loss': 43.64380860328674,\n",
       "  'train_acc': 96.134,\n",
       "  'loss': 18.77369862422347,\n",
       "  'acc': 94.34},\n",
       " {'epoch': 69,\n",
       "  'train_loss': 43.07374004647136,\n",
       "  'train_acc': 96.144,\n",
       "  'loss': 18.842558208853006,\n",
       "  'acc': 94.4},\n",
       " {'epoch': 70,\n",
       "  'train_loss': 43.43833178281784,\n",
       "  'train_acc': 96.194,\n",
       "  'loss': 18.924823943525553,\n",
       "  'acc': 94.33},\n",
       " {'epoch': 71,\n",
       "  'train_loss': 44.677271615713835,\n",
       "  'train_acc': 96.088,\n",
       "  'loss': 18.60051567852497,\n",
       "  'acc': 94.44},\n",
       " {'epoch': 72,\n",
       "  'train_loss': 42.44223950430751,\n",
       "  'train_acc': 96.242,\n",
       "  'loss': 18.88460859656334,\n",
       "  'acc': 94.43},\n",
       " {'epoch': 73,\n",
       "  'train_loss': 42.84537471085787,\n",
       "  'train_acc': 96.16,\n",
       "  'loss': 18.97738031297922,\n",
       "  'acc': 94.26},\n",
       " {'epoch': 74,\n",
       "  'train_loss': 44.10967303439975,\n",
       "  'train_acc': 96.116,\n",
       "  'loss': 18.680970668792725,\n",
       "  'acc': 94.53},\n",
       " {'epoch': 75,\n",
       "  'train_loss': 42.72234381362796,\n",
       "  'train_acc': 96.198,\n",
       "  'loss': 18.773471999913454,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 76,\n",
       "  'train_loss': 43.54933253861964,\n",
       "  'train_acc': 96.18,\n",
       "  'loss': 18.77883967384696,\n",
       "  'acc': 94.48},\n",
       " {'epoch': 77,\n",
       "  'train_loss': 43.475012712180614,\n",
       "  'train_acc': 96.202,\n",
       "  'loss': 19.033240754157305,\n",
       "  'acc': 94.37},\n",
       " {'epoch': 78,\n",
       "  'train_loss': 43.7837705463171,\n",
       "  'train_acc': 96.178,\n",
       "  'loss': 18.722032882273197,\n",
       "  'acc': 94.49},\n",
       " {'epoch': 79,\n",
       "  'train_loss': 43.647408191114664,\n",
       "  'train_acc': 96.176,\n",
       "  'loss': 18.770021356642246,\n",
       "  'acc': 94.33},\n",
       " {'epoch': 80,\n",
       "  'train_loss': 42.712912917137146,\n",
       "  'train_acc': 96.228,\n",
       "  'loss': 18.848756961524487,\n",
       "  'acc': 94.43},\n",
       " {'epoch': 81,\n",
       "  'train_loss': 43.44316331669688,\n",
       "  'train_acc': 96.18,\n",
       "  'loss': 19.02568158507347,\n",
       "  'acc': 94.39},\n",
       " {'epoch': 82,\n",
       "  'train_loss': 43.9487675614655,\n",
       "  'train_acc': 96.082,\n",
       "  'loss': 18.621065951883793,\n",
       "  'acc': 94.53},\n",
       " {'epoch': 83,\n",
       "  'train_loss': 44.18728920444846,\n",
       "  'train_acc': 96.2,\n",
       "  'loss': 18.577500011771917,\n",
       "  'acc': 94.51},\n",
       " {'epoch': 84,\n",
       "  'train_loss': 44.385494358837605,\n",
       "  'train_acc': 96.14,\n",
       "  'loss': 18.480370245873928,\n",
       "  'acc': 94.5},\n",
       " {'epoch': 85,\n",
       "  'train_loss': 42.09251540899277,\n",
       "  'train_acc': 96.328,\n",
       "  'loss': 19.116676028817892,\n",
       "  'acc': 94.36},\n",
       " {'epoch': 86,\n",
       "  'train_loss': 42.611291855573654,\n",
       "  'train_acc': 96.306,\n",
       "  'loss': 19.00342694297433,\n",
       "  'acc': 94.36},\n",
       " {'epoch': 87,\n",
       "  'train_loss': 43.391758784651756,\n",
       "  'train_acc': 96.196,\n",
       "  'loss': 19.298787094652653,\n",
       "  'acc': 94.26},\n",
       " {'epoch': 88,\n",
       "  'train_loss': 42.78263063542545,\n",
       "  'train_acc': 96.312,\n",
       "  'loss': 18.65197752416134,\n",
       "  'acc': 94.45},\n",
       " {'epoch': 89,\n",
       "  'train_loss': 44.05705514922738,\n",
       "  'train_acc': 96.098,\n",
       "  'loss': 18.665730196982622,\n",
       "  'acc': 94.47},\n",
       " {'epoch': 90,\n",
       "  'train_loss': 43.863482639193535,\n",
       "  'train_acc': 96.146,\n",
       "  'loss': 18.722779743373394,\n",
       "  'acc': 94.42},\n",
       " {'epoch': 91,\n",
       "  'train_loss': 42.05923563428223,\n",
       "  'train_acc': 96.33,\n",
       "  'loss': 18.752681951969862,\n",
       "  'acc': 94.47},\n",
       " {'epoch': 92,\n",
       "  'train_loss': 42.037401385605335,\n",
       "  'train_acc': 96.284,\n",
       "  'loss': 18.788171101361513,\n",
       "  'acc': 94.36},\n",
       " {'epoch': 93,\n",
       "  'train_loss': 43.47616632282734,\n",
       "  'train_acc': 96.17,\n",
       "  'loss': 18.895152859389782,\n",
       "  'acc': 94.37},\n",
       " {'epoch': 94,\n",
       "  'train_loss': 43.740633653476834,\n",
       "  'train_acc': 96.166,\n",
       "  'loss': 18.764313738793135,\n",
       "  'acc': 94.36},\n",
       " {'epoch': 95,\n",
       "  'train_loss': 42.23448581248522,\n",
       "  'train_acc': 96.288,\n",
       "  'loss': 19.0474238358438,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 96,\n",
       "  'train_loss': 43.32381239160895,\n",
       "  'train_acc': 96.162,\n",
       "  'loss': 18.75194088742137,\n",
       "  'acc': 94.45},\n",
       " {'epoch': 97,\n",
       "  'train_loss': 42.0506681650877,\n",
       "  'train_acc': 96.316,\n",
       "  'loss': 18.954661574214697,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 98,\n",
       "  'train_loss': 44.122707951813936,\n",
       "  'train_acc': 96.17,\n",
       "  'loss': 18.597111642360687,\n",
       "  'acc': 94.48},\n",
       " {'epoch': 99,\n",
       "  'train_loss': 42.358739072456956,\n",
       "  'train_acc': 96.314,\n",
       "  'loss': 18.90058806538582,\n",
       "  'acc': 94.41},\n",
       " {'epoch': 100, 'acc': 94.53},\n",
       " {'epoch': 100,\n",
       "  'train_loss': 41.23974456638098,\n",
       "  'train_acc': 96.392,\n",
       "  'loss': 18.998775713145733,\n",
       "  'acc': 94.31},\n",
       " {'epoch': 101,\n",
       "  'train_loss': 43.24047443456948,\n",
       "  'train_acc': 96.242,\n",
       "  'loss': 18.90728274732828,\n",
       "  'acc': 94.39},\n",
       " {'epoch': 102,\n",
       "  'train_loss': 42.90843731537461,\n",
       "  'train_acc': 96.284,\n",
       "  'loss': 18.609498161822557,\n",
       "  'acc': 94.46},\n",
       " {'epoch': 103,\n",
       "  'train_loss': 42.70436827838421,\n",
       "  'train_acc': 96.314,\n",
       "  'loss': 19.021365351974964,\n",
       "  'acc': 94.3},\n",
       " {'epoch': 104,\n",
       "  'train_loss': 42.538116447627544,\n",
       "  'train_acc': 96.242,\n",
       "  'loss': 18.69276938587427,\n",
       "  'acc': 94.49},\n",
       " {'epoch': 105,\n",
       "  'train_loss': 42.5353200212121,\n",
       "  'train_acc': 96.266,\n",
       "  'loss': 18.780590016394854,\n",
       "  'acc': 94.39},\n",
       " {'epoch': 106,\n",
       "  'train_loss': 43.78157025948167,\n",
       "  'train_acc': 96.152,\n",
       "  'loss': 18.768731370568275,\n",
       "  'acc': 94.42},\n",
       " {'epoch': 107,\n",
       "  'train_loss': 44.140888852998614,\n",
       "  'train_acc': 96.134,\n",
       "  'loss': 18.936606910079718,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 108,\n",
       "  'train_loss': 43.96957445517182,\n",
       "  'train_acc': 96.15,\n",
       "  'loss': 18.7267655543983,\n",
       "  'acc': 94.29},\n",
       " {'epoch': 109,\n",
       "  'train_loss': 44.076682567596436,\n",
       "  'train_acc': 96.056,\n",
       "  'loss': 18.781143058091402,\n",
       "  'acc': 94.38}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10_with_PDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

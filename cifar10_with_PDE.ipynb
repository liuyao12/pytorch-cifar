{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf6nUgErY6Bh"
   },
   "source": [
    "# Cifar10 with PDE\n",
    "\n",
    "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
    "\n",
    "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning only includes bits of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
    "\n",
    "* Code based off of https://github.com/kuangliu/pytorch-cifar, and the [official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) \n",
    "\n",
    "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPeChzrK7iYC"
   },
   "source": [
    "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [notebook](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations) on observable.\n",
    "\n",
    "neural network | heat equation\n",
    ":----:|:-------:\n",
    "input layer | initial condition\n",
    "feed forward | solving the equation\n",
    "hidden layers | solution at intermediate times\n",
    "output layer | solution at final time\n",
    "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
    "weights | coefficients\n",
    "boundary handling (padding) | boundary condition\n",
    "multiple channels | system of (coupled) PDEs\n",
    "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
    "16×16×1×1 kernel | 16×16 matrix of constants\n",
    "\n",
    "\n",
    "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply making it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should theoretically enable the neural net to learn more ways to deform than diffusion and translation (e.g., rotation and scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lmUdhteH5N9s",
    "outputId": "fdb9e1a1-a178-4b65-a778-468c9263b8c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Testing on a random input:\n",
      "INPUT  torch.Size([1, 3, 32, 32])\n",
      "OUTPUT torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "ResNet in PyTorch  https://github.com/kuangliu/pytorch-cifar\n",
    "Reference:\n",
    "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, twist=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.twist = twist\n",
    "        self.XY1, self.XY2 = None, None\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        if self.XY1 is None:\n",
    "            _, c, h, w = list(x1.shape)\n",
    "            k = c // 3\n",
    "            ones = np.ones((h,w), dtype='float32')\n",
    "            XX = np.indices((h,w), dtype='float32')[1] / w - 0.5\n",
    "            YY = np.indices((h,w), dtype='float32')[0] / h - 0.5\n",
    "            XY1 = [XX] * k + [YY] * k + [ones] * (c - 2 * k)\n",
    "            self.XY1 = torch.from_numpy(np.stack(XY1, axis=0)).to(x.device)\n",
    "        if self.twist:\n",
    "            x1 = self.XY1 * x1\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.XY2 is None:\n",
    "            _, c, h, w = list(x2.shape)\n",
    "            k = c // 3\n",
    "            ones = np.ones((h,w), dtype='float32')\n",
    "            XX = np.indices((h,w), dtype='float32')[1] / w - 0.5\n",
    "            YY = np.indices((h,w), dtype='float32')[0] / h - 0.5\n",
    "            XY2 = [XX] * k + [YY] * k + [ones] * (c - 2 * k)\n",
    "            self.XY2 = torch.from_numpy(np.stack(XY2, axis=0)).to(x.device)\n",
    "        if self.twist:\n",
    "            x2 = self.XY2 * x2\n",
    "        x3 = self.shortcut(x) + self.bn2(x2)\n",
    "        x3 = F.relu(x3)\n",
    "        return x3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, twist=True):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.twist = twist\n",
    "        self.channels = channels\n",
    "        self.XY = None\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        k = self.channels // 3\n",
    "        if self.twist: \n",
    "            # symmetrize the kernels (force it to be a 1st-order diff op, i.e. a vector field)\n",
    "            self.conv2.weight.data[:,:k] = (self.conv2.weight[:,:k] - self.conv2.weight[:,:k].flip(2).flip(3))/2\n",
    "            # make y-vector perpendicular to x-vector\n",
    "            self.conv2.weight.data[:,k:2*k] = self.conv2.weight[:,:k].transpose(2,3).flip(3)\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.twist:\n",
    "            if self.XY is None: # initialize self.XY\n",
    "                _, c, h, w = list(x2.shape)\n",
    "                ones = np.ones((h,w), dtype='float32')\n",
    "                XX = np.indices((h,w), dtype='float32')[1] * 0.1 / w - 0.05\n",
    "                YY = np.indices((h,w), dtype='float32')[0] * 0.1 / h - 0.05\n",
    "                XY = [XX] * k + [YY] * k + [ones] * (c - 2 * k)\n",
    "                self.XY = torch.from_numpy(np.stack(XY, axis=0)).to(x.device)\n",
    "            x2 = self.XY * x2\n",
    "            x2[:,2*k:3*k] += x2[:,:k] + x2[:,k:2*k]\n",
    "        x3 = F.relu(self.bn2(x2))\n",
    "        x4 = self.conv3(x3)\n",
    "        x4 = self.bn3(x4)\n",
    "        x4 += self.shortcut(x)\n",
    "        x4 = F.relu(x4)\n",
    "        return x4\n",
    "\n",
    "\n",
    "class PDE_Block(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1, twist=True):\n",
    "        super(PDE_Block, self).__init__()\n",
    "        self.twist = twist and in_channels == channels and stride == 1\n",
    "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.convx = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.convy = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        self.kernel_x = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]]) / 2\n",
    "        self.kernel_y = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]]) / 2\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.XX is None:\n",
    "            _, _, h, w = tuple(x.shape)\n",
    "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
    "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
    "#             self.conv.weight.data = self.orthogonal(self.conv.weight)\n",
    "#             self.convx.weight.data = self.orthogonal(self.convx.weight)3)\n",
    "        \n",
    "        if self.twist:\n",
    "            # symmetrize kernels\n",
    "            # self.convx.weight.data = self.convx.weight[:,:,0:1,1:2] * self.kernel_y.to(x.device\n",
    "            #                    ) +  self.convx.weight[:,:,1:2,0:1] * self.kernel_x.to(x.device)\n",
    "            self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3)) / 2\n",
    "            self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3)) / 2\n",
    "            # self.convy.weight.data = self.convx.weight.transpose(2,3).flip(2)\n",
    "            for _ in range(3):\n",
    "                x = self.Euler_step(x)\n",
    "        else:\n",
    "            x = self.Euler_step(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def orthogonal(self, x):\n",
    "        shape = x.shape\n",
    "        q, r = torch.qr(self.convx.weight.view(list(shape)[0], -1))\n",
    "        return r.view(shape).to(x.device)\n",
    "    \n",
    "    def Euler_step(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        if self.twist:\n",
    "            x1 += self.XX * self.convx(x) + self.YY * self.convy(x)\n",
    "        x1 += self.shortcut(x)\n",
    "        return x1\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.num_blocks = num_blocks\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1, twist=True)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2, twist=True)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2, twist=True)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2, twist=False)\n",
    "        self.linear = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride, twist=True):\n",
    "        strides = [stride] + [1] * num_blocks\n",
    "        layers = []\n",
    "        for idx, stride in enumerate(strides):\n",
    "            # twist = twist and idx < 3\n",
    "            layers.append(block(self.in_channels, channels, stride, twist))\n",
    "            self.in_channels = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "# net = ResNet50()\n",
    "net = ResNet(PDE_Block, [6,4,3,3])\n",
    "epoch = 0 \n",
    "lr = 0.1\n",
    "checkpoint = {'acc': 0, 'epoch': 0}\n",
    "history = [{'acc': 0, 'epoch': 0}]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.to(device)\n",
    "print('Testing on a random input:')\n",
    "test = torch.randn(1,3,32,32).to(device)\n",
    "print('INPUT ', test.shape)\n",
    "print('OUTPUT', net(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ozKUGgKwcruw",
    "outputId": "f35c1191-0bfb-4436-be1f-2f2f5622f6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(1.1,1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1RzI4cWWHVlg",
    "outputId": "182f6eb5-1750-463d-a07d-c6dcd15ed0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (lr = 0.1)\n",
      "train loss: 1.814 | acc: 32.030 (16015/50000)\n",
      "test  loss: 1.718 | acc: 37.73  ( 3773/10000) (up by 37.73)\n",
      "Epoch 1 (lr = 0.1)\n",
      "train loss: 1.534 | acc: 43.780 (21890/50000)\n",
      "test  loss: 1.566 | acc: 43.07  ( 4307/10000) (up by 5.34)\n",
      "Epoch 2 (lr = 0.1)\n",
      "train loss: 1.387 | acc: 49.500 (24750/50000)\n",
      "test  loss: 1.369 | acc: 51.85  ( 5185/10000) (up by 8.78)\n",
      "Epoch 3 (lr = 0.1)\n",
      "train loss: 1.267 | acc: 54.428 (27214/50000)\n",
      "test  loss: 1.121 | acc: 59.82  ( 5982/10000) (up by 7.97)\n",
      "Epoch 4 (lr = 0.1)\n",
      "train loss: 1.175 | acc: 57.844 (28922/50000)\n",
      "test  loss: 1.146 | acc: 59.20  ( 5920/10000)\n",
      "Epoch 5 (lr = 0.1)\n",
      "train loss: 1.106 | acc: 60.608 (30304/50000)\n",
      "test  loss: 1.215 | acc: 57.78  ( 5778/10000)\n",
      "Epoch 6 (lr = 0.1)\n",
      "train loss: 1.061 | acc: 62.464 (31232/50000)\n",
      "test  loss: 1.088 | acc: 61.65  ( 6165/10000) (up by 1.83)\n",
      "Epoch 7 (lr = 0.1)\n",
      "train loss: 1.030 | acc: 63.856 (31928/50000)\n",
      "test  loss: 1.012 | acc: 64.94  ( 6494/10000) (up by 3.29)\n",
      "Epoch 8 (lr = 0.1)\n",
      "train loss: 0.995 | acc: 64.968 (32484/50000)\n",
      "test  loss: 0.970 | acc: 66.52  ( 6652/10000) (up by 1.58)\n",
      "Epoch 9 (lr = 0.1)\n",
      "train loss: 0.974 | acc: 65.610 (32805/50000)\n",
      "test  loss: 1.391 | acc: 55.90  ( 5590/10000)\n",
      "Epoch 10 (lr = 0.1)\n",
      "train loss: 0.951 | acc: 66.732 (33366/50000)\n",
      "test  loss: 1.106 | acc: 62.32  ( 6232/10000)\n",
      "Epoch 11 (lr = 0.1)\n",
      "train loss: 0.932 | acc: 67.430 (33715/50000)\n",
      "test  loss: 1.003 | acc: 65.10  ( 6510/10000)\n",
      "Epoch 12 (lr = 0.1)\n",
      "train loss: 0.915 | acc: 67.944 (33972/50000)\n",
      "test  loss: 1.280 | acc: 59.08  ( 5908/10000)\n",
      "Epoch 13 (lr = 0.1)\n",
      "train loss: 0.899 | acc: 68.460 (34230/50000)\n",
      "test  loss: 0.913 | acc: 67.93  ( 6793/10000) (up by 1.41)\n",
      "Epoch 14 (lr = 0.1)\n",
      "train loss: 0.885 | acc: 69.088 (34544/50000)\n",
      "test  loss: 0.958 | acc: 67.00  ( 6700/10000)\n",
      "Epoch 15 (lr = 0.1)\n",
      "train loss: 0.870 | acc: 69.564 (34782/50000)\n",
      "test  loss: 1.146 | acc: 61.93  ( 6193/10000)\n",
      "Epoch 16 (lr = 0.1)\n",
      "train loss: 0.872 | acc: 69.700 (34850/50000)\n",
      "test  loss: 0.908 | acc: 68.76  ( 6876/10000) (up by 0.83)\n",
      "Epoch 17 (lr = 0.1)\n",
      "train loss: 0.846 | acc: 70.416 (35208/50000)\n",
      "test  loss: 0.925 | acc: 68.39  ( 6839/10000)\n",
      "Epoch 18 (lr = 0.1)\n",
      "train loss: 0.850 | acc: 70.250 (35125/50000)\n",
      "test  loss: 0.786 | acc: 73.14  ( 7314/10000) (up by 4.38)\n",
      "Epoch 19 (lr = 0.1)\n",
      "train loss: 0.835 | acc: 70.958 (35479/50000)\n",
      "test  loss: 0.897 | acc: 69.64  ( 6964/10000)\n",
      "Epoch 20 (lr = 0.1)\n",
      "train loss: 0.822 | acc: 71.406 (35703/50000)\n",
      "test  loss: 0.887 | acc: 69.13  ( 6913/10000)\n",
      "Epoch 21 (lr = 0.1)\n",
      "train loss: 0.815 | acc: 71.582 (35791/50000)\n",
      "test  loss: 0.996 | acc: 65.17  ( 6517/10000)\n",
      "Epoch 22 (lr = 0.1)\n",
      "train loss: 0.808 | acc: 71.902 (35951/50000)\n",
      "test  loss: 1.045 | acc: 65.57  ( 6557/10000)\n",
      "Epoch 23 (lr = 0.1)\n",
      "train loss: 0.809 | acc: 71.804 (35902/50000)\n",
      "test  loss: 0.762 | acc: 73.00  ( 7300/10000)\n",
      "Epoch 24 (lr = 0.1)\n",
      "train loss: 0.792 | acc: 72.668 (36334/50000)\n",
      "test  loss: 1.133 | acc: 63.55  ( 6355/10000)\n",
      "Epoch 25 (lr = 0.1)\n",
      "train loss: 0.797 | acc: 72.254 (36127/50000)\n",
      "test  loss: 0.830 | acc: 71.29  ( 7129/10000)\n",
      "Epoch 26 (lr = 0.1)\n",
      "train loss: 0.789 | acc: 72.644 (36322/50000)\n",
      "test  loss: 0.869 | acc: 71.16  ( 7116/10000)\n",
      "Epoch 27 (lr = 0.1)\n",
      "train loss: 0.786 | acc: 72.712 (36356/50000)\n",
      "test  loss: 0.948 | acc: 67.16  ( 6716/10000)\n",
      "Epoch 28 (lr = 0.1)\n",
      "train loss: 0.786 | acc: 72.768 (36384/50000)\n",
      "test  loss: 0.966 | acc: 66.22  ( 6622/10000)\n",
      "Epoch 29 (lr = 0.1)\n",
      "train loss: 0.779 | acc: 73.068 (36534/50000)\n",
      "test  loss: 1.201 | acc: 61.96  ( 6196/10000)\n",
      "Epoch 30 (lr = 0.1)\n",
      "train loss: 0.782 | acc: 72.956 (36478/50000)\n",
      "test  loss: 0.863 | acc: 70.36  ( 7036/10000)\n",
      "Epoch 31 (lr = 0.1)\n",
      "train loss: 0.770 | acc: 73.380 (36690/50000)\n",
      "test  loss: 1.168 | acc: 63.67  ( 6367/10000)\n",
      "Epoch 32 (lr = 0.1)\n",
      "train loss: 0.771 | acc: 73.170 (36585/50000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-7eac4e8a1b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finish at lr = {}, acc = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-7eac4e8a1b06>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loss_func)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(loss_func, opt):\n",
    "    global history\n",
    "    print('Epoch {} (lr = {})'.format(epoch, lr))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = net(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = pred.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
    "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
    "    \n",
    "def test(loss_func):\n",
    "    global checkpoint, history\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = pred.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    history[-1]['loss'] = test_loss\n",
    "    history[-1]['acc'] = acc\n",
    "    if acc > checkpoint['acc']:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
    "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
    "               acc - checkpoint['acc']))\n",
    "        # print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        checkpoint = state\n",
    "        # if not os.path.isdir('checkpoint'):\n",
    "        #     os.mkdir('checkpoint')\n",
    "        # torch.save(state, './checkpoint/ckpt.pth')\n",
    "    else:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
    "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
    "\n",
    "for _ in range(300):\n",
    "    global epoch, checkpoint, history\n",
    "#     if epoch < 30:\n",
    "#         lr = (epoch + 1) / 300\n",
    "#         for param_group in opt.param_groups:\n",
    "#             param_group['lr'] = lr\n",
    "    if epoch - checkpoint['epoch'] >= 20:\n",
    "        if lr < 0.00011 or history[-1].get('train_acc', 0) > 99.9:\n",
    "            break\n",
    "        lr *= 0.1\n",
    "        print('learning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
    "        print('loading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        checkpoint['epoch'] = epoch\n",
    "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    train(loss_func, opt)\n",
    "    test(loss_func)\n",
    "    epoch += 1\n",
    "print('finish at lr = {}, acc = {}'.format(lr, checkpoint['acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7B52zeW88ef"
   },
   "source": [
    "### Results: \n",
    "\n",
    "Baseline (classic ResNet)\n",
    "\n",
    "* ResNet50: 94.29 Bottleneck 32, [3,4,6,3], twist=[F,F,F,F]\n",
    "\n",
    "With \"twist\":\n",
    "* 94.15 Bottleneck 32, [3,4,6,3], twist=[T,T,T,T]\n",
    "* 94.59 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.84 Bottleneck 32, [8,8,8,3], twist=[T,T,T,F]\n",
    "* 94.22 Bottleneck 32, [8,8,16,3], twist=[T,T,T,F]\n",
    "\n",
    "Everything is run with lr=[0.1, 0.01, 0.001], downgrading if plateaued for 20 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10_with_PDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

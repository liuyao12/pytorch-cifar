{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cifar10_with_PDE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tf6nUgErY6Bh"
      },
      "source": [
        "# ResNet with a \"twist\"\n",
        "\n",
        "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
        "\n",
        "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning typically includes basics of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
        "\n",
        "* Code based off of https://github.com/kuangliu/pytorch-cifar\n",
        "\n",
        "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BPeChzrK7iYC"
      },
      "source": [
        "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [blog post on Observable](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations).\n",
        "\n",
        "neural network | heat equation\n",
        ":----:|:-------:\n",
        "input layer | initial condition\n",
        "feed forward | solving the equation\n",
        "hidden layers | solution at intermediate times\n",
        "output layer | solution at final time\n",
        "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
        "weights | coefficients\n",
        "boundary handling (padding) | boundary condition\n",
        "multiple channels/filters/feature maps | system of (coupled) PDEs\n",
        "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
        "16×16×1×1 kernel | 16×16 matrix of constants\n",
        "groups=2 (in Conv2d) | matrix is block diagonal (direct sum of 2 blocks)\n",
        "\n",
        "\n",
        "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply trying to make it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should (in theory) enable the neural net to learn more ways to deform the input than diffusion and translation (e.g., rotation and scaling)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lmUdhteH5N9s",
        "outputId": "ed22c545-89ff-4e1c-f44a-5609bfbd010e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "''' \n",
        "ResNet in PyTorch, forked from https://github.com/kuangliu/pytorch-cifar\n",
        "Reference:\n",
        "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.match = stride == 1 and in_channels == self.expansion * channels\n",
        "        self.twist = False\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.XX, self.YY = None, None\n",
        "        self.conv1x = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv1y = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if not self.match:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x1 = self.conv1(x)\n",
        "        if self.twist:\n",
        "            _, c, h, w = tuple(x1.shape)\n",
        "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
        "            self.conv1x.weight.data = (self.conv1x.weight - self.conv1x.weight.flip(2).flip(3)) / 2\n",
        "            # copy the x-kernel to be the y-kernel\n",
        "            # self.conv1y.weight.data = (self.conv1y.weight - self.conv1y.weight.flip(2).flip(3)) / 2\n",
        "            self.conv1y.weight.data = self.conv1x.weight.transpose(2,3).flip(2)\n",
        "            if self.XX is None:\n",
        "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
        "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
        "                # print(\"twist initialized, self.XX\", self.XX.shape, self.XX.mean().item())\n",
        "            x1 = self.conv1(x) + self.XX * self.conv1x(x) + self.YY * self.conv1y(x)\n",
        "            # print(\"twist initialized, outside self.XX\", self.XX.shape, self.XX.mean().item())\n",
        "        \n",
        "        x2 = F.relu(self.bn2(x1))\n",
        "        if self.twist:\n",
        "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
        "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
        "            # copy the x-kernel to be the y-kernel\n",
        "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
        "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(2)\n",
        "            x3 = self.conv2(x2) + self.XX * self.conv2x(x2) + self.YY * self.conv2y(x2)\n",
        "        else:\n",
        "            x3 = self.conv2(x2)\n",
        "        x3 += self.shortcut(x)\n",
        "        return x3\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.twist = False\n",
        "        self.channels = channels\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.XX, self.YY = None, None\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x1 = F.relu(self.bn1(x1))\n",
        "        if self.twist: \n",
        "            # symmetrize the kernels (force it to be a 1st-order diff op, i.e. a vector field)\n",
        "            tmp = (self.conv2x.weight - self.conv2x.weight.flip(2)) / 2\n",
        "            self.conv2x.weight.data = (tmp - tmp.flip(3)) / 2\n",
        "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
        "            # make y-vector perpendicular to x-vector\n",
        "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(3)\n",
        "        x2 = self.conv2(x1)\n",
        "        if self.twist:\n",
        "            if self.XX is None: # initialize self.XY\n",
        "                _, c, h, w = tuple(x2.shape)\n",
        "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
        "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
        "            x2 += self.XX * self.conv2x(x1) + self.YY * self.conv2y(x1)\n",
        "        x3 = F.relu(self.bn2(x2))\n",
        "        x4 = self.conv3(x3)\n",
        "        x4 = self.bn3(x4)\n",
        "        x4 += self.shortcut(x)\n",
        "        x4 = F.relu(x4)\n",
        "        return x4\n",
        "\n",
        "\n",
        "class PDEBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(PDEBlock, self).__init__()\n",
        "        self.twist = False\n",
        "        self.iterations = 1\n",
        "        self.expand = in_channels != channels or stride != 1\n",
        "        if self.expand:\n",
        "            self.conv0 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=stride, padding=1, bias=False)\n",
        "            self.bn0 = nn.BatchNorm2d(channels)\n",
        "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.XX, self.YY = None, None\n",
        "        self.convx = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.convy = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.expand:\n",
        "            x = self.bn0(self.conv0(x))\n",
        "        if self.XX is None:\n",
        "            _, _, h, w = tuple(x.shape)\n",
        "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
        "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
        "        \n",
        "        if self.twist:\n",
        "            # symmetrize kernels\n",
        "            self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3)) / 2\n",
        "            # self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3)) / 2\n",
        "            self.convy.weight.data = self.convx.weight.transpose(2,3).flip(2)\n",
        "        for i in range(self.iterations):\n",
        "            x = self.Euler_step(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "    \n",
        "    def Euler_step(self, x):\n",
        "        if self.twist:\n",
        "            x1 = self.conv(x) + self.XX * self.convx(x) + self.YY * self.convy(x)\n",
        "        else:\n",
        "            x1 = self.conv(x)\n",
        "        return x + x1 / self.iterations\n",
        "\n",
        "    \n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 32\n",
        "        channels = [self.in_channels * i for i in [1, 2, 4, 4]]\n",
        "        self.num_blocks = num_blocks\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
        "        # self.layer4 = self._make_layer(block, channels[3], num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(channels[2] * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, channels, num_blocks, stride):\n",
        "        if num_blocks == 0:\n",
        "            layers = [nn.Conv2d(self.in_channels, channels, kernel_size=1, stride=stride, padding=1, bias=False),\n",
        "                      nn.BatchNorm2d(channels)]\n",
        "            self.in_channels = channels * block.expansion\n",
        "        else:\n",
        "            strides = [stride] + [1] * (num_blocks - 1)\n",
        "            layers = []\n",
        "            for idx, stride in enumerate(strides):\n",
        "                # twist = twist and idx < 3\n",
        "                layers.append(block(self.in_channels, channels, stride))\n",
        "                self.in_channels = channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # x = self.layer4(x)\n",
        "        x = F.avg_pool2d(x, 8)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "# net = ResNet50()\n",
        "net = ResNet(PDEBlock, [3,3,3])\n",
        "epoch = 0 \n",
        "lr = 0.1\n",
        "checkpoint = {'acc': 0, 'epoch': 0}\n",
        "history = [{'acc': 0, 'epoch': 0}]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device =', device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "net.to(device)\n",
        "print('Testing on a random input:')\n",
        "test = torch.randn(1,3,32,32).to(device)\n",
        "print('INPUT ', test.shape)\n",
        "print('OUTPUT', net(test).shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device = cuda\n",
            "Testing on a random input:\n",
            "INPUT  torch.Size([1, 3, 32, 32])\n",
            "OUTPUT torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ozKUGgKwcruw",
        "outputId": "b786d887-d97a-4ee0-fa48-4315cd09769f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.3,0.3), scale=(0.8,1.2)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:07, 23709916.62it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1RzI4cWWHVlg",
        "outputId": "2aaf2933-e636-48b5-8e6b-6c7690c31c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Training\n",
        "def train(loss_func, opt):\n",
        "    global history\n",
        "    print('Epoch {} (lr={:.4f})'.format(epoch, lr))\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (x, y) in enumerate(trainloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt.zero_grad()\n",
        "        pred = net(x)\n",
        "        loss = loss_func(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = pred.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += predicted.eq(y).sum().item()\n",
        "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
        "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
        "    \n",
        "def test(loss_func):\n",
        "    global checkpoint, history\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(testloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = net(x)\n",
        "            loss = loss_func(pred, y)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = pred.max(1)\n",
        "            total += y.size(0)\n",
        "            correct += predicted.eq(y).sum().item()\n",
        "    acc = 100. * correct / total\n",
        "    history[-1]['loss'] = test_loss\n",
        "    history[-1]['acc'] = acc\n",
        "    if acc > checkpoint['acc']:\n",
        "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
        "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
        "               acc - checkpoint['acc']))\n",
        "        # print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'lr': lr,\n",
        "            'acc': acc,\n",
        "            'epoch': epoch\n",
        "        }\n",
        "        checkpoint = state\n",
        "        # if not os.path.isdir('checkpoint'):\n",
        "        #     os.mkdir('checkpoint')\n",
        "        # torch.save(state, './checkpoint/ckpt.pth')\n",
        "    else:\n",
        "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
        "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
        "\n",
        "\n",
        "from math import exp, e\n",
        "\n",
        "def lr_schedule(x, lr):\n",
        "    x0 = 10\n",
        "    y0 = 0.1\n",
        "    return 0.001 + exp(- x / x0) * x * e * y0 / x0\n",
        "#     if x < x0:\n",
        "#         return 0.001 if x == 0 else lr + y0 / x0\n",
        "#     elif x < 100:\n",
        "#         return 0.001 + (lr - 0.001) * 0.95\n",
        "\n",
        "for _ in range(50):\n",
        "    global epoch, checkpoint, history\n",
        "    if history[-1].get('train_acc', 0) > 99.99:\n",
        "        break\n",
        "    if epoch == 0:\n",
        "        m = net.module\n",
        "        for layer in [m.layer1, m.layer2, m.layer3]: #, m.layer4]:\n",
        "            for i in range(len(layer)):\n",
        "                layer[i].twist = True\n",
        "        print(\"twist on\")\n",
        "        print('testing on random initial weights:')\n",
        "        test(loss_func)\n",
        "    if epoch < 0:\n",
        "        lr = lr_schedule(epoch, lr)\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "    elif epoch - checkpoint['epoch'] >= 30:\n",
        "        lr = checkpoint['lr'] * 0.1\n",
        "        print('\\nlearning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
        "        print('loading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
        "        net.load_state_dict(checkpoint['net'])\n",
        "        checkpoint['epoch'] = epoch\n",
        "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
        "        opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        \n",
        "    train(loss_func, opt)\n",
        "    test(loss_func)\n",
        "    epoch += 1\n",
        "print('finish at lr = {}, acc = {}'.format(lr, checkpoint['acc']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twist on\n",
            "testing on random initial weights:\n",
            "test  loss: 2.310 | acc: 8.74  ( 874/10000) (up by 8.74)\n",
            "Epoch 0 (lr=0.1000)\n",
            "train loss: 1.715 | acc: 35.942 (17971/50000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0WrCG8iuIB_",
        "colab_type": "code",
        "colab": {},
        "outputId": "c5117046-7752-4f8b-de48-21bd325d4fe9"
      },
      "source": [
        "m = net.module\n",
        "print(m.conv1.weight.mean().item(), m.conv1.weight.std().item(), m.conv1.weight.shape)\n",
        "for layer in [m.layer1, m.layer2, m.layer3, m.layer4]:\n",
        "    print()\n",
        "    for i in range(len(layer)):\n",
        "        print(layer[i].conv.weight.mean().item(), layer[i].conv.weight.std().item(), layer[i].convx.weight.std().item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.00010978416685247794 0.10759923607110977 torch.Size([64, 3, 3, 3])\n",
            "\n",
            "-0.0008422630489803851 0.018080590292811394 0.0032747811637818813\n",
            "-0.001270839711651206 0.021102633327245712 0.003429204924032092\n",
            "-0.0014117248356342316 0.023197108879685402 0.00417900737375021\n",
            "\n",
            "-0.004404743667691946 0.0251794271171093 0.024030175060033798\n",
            "-0.0007541141239926219 0.0175800658762455 0.0034878645092248917\n",
            "-0.0008282261551357806 0.016995558515191078 0.003441042033955455\n",
            "\n",
            "-0.0027684776578098536 0.015212113969027996 0.017009109258651733\n",
            "\n",
            "-0.00012686442642007023 0.004946237895637751 0.012030222453176975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y7B52zeW88ef"
      },
      "source": [
        "### Results: \n",
        "\n",
        "Training with lr=[0.1, 0.01, 0.001], downgrading if plateaued for 20 epochs.\n",
        "\n",
        "Baseline (classic ResNet)\n",
        "\n",
        "* ResNet50: 94.29 Bottleneck 32, [3,4,6,3], twist=[F,F,F,F]\n",
        "\n",
        "With \"twist\":\n",
        "* 94.52 BasicBlock 64, [2,2,2,2], twist=[T,T,T,T], rotation_aug=10\n",
        "\n",
        "* 94.15 Bottleneck 32, [3,4,6,3], twist=[T,T,T,T]\n",
        "* 94.59 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
        "* 94.28 Bottleneck 32, [3,4,6,3], twist=[T,F,F,F]\n",
        "* 94.84 Bottleneck 32, [8,8,8,3], twist=[T,T,T,F]\n",
        "* 94.22 Bottleneck 32, [8,8,16,3], twist=[T,T,T,F]\n",
        "\n",
        "When training with lr= x e^(-x) -- rising linearly, then falling off exponentially -- it converges faster, in about 100 epochs\n",
        "\n",
        "* 93.98 ResNet50 Bottleneck 16\n",
        "* 94.53 (train_acc=96.13) , BasicBlock 64, [3,4,6,3], twist=[T,T,T,F]\n",
        "* 93.19 Bottleneck 16, [3,4,6,3], twist=[T,T,T,F]\n",
        "* 94.06 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
        "* 94.72 Bottleneck 64, [3,4,6,3], twist=[T,T,T,F]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ90kBl6uICE",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb8a3557-6c80-4f92-fee8-918da5ad2c66"
      },
      "source": [
        "torch.tensor([\n",
        " [2.8104, 2.6607, 2.0765, 1.7392, 1.5272, 1.3853, 1.1833, 1.1261, 0.8286,\n",
        "        0.7381, 0.6732, 0.5530, 0.4448, 0.3987, 0.3280, 0.2829, 0.2381, 0.2129,\n",
        "        0.1887, 0.1716, 0.1538, 0.1227, 0.1100, 0.0991, 0.0793, 0.0575, 0.0470],\n",
        " [2.5849, 2.4312, 2.0399, 1.4542, 1.3491, 1.2283, 1.0040, 0.7782, 0.7403,\n",
        "        0.6404, 0.4244, 0.4096, 0.3201, 0.2533, 0.2170, 0.1899, 0.1393, 0.1194,\n",
        "        0.1111, 0.0810, 0.0702, 0.0599, 0.0491, 0.0427, 0.0316, 0.0281, 0.0211],\n",
        " [2.3489, 2.1176, 1.8689, 1.3103, 1.1440, 1.1354, 0.8471, 0.7960, 0.6468,\n",
        "        0.4969, 0.4144, 0.3684, 0.2797, 0.2498, 0.1899, 0.1561, 0.1421, 0.1118,\n",
        "        0.0815, 0.0706, 0.0602, 0.0536, 0.0389, 0.0341, 0.0209, 0.0138, 0.0093],\n",
        " [2.3597, 2.0739, 1.8342, 1.2758, 1.1094, 1.0668, 0.9078, 0.8299, 0.6184,\n",
        "        0.5731, 0.4374, 0.4200, 0.2464, 0.2361, 0.2333, 0.1684, 0.1497, 0.1077,\n",
        "        0.0909, 0.0606, 0.0542, 0.0475, 0.0420, 0.0290, 0.0157, 0.0121, 0.0053],\n",
        " [2.4256, 2.0896, 1.8365, 1.3078, 1.1393, 1.1217, 1.0316, 0.8114, 0.6341,\n",
        "        0.5491, 0.4557, 0.3869, 0.2779, 0.2309, 0.1898, 0.1831, 0.1766, 0.1271,\n",
        "        0.0781, 0.0603, 0.0509, 0.0406, 0.0363, 0.0292, 0.0185, 0.0120, 0.0060],\n",
        " [2.4849, 2.0532, 1.8571, 1.3805, 1.1323, 1.1066, 1.0433, 0.8157, 0.6629,\n",
        "        0.5746, 0.4429, 0.3829, 0.2724, 0.2594, 0.1690, 0.1577, 0.1473, 0.1279,\n",
        "        0.0809, 0.0602, 0.0458, 0.0376, 0.0331, 0.0263, 0.0196, 0.0110, 0.0061],\n",
        " [2.4709, 2.0658, 1.8636, 1.4249, 1.1889, 1.1478, 0.9837, 0.7942, 0.6960,\n",
        "        0.5268, 0.4503, 0.4186, 0.2792, 0.2359, 0.1945, 0.1636, 0.1547, 0.1163,\n",
        "        0.0766, 0.0685, 0.0539, 0.0411, 0.0342, 0.0238, 0.0154, 0.0120, 0.0039],\n",
        " [2.5026, 2.0597, 1.8135, 1.4707, 1.2017, 1.0554, 0.9883, 0.8208, 0.7253,\n",
        "        0.5223, 0.4767, 0.4090, 0.3169, 0.2375, 0.1904, 0.1654, 0.1372, 0.1143,\n",
        "        0.0842, 0.0540, 0.0447, 0.0322, 0.0279, 0.0192, 0.0152, 0.0099, 0.0069],\n",
        " [2.5069, 2.0750, 1.7927, 1.5006, 1.2215, 1.1006, 0.9533, 0.8030, 0.7206,\n",
        "        0.5561, 0.4817, 0.4372, 0.3454, 0.2270, 0.1583, 0.1296, 0.1175, 0.0945,\n",
        "        0.0714, 0.0528, 0.0451, 0.0355, 0.0255, 0.0217, 0.0172, 0.0125, 0.0101],\n",
        " [2.5199, 2.0277, 1.7792, 1.4563, 1.2592, 1.0659, 0.9456, 0.8021, 0.7113,\n",
        "        0.5484, 0.4876, 0.4590, 0.3279, 0.1928, 0.1862, 0.1847, 0.1571, 0.1049,\n",
        "        0.0790, 0.0580, 0.0396, 0.0352, 0.0295, 0.0217, 0.0183, 0.0103, 0.0057],\n",
        " [2.3893, 1.9121, 1.6831, 1.3817, 1.1941, 1.0182, 0.9111, 0.7532, 0.6538,\n",
        "        0.5459, 0.4672, 0.4358, 0.3111, 0.1830, 0.1812, 0.1646, 0.1533, 0.0977,\n",
        "        0.0770, 0.0575, 0.0403, 0.0324, 0.0275, 0.0221, 0.0167, 0.0096, 0.0064],\n",
        " [2.2690, 1.8165, 1.5975, 1.3160, 1.1309, 0.9703, 0.8707, 0.7099, 0.6191,\n",
        "        0.5291, 0.4453, 0.4059, 0.2917, 0.1800, 0.1698, 0.1525, 0.1403, 0.0821,\n",
        "        0.0720, 0.0552, 0.0420, 0.0310, 0.0275, 0.0211, 0.0167, 0.0098, 0.0062],\n",
        " [2.1601, 1.7282, 1.5242, 1.2564, 1.0730, 0.9203, 0.8337, 0.6863, 0.5854,\n",
        "        0.5140, 0.4252, 0.3822, 0.2758, 0.1659, 0.1575, 0.1521, 0.1334, 0.0783,\n",
        "        0.0741, 0.0565, 0.0421, 0.0305, 0.0250, 0.0212, 0.0163, 0.0090, 0.0055],\n",
        " [2.0608, 1.6436, 1.4555, 1.2012, 1.0342, 0.8702, 0.7983, 0.6782, 0.5550,\n",
        "        0.4913, 0.4171, 0.3638, 0.2737, 0.1557, 0.1502, 0.1329, 0.1208, 0.0756,\n",
        "        0.0704, 0.0500, 0.0414, 0.0295, 0.0238, 0.0203, 0.0151, 0.0089, 0.0044],\n",
        " [2.0513, 1.6338, 1.4452, 1.1936, 1.0283, 0.8667, 0.7949, 0.6695, 0.5496,\n",
        "        0.4944, 0.4150, 0.3623, 0.2676, 0.1548, 0.1484, 0.1310, 0.1200, 0.0749,\n",
        "        0.0701, 0.0505, 0.0418, 0.0292, 0.0238, 0.0202, 0.0152, 0.0090, 0.0041]\n",
        "]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 27])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4S2q8OxuICH",
        "colab_type": "code",
        "colab": {},
        "outputId": "5f9559d5-dcd2-4726-c31a-9ce53ba8c69e"
      },
      "source": [
        "history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'acc': 10.0, 'epoch': 0, 'loss': 230.31848001480103},\n",
              " {'epoch': 0,\n",
              "  'train_loss': 754.6636430025101,\n",
              "  'train_acc': 28.956,\n",
              "  'loss': 177.68029403686523,\n",
              "  'acc': 35.7},\n",
              " {'epoch': 1,\n",
              "  'train_loss': 582.1219648122787,\n",
              "  'train_acc': 45.178,\n",
              "  'loss': 166.2290700674057,\n",
              "  'acc': 49.44},\n",
              " {'epoch': 2,\n",
              "  'train_loss': 487.1171388030052,\n",
              "  'train_acc': 55.164,\n",
              "  'loss': 141.82904732227325,\n",
              "  'acc': 55.11},\n",
              " {'epoch': 3,\n",
              "  'train_loss': 438.89170759916306,\n",
              "  'train_acc': 59.926,\n",
              "  'loss': 126.66632717847824,\n",
              "  'acc': 55.96},\n",
              " {'epoch': 4,\n",
              "  'train_loss': 402.5308494567871,\n",
              "  'train_acc': 63.694,\n",
              "  'loss': 142.63620400428772,\n",
              "  'acc': 57.95},\n",
              " {'epoch': 5,\n",
              "  'train_loss': 371.8175357580185,\n",
              "  'train_acc': 66.636,\n",
              "  'loss': 113.34880620241165,\n",
              "  'acc': 62.46},\n",
              " {'epoch': 6,\n",
              "  'train_loss': 346.9781383275986,\n",
              "  'train_acc': 68.852,\n",
              "  'loss': 82.87301754951477,\n",
              "  'acc': 71.34},\n",
              " {'epoch': 7,\n",
              "  'train_loss': 329.3637217283249,\n",
              "  'train_acc': 70.536,\n",
              "  'loss': 95.5984001159668,\n",
              "  'acc': 69.68},\n",
              " {'epoch': 8,\n",
              "  'train_loss': 316.4112443327904,\n",
              "  'train_acc': 71.912,\n",
              "  'loss': 111.94636160135269,\n",
              "  'acc': 65.53},\n",
              " {'epoch': 9,\n",
              "  'train_loss': 309.1336905658245,\n",
              "  'train_acc': 72.488,\n",
              "  'loss': 85.85902404785156,\n",
              "  'acc': 70.85},\n",
              " {'epoch': 10,\n",
              "  'train_loss': 302.88826566934586,\n",
              "  'train_acc': 73.134,\n",
              "  'loss': 92.3339295387268,\n",
              "  'acc': 70.27},\n",
              " {'epoch': 11,\n",
              "  'train_loss': 295.1024594902992,\n",
              "  'train_acc': 73.974,\n",
              "  'loss': 68.32917627692223,\n",
              "  'acc': 76.86},\n",
              " {'epoch': 12,\n",
              "  'train_loss': 287.4972376227379,\n",
              "  'train_acc': 74.72,\n",
              "  'loss': 65.69839265942574,\n",
              "  'acc': 77.35},\n",
              " {'epoch': 13,\n",
              "  'train_loss': 281.67764607071877,\n",
              "  'train_acc': 75.3,\n",
              "  'loss': 75.40403607487679,\n",
              "  'acc': 75.09},\n",
              " {'epoch': 14,\n",
              "  'train_loss': 280.0892382860184,\n",
              "  'train_acc': 75.198,\n",
              "  'loss': 78.22349175810814,\n",
              "  'acc': 73.27},\n",
              " {'epoch': 15,\n",
              "  'train_loss': 271.89180263876915,\n",
              "  'train_acc': 75.85,\n",
              "  'loss': 94.26444393396378,\n",
              "  'acc': 71.44},\n",
              " {'epoch': 16,\n",
              "  'train_loss': 264.6638306379318,\n",
              "  'train_acc': 76.582,\n",
              "  'loss': 78.96824687719345,\n",
              "  'acc': 73.0},\n",
              " {'epoch': 17,\n",
              "  'train_loss': 258.00521370768547,\n",
              "  'train_acc': 77.042,\n",
              "  'loss': 81.85121387243271,\n",
              "  'acc': 74.07},\n",
              " {'epoch': 18,\n",
              "  'train_loss': 251.63884815573692,\n",
              "  'train_acc': 77.78,\n",
              "  'loss': 94.05693364143372,\n",
              "  'acc': 70.42},\n",
              " {'epoch': 19,\n",
              "  'train_loss': 245.31524714827538,\n",
              "  'train_acc': 78.31,\n",
              "  'loss': 71.65650033950806,\n",
              "  'acc': 76.22},\n",
              " {'epoch': 20,\n",
              "  'train_loss': 240.43961921334267,\n",
              "  'train_acc': 78.954,\n",
              "  'loss': 67.78455439209938,\n",
              "  'acc': 78.77},\n",
              " {'epoch': 21,\n",
              "  'train_loss': 234.90850976109505,\n",
              "  'train_acc': 79.226,\n",
              "  'loss': 75.9235465824604,\n",
              "  'acc': 75.34},\n",
              " {'epoch': 22,\n",
              "  'train_loss': 229.67679852247238,\n",
              "  'train_acc': 79.652,\n",
              "  'loss': 62.795541524887085,\n",
              "  'acc': 79.91},\n",
              " {'epoch': 23,\n",
              "  'train_loss': 223.76602506637573,\n",
              "  'train_acc': 80.206,\n",
              "  'loss': 60.756801038980484,\n",
              "  'acc': 79.22},\n",
              " {'epoch': 24,\n",
              "  'train_loss': 216.51522994041443,\n",
              "  'train_acc': 80.79,\n",
              "  'loss': 64.70525580644608,\n",
              "  'acc': 78.71},\n",
              " {'epoch': 25,\n",
              "  'train_loss': 212.19084921479225,\n",
              "  'train_acc': 81.286,\n",
              "  'loss': 60.04075738787651,\n",
              "  'acc': 79.87},\n",
              " {'epoch': 26,\n",
              "  'train_loss': 204.96553593873978,\n",
              "  'train_acc': 81.864,\n",
              "  'loss': 57.327406108379364,\n",
              "  'acc': 81.77},\n",
              " {'epoch': 27,\n",
              "  'train_loss': 199.871346950531,\n",
              "  'train_acc': 82.402,\n",
              "  'loss': 60.44831410050392,\n",
              "  'acc': 79.72},\n",
              " {'epoch': 28,\n",
              "  'train_loss': 194.64086565375328,\n",
              "  'train_acc': 82.806,\n",
              "  'loss': 50.15753731131554,\n",
              "  'acc': 83.13},\n",
              " {'epoch': 29,\n",
              "  'train_loss': 188.3463372439146,\n",
              "  'train_acc': 83.22,\n",
              "  'loss': 58.99103108048439,\n",
              "  'acc': 80.12},\n",
              " {'epoch': 30,\n",
              "  'train_loss': 180.94015794992447,\n",
              "  'train_acc': 83.938,\n",
              "  'loss': 38.15060582756996,\n",
              "  'acc': 86.92},\n",
              " {'epoch': 31,\n",
              "  'train_loss': 175.48647716641426,\n",
              "  'train_acc': 84.448,\n",
              "  'loss': 42.540227845311165,\n",
              "  'acc': 85.94},\n",
              " {'epoch': 32,\n",
              "  'train_loss': 170.44659201800823,\n",
              "  'train_acc': 84.812,\n",
              "  'loss': 44.465014934539795,\n",
              "  'acc': 85.29},\n",
              " {'epoch': 33,\n",
              "  'train_loss': 165.44948069751263,\n",
              "  'train_acc': 85.374,\n",
              "  'loss': 43.63767430186272,\n",
              "  'acc': 85.15},\n",
              " {'epoch': 34,\n",
              "  'train_loss': 158.86690957844257,\n",
              "  'train_acc': 85.898,\n",
              "  'loss': 53.85048881173134,\n",
              "  'acc': 83.04},\n",
              " {'epoch': 35,\n",
              "  'train_loss': 154.87540428340435,\n",
              "  'train_acc': 86.246,\n",
              "  'loss': 39.88396769762039,\n",
              "  'acc': 86.62},\n",
              " {'epoch': 36,\n",
              "  'train_loss': 149.15163958072662,\n",
              "  'train_acc': 86.86,\n",
              "  'loss': 35.54605996608734,\n",
              "  'acc': 87.68},\n",
              " {'epoch': 37,\n",
              "  'train_loss': 142.92334270477295,\n",
              "  'train_acc': 87.412,\n",
              "  'loss': 38.27856756746769,\n",
              "  'acc': 86.99},\n",
              " {'epoch': 38,\n",
              "  'train_loss': 138.7407031059265,\n",
              "  'train_acc': 87.728,\n",
              "  'loss': 35.043267503380775,\n",
              "  'acc': 88.47},\n",
              " {'epoch': 39,\n",
              "  'train_loss': 136.36564065515995,\n",
              "  'train_acc': 87.846,\n",
              "  'loss': 35.78306883573532,\n",
              "  'acc': 88.3},\n",
              " {'epoch': 40,\n",
              "  'train_loss': 128.22218723595142,\n",
              "  'train_acc': 88.572,\n",
              "  'loss': 30.021601274609566,\n",
              "  'acc': 89.9},\n",
              " {'epoch': 41,\n",
              "  'train_loss': 123.65468882024288,\n",
              "  'train_acc': 89.05,\n",
              "  'loss': 33.20254376530647,\n",
              "  'acc': 89.01},\n",
              " {'epoch': 42,\n",
              "  'train_loss': 119.40172852575779,\n",
              "  'train_acc': 89.37,\n",
              "  'loss': 34.89013160765171,\n",
              "  'acc': 88.71},\n",
              " {'epoch': 43,\n",
              "  'train_loss': 114.83898785710335,\n",
              "  'train_acc': 89.764,\n",
              "  'loss': 34.3774391785264,\n",
              "  'acc': 88.84},\n",
              " {'epoch': 44,\n",
              "  'train_loss': 108.44792911410332,\n",
              "  'train_acc': 90.27,\n",
              "  'loss': 28.022722877562046,\n",
              "  'acc': 90.72},\n",
              " {'epoch': 45,\n",
              "  'train_loss': 106.37337556481361,\n",
              "  'train_acc': 90.58,\n",
              "  'loss': 26.42863517254591,\n",
              "  'acc': 91.35},\n",
              " {'epoch': 46,\n",
              "  'train_loss': 102.38227233290672,\n",
              "  'train_acc': 90.858,\n",
              "  'loss': 30.987299293279648,\n",
              "  'acc': 90.07},\n",
              " {'epoch': 47,\n",
              "  'train_loss': 97.59058793634176,\n",
              "  'train_acc': 91.362,\n",
              "  'loss': 25.522725239396095,\n",
              "  'acc': 91.65},\n",
              " {'epoch': 48,\n",
              "  'train_loss': 92.82399659603834,\n",
              "  'train_acc': 91.736,\n",
              "  'loss': 25.45881725847721,\n",
              "  'acc': 91.74},\n",
              " {'epoch': 49,\n",
              "  'train_loss': 91.53302105516195,\n",
              "  'train_acc': 91.816,\n",
              "  'loss': 28.90867704898119,\n",
              "  'acc': 90.81},\n",
              " {'epoch': 50,\n",
              "  'train_loss': 84.91942705214024,\n",
              "  'train_acc': 92.452,\n",
              "  'loss': 25.496378526091576,\n",
              "  'acc': 92.16},\n",
              " {'epoch': 51,\n",
              "  'train_loss': 81.54751116782427,\n",
              "  'train_acc': 92.772,\n",
              "  'loss': 24.12046457082033,\n",
              "  'acc': 92.23},\n",
              " {'epoch': 52,\n",
              "  'train_loss': 78.19618439674377,\n",
              "  'train_acc': 93.026,\n",
              "  'loss': 24.495963007211685,\n",
              "  'acc': 92.12},\n",
              " {'epoch': 53,\n",
              "  'train_loss': 75.00710986182094,\n",
              "  'train_acc': 93.37,\n",
              "  'loss': 23.212439574301243,\n",
              "  'acc': 92.73},\n",
              " {'epoch': 54,\n",
              "  'train_loss': 73.11572906374931,\n",
              "  'train_acc': 93.434,\n",
              "  'loss': 25.023869276046753,\n",
              "  'acc': 92.63},\n",
              " {'epoch': 55,\n",
              "  'train_loss': 68.57173947244883,\n",
              "  'train_acc': 93.93,\n",
              "  'loss': 21.574408560991287,\n",
              "  'acc': 93.31},\n",
              " {'epoch': 56,\n",
              "  'train_loss': 66.48064863309264,\n",
              "  'train_acc': 94.144,\n",
              "  'loss': 21.594503670930862,\n",
              "  'acc': 93.16},\n",
              " {'epoch': 57,\n",
              "  'train_loss': 63.750387877225876,\n",
              "  'train_acc': 94.18,\n",
              "  'loss': 21.48833091557026,\n",
              "  'acc': 93.44},\n",
              " {'epoch': 58,\n",
              "  'train_loss': 60.735757905989885,\n",
              "  'train_acc': 94.62,\n",
              "  'loss': 24.419834703207016,\n",
              "  'acc': 92.78},\n",
              " {'epoch': 59,\n",
              "  'train_loss': 58.03293234668672,\n",
              "  'train_acc': 94.908,\n",
              "  'loss': 21.755030311644077,\n",
              "  'acc': 93.46},\n",
              " {'epoch': 60,\n",
              "  'train_loss': 53.86719523742795,\n",
              "  'train_acc': 95.222,\n",
              "  'loss': 23.144079133868217,\n",
              "  'acc': 93.08},\n",
              " {'epoch': 61,\n",
              "  'train_loss': 53.972158446908,\n",
              "  'train_acc': 95.132,\n",
              "  'loss': 19.754779256880283,\n",
              "  'acc': 93.99},\n",
              " {'epoch': 62,\n",
              "  'train_loss': 51.23151122033596,\n",
              "  'train_acc': 95.452,\n",
              "  'loss': 20.359240047633648,\n",
              "  'acc': 93.85},\n",
              " {'epoch': 63,\n",
              "  'train_loss': 47.32641971856356,\n",
              "  'train_acc': 95.848,\n",
              "  'loss': 19.53351991251111,\n",
              "  'acc': 93.97},\n",
              " {'epoch': 64,\n",
              "  'train_loss': 46.20965064689517,\n",
              "  'train_acc': 95.926,\n",
              "  'loss': 21.242604319006205,\n",
              "  'acc': 93.87},\n",
              " {'epoch': 65,\n",
              "  'train_loss': 44.43762354180217,\n",
              "  'train_acc': 96.126,\n",
              "  'loss': 20.38332138210535,\n",
              "  'acc': 94.06},\n",
              " {'epoch': 66,\n",
              "  'train_loss': 41.08667369186878,\n",
              "  'train_acc': 96.404,\n",
              "  'loss': 22.153285827487707,\n",
              "  'acc': 93.81},\n",
              " {'epoch': 67,\n",
              "  'train_loss': 40.79927361011505,\n",
              "  'train_acc': 96.37,\n",
              "  'loss': 20.84742197394371,\n",
              "  'acc': 93.98},\n",
              " {'epoch': 68,\n",
              "  'train_loss': 39.64173465967178,\n",
              "  'train_acc': 96.536,\n",
              "  'loss': 20.524369411170483,\n",
              "  'acc': 94.03},\n",
              " {'epoch': 69,\n",
              "  'train_loss': 37.92313813790679,\n",
              "  'train_acc': 96.704,\n",
              "  'loss': 20.480248659849167,\n",
              "  'acc': 93.91},\n",
              " {'epoch': 70,\n",
              "  'train_loss': 36.48559933900833,\n",
              "  'train_acc': 96.822,\n",
              "  'loss': 19.963090676814318,\n",
              "  'acc': 94.29},\n",
              " {'epoch': 71,\n",
              "  'train_loss': 34.31297986395657,\n",
              "  'train_acc': 97.03,\n",
              "  'loss': 19.80246415734291,\n",
              "  'acc': 94.44},\n",
              " {'epoch': 72,\n",
              "  'train_loss': 35.036303075030446,\n",
              "  'train_acc': 96.95,\n",
              "  'loss': 20.067306831479073,\n",
              "  'acc': 94.31},\n",
              " {'epoch': 73,\n",
              "  'train_loss': 33.17550626397133,\n",
              "  'train_acc': 97.162,\n",
              "  'loss': 20.037591498345137,\n",
              "  'acc': 94.35},\n",
              " {'epoch': 74,\n",
              "  'train_loss': 31.534218225628138,\n",
              "  'train_acc': 97.244,\n",
              "  'loss': 19.855324421077967,\n",
              "  'acc': 94.3},\n",
              " {'epoch': 75,\n",
              "  'train_loss': 32.01982982456684,\n",
              "  'train_acc': 97.188,\n",
              "  'loss': 19.95951084420085,\n",
              "  'acc': 94.46},\n",
              " {'epoch': 76,\n",
              "  'train_loss': 29.761041399091482,\n",
              "  'train_acc': 97.396,\n",
              "  'loss': 20.00375633686781,\n",
              "  'acc': 94.38},\n",
              " {'epoch': 77,\n",
              "  'train_loss': 29.598727371543646,\n",
              "  'train_acc': 97.458,\n",
              "  'loss': 20.209928046911955,\n",
              "  'acc': 94.32},\n",
              " {'epoch': 78,\n",
              "  'train_loss': 28.85201194509864,\n",
              "  'train_acc': 97.5,\n",
              "  'loss': 19.417937647551298,\n",
              "  'acc': 94.64},\n",
              " {'epoch': 79,\n",
              "  'train_loss': 27.52188055217266,\n",
              "  'train_acc': 97.638,\n",
              "  'loss': 20.07254132628441,\n",
              "  'acc': 94.51},\n",
              " {'epoch': 80,\n",
              "  'train_loss': 26.949739089235663,\n",
              "  'train_acc': 97.666,\n",
              "  'loss': 20.523633308708668,\n",
              "  'acc': 94.3},\n",
              " {'epoch': 81,\n",
              "  'train_loss': 27.287329031154513,\n",
              "  'train_acc': 97.64,\n",
              "  'loss': 20.47282698005438,\n",
              "  'acc': 94.45},\n",
              " {'epoch': 82,\n",
              "  'train_loss': 26.864581767469645,\n",
              "  'train_acc': 97.656,\n",
              "  'loss': 20.356502521783113,\n",
              "  'acc': 94.41},\n",
              " {'epoch': 83,\n",
              "  'train_loss': 24.70988541841507,\n",
              "  'train_acc': 97.872,\n",
              "  'loss': 20.502027813345194,\n",
              "  'acc': 94.32},\n",
              " {'epoch': 84,\n",
              "  'train_loss': 25.37227918021381,\n",
              "  'train_acc': 97.898,\n",
              "  'loss': 20.41370415315032,\n",
              "  'acc': 94.43},\n",
              " {'epoch': 85,\n",
              "  'train_loss': 24.717292906716466,\n",
              "  'train_acc': 97.888,\n",
              "  'loss': 19.961603667587042,\n",
              "  'acc': 94.62},\n",
              " {'epoch': 86,\n",
              "  'train_loss': 23.330789199098945,\n",
              "  'train_acc': 98.056,\n",
              "  'loss': 20.451539799571037,\n",
              "  'acc': 94.49},\n",
              " {'epoch': 87,\n",
              "  'train_loss': 23.76170135103166,\n",
              "  'train_acc': 97.966,\n",
              "  'loss': 20.12364847585559,\n",
              "  'acc': 94.53},\n",
              " {'epoch': 88,\n",
              "  'train_loss': 23.489305537194014,\n",
              "  'train_acc': 97.952,\n",
              "  'loss': 20.716168001294136,\n",
              "  'acc': 94.6},\n",
              " {'epoch': 89,\n",
              "  'train_loss': 22.971767948940396,\n",
              "  'train_acc': 98.042,\n",
              "  'loss': 20.09196463599801,\n",
              "  'acc': 94.57},\n",
              " {'epoch': 90,\n",
              "  'train_loss': 21.879804531112313,\n",
              "  'train_acc': 98.13,\n",
              "  'loss': 20.74242278933525,\n",
              "  'acc': 94.42},\n",
              " {'epoch': 91,\n",
              "  'train_loss': 22.351364836096764,\n",
              "  'train_acc': 98.128,\n",
              "  'loss': 20.309488777071238,\n",
              "  'acc': 94.44},\n",
              " {'epoch': 92,\n",
              "  'train_loss': 22.70437694899738,\n",
              "  'train_acc': 98.02,\n",
              "  'loss': 20.474807541817427,\n",
              "  'acc': 94.52},\n",
              " {'epoch': 93,\n",
              "  'train_loss': 21.98940627090633,\n",
              "  'train_acc': 98.144,\n",
              "  'loss': 20.276823595166206,\n",
              "  'acc': 94.54},\n",
              " {'epoch': 94,\n",
              "  'train_loss': 23.13129918463528,\n",
              "  'train_acc': 98.008,\n",
              "  'loss': 20.933124992996454,\n",
              "  'acc': 94.51},\n",
              " {'epoch': 95,\n",
              "  'train_loss': 21.944106662645936,\n",
              "  'train_acc': 98.132,\n",
              "  'loss': 20.379710882902145,\n",
              "  'acc': 94.64},\n",
              " {'epoch': 96,\n",
              "  'train_loss': 22.127670703455806,\n",
              "  'train_acc': 98.104,\n",
              "  'loss': 20.34023194387555,\n",
              "  'acc': 94.58},\n",
              " {'epoch': 97,\n",
              "  'train_loss': 21.292805939912796,\n",
              "  'train_acc': 98.212,\n",
              "  'loss': 20.245772939175367,\n",
              "  'acc': 94.67},\n",
              " {'epoch': 98,\n",
              "  'train_loss': 21.899103125557303,\n",
              "  'train_acc': 98.062,\n",
              "  'loss': 20.1459485180676,\n",
              "  'acc': 94.66},\n",
              " {'epoch': 99,\n",
              "  'train_loss': 22.581811985000968,\n",
              "  'train_acc': 98.082,\n",
              "  'loss': 20.259088438004255,\n",
              "  'acc': 94.61},\n",
              " {'epoch': 100,\n",
              "  'train_loss': 22.166775664314628,\n",
              "  'train_acc': 98.116,\n",
              "  'loss': 20.203341081738472,\n",
              "  'acc': 94.53},\n",
              " {'epoch': 101,\n",
              "  'train_loss': 21.575932003557682,\n",
              "  'train_acc': 98.154,\n",
              "  'loss': 20.720015674829483,\n",
              "  'acc': 94.38},\n",
              " {'epoch': 102,\n",
              "  'train_loss': 21.44084707275033,\n",
              "  'train_acc': 98.112,\n",
              "  'loss': 19.995714470744133,\n",
              "  'acc': 94.67},\n",
              " {'epoch': 103,\n",
              "  'train_loss': 20.660856567323208,\n",
              "  'train_acc': 98.242,\n",
              "  'loss': 20.289707750082016,\n",
              "  'acc': 94.64},\n",
              " {'epoch': 104,\n",
              "  'train_loss': 21.513843407854438,\n",
              "  'train_acc': 98.2,\n",
              "  'loss': 20.230567198246717,\n",
              "  'acc': 94.69},\n",
              " {'epoch': 105,\n",
              "  'train_loss': 22.182401878759265,\n",
              "  'train_acc': 98.142,\n",
              "  'loss': 20.23087851330638,\n",
              "  'acc': 94.68},\n",
              " {'epoch': 106,\n",
              "  'train_loss': 21.77024850808084,\n",
              "  'train_acc': 98.146,\n",
              "  'loss': 20.081487104296684,\n",
              "  'acc': 94.72},\n",
              " {'epoch': 107,\n",
              "  'train_loss': 20.370955986902118,\n",
              "  'train_acc': 98.296,\n",
              "  'loss': 20.24106700718403,\n",
              "  'acc': 94.67},\n",
              " {'epoch': 108,\n",
              "  'train_loss': 21.607679257169366,\n",
              "  'train_acc': 98.176,\n",
              "  'loss': 20.315171789377928,\n",
              "  'acc': 94.63},\n",
              " {'epoch': 109,\n",
              "  'train_loss': 21.645886505022645,\n",
              "  'train_acc': 98.19,\n",
              "  'loss': 20.813805993646383,\n",
              "  'acc': 94.56},\n",
              " {'epoch': 110,\n",
              "  'train_loss': 20.99921192973852,\n",
              "  'train_acc': 98.264,\n",
              "  'loss': 20.50289072841406,\n",
              "  'acc': 94.68},\n",
              " {'epoch': 111,\n",
              "  'train_loss': 20.661783035844564,\n",
              "  'train_acc': 98.236,\n",
              "  'loss': 20.414362780749798,\n",
              "  'acc': 94.71},\n",
              " {'epoch': 112,\n",
              "  'train_loss': 20.90364129282534,\n",
              "  'train_acc': 98.212,\n",
              "  'loss': 20.484193336218596,\n",
              "  'acc': 94.57},\n",
              " {'epoch': 113,\n",
              "  'train_loss': 21.956725258380175,\n",
              "  'train_acc': 98.106,\n",
              "  'loss': 20.704868871718645,\n",
              "  'acc': 94.6},\n",
              " {'epoch': 114,\n",
              "  'train_loss': 20.60666592232883,\n",
              "  'train_acc': 98.244,\n",
              "  'loss': 20.192240923643112,\n",
              "  'acc': 94.68},\n",
              " {'epoch': 115,\n",
              "  'train_loss': 20.535031436011195,\n",
              "  'train_acc': 98.326,\n",
              "  'loss': 20.386975217610598,\n",
              "  'acc': 94.71},\n",
              " {'epoch': 116, 'acc': 94.72},\n",
              " {'epoch': 116,\n",
              "  'train_loss': 20.362896433100104,\n",
              "  'train_acc': 98.264,\n",
              "  'loss': 19.973237723112106,\n",
              "  'acc': 94.8},\n",
              " {'epoch': 117,\n",
              "  'train_loss': 21.46388584189117,\n",
              "  'train_acc': 98.182,\n",
              "  'loss': 20.030115962028503,\n",
              "  'acc': 94.79},\n",
              " {'epoch': 118,\n",
              "  'train_loss': 20.753612684085965,\n",
              "  'train_acc': 98.158,\n",
              "  'loss': 20.251006454229355,\n",
              "  'acc': 94.7},\n",
              " {'epoch': 119,\n",
              "  'train_loss': 19.95198694244027,\n",
              "  'train_acc': 98.33,\n",
              "  'loss': 20.087826397269964,\n",
              "  'acc': 94.77},\n",
              " {'epoch': 120,\n",
              "  'train_loss': 20.72102808393538,\n",
              "  'train_acc': 98.246,\n",
              "  'loss': 20.18049632012844,\n",
              "  'acc': 94.74},\n",
              " {'epoch': 121,\n",
              "  'train_loss': 20.436268098652363,\n",
              "  'train_acc': 98.276,\n",
              "  'loss': 20.031006831675768,\n",
              "  'acc': 94.64},\n",
              " {'epoch': 122,\n",
              "  'train_loss': 20.222248420119286,\n",
              "  'train_acc': 98.342,\n",
              "  'loss': 20.101380709558725,\n",
              "  'acc': 94.67}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynzOgqVMuICK",
        "colab_type": "code",
        "colab": {},
        "outputId": "1404db2c-2218-4b94-bc9a-117c2a497a48"
      },
      "source": [
        "print(lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZdY_Q8kuICN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYW3Vt6uICQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf6nUgErY6Bh"
   },
   "source": [
    "# ResNet with a \"twist\"\n",
    "\n",
    "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
    "\n",
    "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning typically includes basics of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
    "\n",
    "* Code based off of https://github.com/kuangliu/pytorch-cifar\n",
    "\n",
    "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPeChzrK7iYC"
   },
   "source": [
    "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [blog post on Observable](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations).\n",
    "\n",
    "neural network | heat equation\n",
    ":----:|:-------:\n",
    "input layer | initial condition\n",
    "feed forward | solving the equation\n",
    "hidden layers | solution at intermediate times\n",
    "output layer | solution at final time\n",
    "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
    "weights | coefficients\n",
    "boundary handling (padding) | boundary condition\n",
    "multiple channels/filters/feature maps | system of (coupled) PDEs\n",
    "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
    "16×16×1×1 kernel | 16×16 matrix of constants\n",
    "\n",
    "\n",
    "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply trying to make it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should (in theory) enable the neural net to learn more ways to deform the input than diffusion and translation (e.g., rotation and scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lmUdhteH5N9s",
    "outputId": "fdb9e1a1-a178-4b65-a778-468c9263b8c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Testing on a random input:\n",
      "INPUT  torch.Size([1, 3, 32, 32])\n",
      "OUTPUT torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "ResNet in PyTorch, forked from https://github.com/kuangliu/pytorch-cifar\n",
    "Reference:\n",
    "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.match = stride == 1 and in_channels == self.expansion * channels\n",
    "        self.twist = False\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.conv1x = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv1y = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if not self.match:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x1 = self.conv1(x)\n",
    "        if self.twist:\n",
    "            _, c, h, w = tuple(x1.shape)\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv1x.weight.data = (self.conv1x.weight - self.conv1x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv1y.weight.data = (self.conv1y.weight - self.conv1y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv1y.weight.data = self.conv1x.weight.transpose(2,3).flip(2)\n",
    "            if self.XX is None:\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "                # print(\"twist initialized, self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "            x1 = self.conv1(x) + self.XX * self.conv1x(x) + self.YY * self.conv1y(x)\n",
    "            # print(\"twist initialized, outside self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "        \n",
    "        x2 = F.relu(self.bn2(x1))\n",
    "        if self.twist:\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(2)\n",
    "            x3 = self.conv2(x2) + self.XX * self.conv2x(x2) + self.YY * self.conv2y(x2)\n",
    "        else:\n",
    "            x3 = self.conv2(x2)\n",
    "        x3 += self.shortcut(x)\n",
    "        return x3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.twist = False\n",
    "        self.channels = channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        if self.twist: \n",
    "            # symmetrize the kernels (force it to be a 1st-order diff op, i.e. a vector field)\n",
    "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
    "            self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            # make y-vector perpendicular to x-vector\n",
    "            # self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(3)\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.twist:\n",
    "            if self.XX is None: # initialize self.XY\n",
    "                _, c, h, w = tuple(x2.shape)\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "            x2 += self.XX * self.conv2x(x1) + self.YY * self.conv2y(x1)\n",
    "        x3 = F.relu(self.bn2(x2))\n",
    "        x4 = self.conv3(x3)\n",
    "        x4 = self.bn3(x4)\n",
    "        x4 += self.shortcut(x)\n",
    "        x4 = F.relu(x4)\n",
    "        return x4\n",
    "\n",
    "\n",
    "class PDEBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(PDEBlock, self).__init__()\n",
    "        self.twist = False\n",
    "        self.match = in_channels == channels and stride == 1\n",
    "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.convx = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.convy = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if not self.match:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                self.conv,\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.XX is None:\n",
    "            _, _, h, w = tuple(x.shape)\n",
    "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
    "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
    "            # self.conv.weight.data = self.orthogonal(self.conv.weight)\n",
    "            # self.convx.weight.data = self.orthogonal(self.convx.weight)\n",
    "        \n",
    "        if self.twist and self.match:\n",
    "            # symmetrize kernels\n",
    "            self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3)) / 2\n",
    "            # self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3)) / 2\n",
    "            self.convy.weight.data = self.convx.weight.transpose(2,3).flip(2)\n",
    "            for i in range(2):\n",
    "                x = self.Euler_step(x)\n",
    "        else:\n",
    "            x = self.Euler_step(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def Euler_step(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        if self.twist and self.match:\n",
    "            x1 += self.XX * self.convx(x) + self.YY * self.convy(x)\n",
    "        x1 += self.shortcut(x)\n",
    "        return x1\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        channels = [self.in_channels * i for i in [1, 2, 4, 4]]\n",
    "        self.num_blocks = num_blocks\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, channels[3], num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(channels[2] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        if num_blocks == 0:\n",
    "            layers = [nn.Conv2d(self.in_channels, channels, kernel_size=1, stride=stride, padding=1, bias=False),\n",
    "                      nn.BatchNorm2d(channels)]\n",
    "            self.in_channels = channels * block.expansion\n",
    "        else:\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            layers = []\n",
    "            for idx, stride in enumerate(strides):\n",
    "                # twist = twist and idx < 3\n",
    "                layers.append(block(self.in_channels, channels, stride))\n",
    "                self.in_channels = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        x = F.avg_pool2d(x, 8)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "# net = ResNet50()\n",
    "net = ResNet(BasicBlock, [17,17,17])\n",
    "epoch = 0 \n",
    "lr = 0.1\n",
    "checkpoint = {'acc': 0, 'epoch': 0}\n",
    "history = [{'acc': 0, 'epoch': 0}]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.to(device)\n",
    "print('Testing on a random input:')\n",
    "test = torch.randn(1,3,32,32).to(device)\n",
    "print('INPUT ', test.shape)\n",
    "print('OUTPUT', net(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ozKUGgKwcruw",
    "outputId": "f35c1191-0bfb-4436-be1f-2f2f5622f6fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.3,0.3), scale=(0.8,1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1RzI4cWWHVlg",
    "outputId": "182f6eb5-1750-463d-a07d-c6dcd15ed0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twist on\n",
      "testing on random initial weights:\n",
      "test  loss: 2.304 | acc: 10.00  ( 1000/10000) (up by 10.00)\n",
      "Epoch 0 (lr=0.1000)\n",
      "train loss: 1.968 | acc: 23.902 (11951/50000)\n",
      "test  loss: 1.912 | acc: 26.57  ( 2657/10000) (up by 16.57)\n",
      "Epoch 1 (lr=0.1000)\n",
      "train loss: 1.773 | acc: 31.778 (15889/50000)\n",
      "test  loss: 1.665 | acc: 38.22  ( 3822/10000) (up by 11.65)\n",
      "Epoch 2 (lr=0.1000)\n",
      "train loss: 1.607 | acc: 39.868 (19934/50000)\n",
      "test  loss: 1.537 | acc: 47.19  ( 4719/10000) (up by 8.97)\n",
      "Epoch 3 (lr=0.1000)\n",
      "train loss: 1.407 | acc: 48.756 (24378/50000)\n",
      "test  loss: 1.642 | acc: 45.00  ( 4500/10000)\n",
      "Epoch 4 (lr=0.1000)\n",
      "train loss: 1.249 | acc: 55.036 (27518/50000)\n",
      "test  loss: 1.193 | acc: 57.26  ( 5726/10000) (up by 10.07)\n",
      "Epoch 5 (lr=0.1000)\n",
      "train loss: 1.142 | acc: 59.404 (29702/50000)\n",
      "test  loss: 1.238 | acc: 56.30  ( 5630/10000)\n",
      "Epoch 6 (lr=0.1000)\n",
      "train loss: 1.045 | acc: 63.176 (31588/50000)\n",
      "test  loss: 1.153 | acc: 63.14  ( 6314/10000) (up by 5.88)\n",
      "Epoch 7 (lr=0.1000)\n",
      "train loss: 0.983 | acc: 65.672 (32836/50000)\n",
      "test  loss: 1.069 | acc: 64.15  ( 6415/10000) (up by 1.01)\n",
      "Epoch 8 (lr=0.1000)\n",
      "train loss: 0.932 | acc: 67.696 (33848/50000)\n",
      "test  loss: 1.051 | acc: 64.74  ( 6474/10000) (up by 0.59)\n",
      "Epoch 9 (lr=0.1000)\n",
      "train loss: 0.902 | acc: 68.700 (34350/50000)\n",
      "test  loss: 1.342 | acc: 59.63  ( 5963/10000)\n",
      "Epoch 10 (lr=0.1000)\n",
      "train loss: 0.875 | acc: 69.616 (34808/50000)\n",
      "test  loss: 0.976 | acc: 66.82  ( 6682/10000) (up by 2.08)\n",
      "Epoch 11 (lr=0.1000)\n",
      "train loss: 0.853 | acc: 70.402 (35201/50000)\n",
      "test  loss: 1.461 | acc: 60.44  ( 6044/10000)\n",
      "Epoch 12 (lr=0.1000)\n",
      "train loss: 0.838 | acc: 70.812 (35406/50000)\n",
      "test  loss: 0.797 | acc: 72.30  ( 7230/10000) (up by 5.48)\n",
      "Epoch 13 (lr=0.1000)\n",
      "train loss: 0.811 | acc: 71.918 (35959/50000)\n",
      "test  loss: 0.926 | acc: 69.54  ( 6954/10000)\n",
      "Epoch 14 (lr=0.1000)\n",
      "train loss: 0.803 | acc: 72.220 (36110/50000)\n",
      "test  loss: 1.177 | acc: 64.15  ( 6415/10000)\n",
      "Epoch 15 (lr=0.1000)\n",
      "train loss: 0.793 | acc: 72.812 (36406/50000)\n",
      "test  loss: 1.046 | acc: 67.85  ( 6785/10000)\n",
      "Epoch 16 (lr=0.1000)\n",
      "train loss: 0.782 | acc: 72.992 (36496/50000)\n",
      "test  loss: 1.466 | acc: 59.35  ( 5935/10000)\n",
      "Epoch 17 (lr=0.1000)\n",
      "train loss: 0.776 | acc: 73.292 (36646/50000)\n",
      "test  loss: 1.358 | acc: 58.16  ( 5816/10000)\n",
      "Epoch 18 (lr=0.1000)\n",
      "train loss: 0.768 | acc: 73.496 (36748/50000)\n",
      "test  loss: 0.846 | acc: 72.73  ( 7273/10000) (up by 0.43)\n",
      "Epoch 19 (lr=0.1000)\n",
      "train loss: 0.754 | acc: 73.956 (36978/50000)\n",
      "test  loss: 1.227 | acc: 65.75  ( 6575/10000)\n",
      "Epoch 20 (lr=0.1000)\n",
      "train loss: 0.761 | acc: 73.804 (36902/50000)\n",
      "test  loss: 1.001 | acc: 70.05  ( 7005/10000)\n",
      "Epoch 21 (lr=0.1000)\n",
      "train loss: 0.754 | acc: 73.792 (36896/50000)\n",
      "test  loss: 1.693 | acc: 54.52  ( 5452/10000)\n",
      "Epoch 22 (lr=0.1000)\n",
      "train loss: 0.746 | acc: 74.482 (37241/50000)\n",
      "test  loss: 1.090 | acc: 66.83  ( 6683/10000)\n",
      "Epoch 23 (lr=0.1000)\n",
      "train loss: 0.734 | acc: 74.882 (37441/50000)\n",
      "test  loss: 0.909 | acc: 70.67  ( 7067/10000)\n",
      "Epoch 24 (lr=0.1000)\n",
      "train loss: 1.033 | acc: 63.906 (31953/50000)\n",
      "test  loss: 10.840 | acc: 10.00  ( 1000/10000)\n",
      "Epoch 25 (lr=0.1000)\n",
      "train loss: 0.903 | acc: 68.498 (34249/50000)\n",
      "test  loss: 1.468 | acc: 58.25  ( 5825/10000)\n",
      "Epoch 26 (lr=0.1000)\n",
      "train loss: 0.808 | acc: 71.940 (35970/50000)\n",
      "test  loss: 1.022 | acc: 66.45  ( 6645/10000)\n",
      "Epoch 27 (lr=0.1000)\n",
      "train loss: 0.792 | acc: 72.438 (36219/50000)\n",
      "test  loss: 0.820 | acc: 72.85  ( 7285/10000) (up by 0.12)\n",
      "Epoch 28 (lr=0.1000)\n",
      "train loss: 0.769 | acc: 73.328 (36664/50000)\n",
      "test  loss: 0.769 | acc: 73.19  ( 7319/10000) (up by 0.34)\n",
      "Epoch 29 (lr=0.1000)\n",
      "train loss: 0.764 | acc: 73.718 (36859/50000)\n",
      "test  loss: 1.798 | acc: 53.37  ( 5337/10000)\n",
      "Epoch 30 (lr=0.1000)\n",
      "train loss: 0.763 | acc: 73.886 (36943/50000)\n",
      "test  loss: 1.145 | acc: 64.74  ( 6474/10000)\n",
      "Epoch 31 (lr=0.1000)\n",
      "train loss: 0.767 | acc: 73.674 (36837/50000)\n",
      "test  loss: 1.087 | acc: 63.50  ( 6350/10000)\n",
      "Epoch 32 (lr=0.1000)\n",
      "train loss: 0.745 | acc: 74.184 (37092/50000)\n",
      "test  loss: 0.956 | acc: 69.04  ( 6904/10000)\n",
      "Epoch 33 (lr=0.1000)\n",
      "train loss: 0.767 | acc: 73.560 (36780/50000)\n",
      "test  loss: 0.831 | acc: 72.17  ( 7217/10000)\n",
      "Epoch 34 (lr=0.1000)\n",
      "train loss: 0.783 | acc: 73.032 (36516/50000)\n",
      "test  loss: 1.241 | acc: 60.83  ( 6083/10000)\n",
      "Epoch 35 (lr=0.1000)\n",
      "train loss: 0.790 | acc: 72.798 (36399/50000)\n",
      "test  loss: 0.868 | acc: 71.22  ( 7122/10000)\n",
      "Epoch 36 (lr=0.1000)\n",
      "train loss: 0.769 | acc: 73.444 (36722/50000)\n",
      "test  loss: 0.853 | acc: 71.25  ( 7125/10000)\n",
      "Epoch 37 (lr=0.1000)\n",
      "train loss: 0.803 | acc: 72.436 (36218/50000)\n",
      "test  loss: 22.269 | acc: 11.26  ( 1126/10000)\n",
      "Epoch 38 (lr=0.1000)\n",
      "train loss: 1.247 | acc: 54.932 (27466/50000)\n",
      "test  loss: 2.311 | acc: 25.64  ( 2564/10000)\n",
      "Epoch 39 (lr=0.1000)\n",
      "train loss: 1.328 | acc: 52.428 (26214/50000)\n",
      "test  loss: 1.308 | acc: 56.47  ( 5647/10000)\n",
      "Epoch 40 (lr=0.1000)\n",
      "train loss: 1.030 | acc: 64.130 (32065/50000)\n",
      "test  loss: 1.825 | acc: 48.26  ( 4826/10000)\n",
      "Epoch 41 (lr=0.1000)\n",
      "train loss: 1.130 | acc: 60.590 (30295/50000)\n",
      "test  loss: 2.563 | acc: 10.30  ( 1030/10000)\n",
      "Epoch 42 (lr=0.1000)\n",
      "train loss: 1.051 | acc: 62.996 (31498/50000)\n",
      "test  loss: 1.684 | acc: 51.96  ( 5196/10000)\n",
      "Epoch 43 (lr=0.1000)\n",
      "train loss: 1.142 | acc: 59.644 (29822/50000)\n",
      "test  loss: 48.553 | acc: 10.98  ( 1098/10000)\n",
      "Epoch 44 (lr=0.1000)\n",
      "train loss: 1.164 | acc: 58.878 (29439/50000)\n",
      "test  loss: 2.183 | acc: 36.53  ( 3653/10000)\n",
      "Epoch 45 (lr=0.1000)\n",
      "train loss: 1.089 | acc: 61.652 (30826/50000)\n",
      "test  loss: 2.078 | acc: 40.33  ( 4033/10000)\n",
      "Epoch 46 (lr=0.1000)\n",
      "train loss: 1.238 | acc: 56.026 (28013/50000)\n",
      "test  loss: 3.393 | acc: 8.80  ( 880/10000)\n",
      "Epoch 47 (lr=0.1000)\n",
      "train loss: 1.139 | acc: 60.052 (30026/50000)\n",
      "test  loss: 1.058 | acc: 63.82  ( 6382/10000)\n",
      "Epoch 48 (lr=0.1000)\n",
      "train loss: 1.103 | acc: 61.282 (30641/50000)\n",
      "test  loss: 1.680 | acc: 49.80  ( 4980/10000)\n",
      "Epoch 49 (lr=0.1000)\n",
      "train loss: 1.388 | acc: 50.380 (25190/50000)\n",
      "test  loss: 5131.466 | acc: 11.99  ( 1199/10000)\n",
      "Epoch 50 (lr=0.1000)\n",
      "train loss: 1.203 | acc: 57.724 (28862/50000)\n",
      "test  loss: 1.164 | acc: 60.81  ( 6081/10000)\n",
      "Epoch 51 (lr=0.1000)\n",
      "train loss: 1.108 | acc: 61.076 (30538/50000)\n",
      "test  loss: 2.046 | acc: 40.16  ( 4016/10000)\n",
      "Epoch 52 (lr=0.1000)\n",
      "train loss: 1.637 | acc: 39.310 (19655/50000)\n",
      "test  loss: 1.990 | acc: 25.59  ( 2559/10000)\n",
      "Epoch 53 (lr=0.1000)\n",
      "train loss: 1.800 | acc: 32.750 (16375/50000)\n",
      "test  loss: 2.142 | acc: 26.27  ( 2627/10000)\n",
      "Epoch 54 (lr=0.1000)\n",
      "train loss: 1.588 | acc: 42.210 (21105/50000)\n",
      "test  loss: 2.944 | acc: 11.01  ( 1101/10000)\n",
      "Epoch 55 (lr=0.1000)\n",
      "train loss: 1.632 | acc: 40.044 (20022/50000)\n",
      "test  loss: 1.323 | acc: 52.36  ( 5236/10000)\n",
      "Epoch 56 (lr=0.1000)\n",
      "train loss: 1.327 | acc: 52.810 (26405/50000)\n",
      "test  loss: 1.823 | acc: 47.63  ( 4763/10000)\n",
      "Epoch 57 (lr=0.1000)\n",
      "train loss: 1.336 | acc: 52.812 (26406/50000)\n",
      "test  loss: 1.651 | acc: 41.04  ( 4104/10000)\n",
      "\n",
      "learning rate downgraded to 0.010000000000000002 at epoch 58\n",
      "loading state_dict from Epoch 28 (acc = 73.19)\n",
      "Epoch 58 (lr=0.0100)\n",
      "train loss: 1.093 | acc: 61.486 (30743/50000)\n",
      "test  loss: 0.849 | acc: 70.11  ( 7011/10000)\n",
      "Epoch 59 (lr=0.0100)\n",
      "train loss: 0.997 | acc: 64.938 (32469/50000)\n",
      "test  loss: 0.769 | acc: 73.38  ( 7338/10000) (up by 0.19)\n",
      "Epoch 60 (lr=0.0100)\n",
      "train loss: 0.960 | acc: 66.046 (33023/50000)\n",
      "test  loss: 0.745 | acc: 74.25  ( 7425/10000) (up by 0.87)\n",
      "Epoch 61 (lr=0.0100)\n",
      "train loss: 0.921 | acc: 67.466 (33733/50000)\n",
      "test  loss: 0.756 | acc: 74.20  ( 7420/10000)\n",
      "Epoch 62 (lr=0.0100)\n",
      "train loss: 0.897 | acc: 68.508 (34254/50000)\n",
      "test  loss: 0.688 | acc: 75.93  ( 7593/10000) (up by 1.68)\n",
      "Epoch 63 (lr=0.0100)\n",
      "train loss: 0.881 | acc: 69.012 (34506/50000)\n",
      "test  loss: 0.740 | acc: 74.21  ( 7421/10000)\n",
      "Epoch 64 (lr=0.0100)\n",
      "train loss: 0.858 | acc: 69.694 (34847/50000)\n",
      "test  loss: 0.667 | acc: 76.64  ( 7664/10000) (up by 0.71)\n",
      "Epoch 65 (lr=0.0100)\n",
      "train loss: 0.838 | acc: 70.456 (35228/50000)\n",
      "test  loss: 0.668 | acc: 77.27  ( 7727/10000) (up by 0.63)\n",
      "Epoch 66 (lr=0.0100)\n",
      "train loss: 0.826 | acc: 70.958 (35479/50000)\n",
      "test  loss: 0.714 | acc: 76.40  ( 7640/10000)\n",
      "Epoch 67 (lr=0.0100)\n",
      "train loss: 0.822 | acc: 71.246 (35623/50000)\n",
      "test  loss: 0.650 | acc: 77.46  ( 7746/10000) (up by 0.19)\n",
      "Epoch 68 (lr=0.0100)\n",
      "train loss: 0.808 | acc: 71.710 (35855/50000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  loss: 0.639 | acc: 77.76  ( 7776/10000) (up by 0.30)\n",
      "Epoch 69 (lr=0.0100)\n",
      "train loss: 0.790 | acc: 72.348 (36174/50000)\n",
      "test  loss: 0.634 | acc: 77.97  ( 7797/10000) (up by 0.21)\n",
      "Epoch 70 (lr=0.0100)\n",
      "train loss: 0.771 | acc: 73.098 (36549/50000)\n",
      "test  loss: 0.725 | acc: 76.50  ( 7650/10000)\n",
      "Epoch 71 (lr=0.0100)\n",
      "train loss: 0.771 | acc: 73.074 (36537/50000)\n",
      "test  loss: 0.605 | acc: 78.50  ( 7850/10000) (up by 0.53)\n",
      "Epoch 72 (lr=0.0100)\n",
      "train loss: 0.755 | acc: 73.642 (36821/50000)\n",
      "test  loss: 0.665 | acc: 76.88  ( 7688/10000)\n",
      "Epoch 73 (lr=0.0100)\n",
      "train loss: 0.746 | acc: 73.796 (36898/50000)\n",
      "test  loss: 0.629 | acc: 78.03  ( 7803/10000)\n",
      "Epoch 74 (lr=0.0100)\n",
      "train loss: 0.742 | acc: 73.776 (36888/50000)\n",
      "test  loss: 0.645 | acc: 77.60  ( 7760/10000)\n",
      "Epoch 75 (lr=0.0100)\n",
      "train loss: 0.733 | acc: 74.458 (37229/50000)\n",
      "test  loss: 0.573 | acc: 80.93  ( 8093/10000) (up by 2.43)\n",
      "Epoch 76 (lr=0.0100)\n",
      "train loss: 0.716 | acc: 75.118 (37559/50000)\n",
      "test  loss: 0.615 | acc: 78.63  ( 7863/10000)\n",
      "Epoch 77 (lr=0.0100)\n",
      "train loss: 0.720 | acc: 75.116 (37558/50000)\n",
      "test  loss: 0.539 | acc: 81.68  ( 8168/10000) (up by 0.75)\n",
      "Epoch 78 (lr=0.0100)\n",
      "train loss: 0.702 | acc: 75.318 (37659/50000)\n",
      "test  loss: 0.671 | acc: 77.61  ( 7761/10000)\n",
      "Epoch 79 (lr=0.0100)\n",
      "train loss: 0.700 | acc: 75.454 (37727/50000)\n",
      "test  loss: 0.637 | acc: 78.55  ( 7855/10000)\n",
      "Epoch 80 (lr=0.0100)\n",
      "train loss: 0.703 | acc: 75.432 (37716/50000)\n",
      "test  loss: 0.587 | acc: 80.11  ( 8011/10000)\n",
      "Epoch 81 (lr=0.0100)\n",
      "train loss: 0.694 | acc: 75.944 (37972/50000)\n",
      "test  loss: 0.589 | acc: 79.90  ( 7990/10000)\n",
      "Epoch 82 (lr=0.0100)\n",
      "train loss: 0.681 | acc: 76.312 (38156/50000)\n",
      "test  loss: 0.613 | acc: 78.94  ( 7894/10000)\n",
      "Epoch 83 (lr=0.0100)\n",
      "train loss: 0.685 | acc: 76.242 (38121/50000)\n",
      "test  loss: 0.551 | acc: 81.05  ( 8105/10000)\n",
      "Epoch 84 (lr=0.0100)\n",
      "train loss: 0.679 | acc: 76.108 (38054/50000)\n",
      "test  loss: 0.628 | acc: 78.53  ( 7853/10000)\n",
      "Epoch 85 (lr=0.0100)\n",
      "train loss: 0.672 | acc: 76.642 (38321/50000)\n",
      "test  loss: 0.590 | acc: 80.20  ( 8020/10000)\n",
      "Epoch 86 (lr=0.0100)\n",
      "train loss: 0.671 | acc: 76.568 (38284/50000)\n",
      "test  loss: 0.512 | acc: 82.32  ( 8232/10000) (up by 0.64)\n",
      "Epoch 87 (lr=0.0100)\n",
      "train loss: 0.667 | acc: 76.802 (38401/50000)\n",
      "test  loss: 0.609 | acc: 79.48  ( 7948/10000)\n",
      "Epoch 88 (lr=0.0100)\n",
      "train loss: 0.661 | acc: 76.888 (38444/50000)\n",
      "test  loss: 0.551 | acc: 81.65  ( 8165/10000)\n",
      "Epoch 89 (lr=0.0100)\n",
      "train loss: 0.662 | acc: 76.992 (38496/50000)\n",
      "test  loss: 0.591 | acc: 80.31  ( 8031/10000)\n",
      "Epoch 90 (lr=0.0100)\n",
      "train loss: 0.649 | acc: 77.624 (38812/50000)\n",
      "test  loss: 0.568 | acc: 80.47  ( 8047/10000)\n",
      "Epoch 91 (lr=0.0100)\n",
      "train loss: 0.646 | acc: 77.410 (38705/50000)\n",
      "test  loss: 0.538 | acc: 82.33  ( 8233/10000) (up by 0.01)\n",
      "Epoch 92 (lr=0.0100)\n",
      "train loss: 0.644 | acc: 77.592 (38796/50000)\n",
      "test  loss: 0.552 | acc: 81.28  ( 8128/10000)\n",
      "Epoch 93 (lr=0.0100)\n",
      "train loss: 0.638 | acc: 77.726 (38863/50000)\n",
      "test  loss: 0.660 | acc: 78.59  ( 7859/10000)\n",
      "Epoch 94 (lr=0.0100)\n",
      "train loss: 0.635 | acc: 78.040 (39020/50000)\n",
      "test  loss: 0.525 | acc: 82.50  ( 8250/10000) (up by 0.17)\n",
      "Epoch 95 (lr=0.0100)\n",
      "train loss: 0.636 | acc: 77.698 (38849/50000)\n",
      "test  loss: 0.494 | acc: 83.03  ( 8303/10000) (up by 0.53)\n",
      "Epoch 96 (lr=0.0100)\n",
      "train loss: 0.625 | acc: 78.218 (39109/50000)\n",
      "test  loss: 0.526 | acc: 82.43  ( 8243/10000)\n",
      "Epoch 97 (lr=0.0100)\n",
      "train loss: 0.623 | acc: 78.234 (39117/50000)\n",
      "test  loss: 0.743 | acc: 77.11  ( 7711/10000)\n",
      "Epoch 98 (lr=0.0100)\n",
      "train loss: 0.621 | acc: 78.480 (39240/50000)\n",
      "test  loss: 0.668 | acc: 79.53  ( 7953/10000)\n",
      "Epoch 99 (lr=0.0100)\n",
      "train loss: 0.616 | acc: 78.546 (39273/50000)\n",
      "test  loss: 0.529 | acc: 81.82  ( 8182/10000)\n",
      "Epoch 100 (lr=0.0100)\n",
      "train loss: 0.616 | acc: 78.628 (39314/50000)\n",
      "test  loss: 0.566 | acc: 80.92  ( 8092/10000)\n",
      "Epoch 101 (lr=0.0100)\n",
      "train loss: 0.610 | acc: 78.858 (39429/50000)\n",
      "test  loss: 0.498 | acc: 83.19  ( 8319/10000) (up by 0.16)\n",
      "Epoch 102 (lr=0.0100)\n",
      "train loss: 0.603 | acc: 78.896 (39448/50000)\n",
      "test  loss: 0.492 | acc: 83.63  ( 8363/10000) (up by 0.44)\n",
      "Epoch 103 (lr=0.0100)\n",
      "train loss: 0.600 | acc: 79.074 (39537/50000)\n",
      "test  loss: 0.669 | acc: 78.97  ( 7897/10000)\n",
      "Epoch 104 (lr=0.0100)\n",
      "train loss: 0.600 | acc: 79.122 (39561/50000)\n",
      "test  loss: 0.548 | acc: 81.96  ( 8196/10000)\n",
      "Epoch 105 (lr=0.0100)\n",
      "train loss: 0.597 | acc: 79.310 (39655/50000)\n",
      "test  loss: 0.608 | acc: 80.22  ( 8022/10000)\n",
      "Epoch 106 (lr=0.0100)\n",
      "train loss: 0.601 | acc: 79.088 (39544/50000)\n",
      "test  loss: 0.647 | acc: 78.84  ( 7884/10000)\n",
      "Epoch 107 (lr=0.0100)\n",
      "train loss: 0.598 | acc: 79.128 (39564/50000)\n",
      "test  loss: 0.568 | acc: 81.24  ( 8124/10000)\n",
      "Epoch 108 (lr=0.0100)\n",
      "train loss: 0.593 | acc: 79.270 (39635/50000)\n",
      "test  loss: 0.637 | acc: 79.66  ( 7966/10000)\n",
      "Epoch 109 (lr=0.0100)\n",
      "train loss: 0.592 | acc: 79.468 (39734/50000)\n",
      "test  loss: 0.633 | acc: 79.41  ( 7941/10000)\n",
      "Epoch 110 (lr=0.0100)\n",
      "train loss: 0.584 | acc: 79.610 (39805/50000)\n",
      "test  loss: 0.518 | acc: 82.46  ( 8246/10000)\n",
      "Epoch 111 (lr=0.0100)\n",
      "train loss: 0.585 | acc: 79.622 (39811/50000)\n",
      "test  loss: 0.534 | acc: 82.61  ( 8261/10000)\n",
      "Epoch 112 (lr=0.0100)\n",
      "train loss: 0.596 | acc: 79.424 (39712/50000)\n",
      "test  loss: 0.686 | acc: 78.31  ( 7831/10000)\n",
      "Epoch 113 (lr=0.0100)\n",
      "train loss: 0.582 | acc: 79.534 (39767/50000)\n",
      "test  loss: 0.564 | acc: 81.15  ( 8115/10000)\n",
      "Epoch 114 (lr=0.0100)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(loss_func, opt):\n",
    "    global history\n",
    "    print('Epoch {} (lr={:.4f})'.format(epoch, lr))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = net(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = pred.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
    "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
    "    \n",
    "def test(loss_func):\n",
    "    global checkpoint, history\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = pred.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    history[-1]['loss'] = test_loss\n",
    "    history[-1]['acc'] = acc\n",
    "    if acc > checkpoint['acc']:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
    "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
    "               acc - checkpoint['acc']))\n",
    "        # print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'lr': lr,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        checkpoint = state\n",
    "        # if not os.path.isdir('checkpoint'):\n",
    "        #     os.mkdir('checkpoint')\n",
    "        # torch.save(state, './checkpoint/ckpt.pth')\n",
    "    else:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
    "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
    "\n",
    "\n",
    "from math import exp, e\n",
    "\n",
    "def lr_schedule(x, lr):\n",
    "    x0 = 10\n",
    "    y0 = 0.1\n",
    "    return 0.001 + exp(- x / x0) * x * e * y0 / x0\n",
    "#     if x < x0:\n",
    "#         return 0.001 if x == 0 else lr + y0 / x0\n",
    "#     elif x < 100:\n",
    "#         return 0.001 + (lr - 0.001) * 0.95\n",
    "\n",
    "for _ in range(350):\n",
    "    global epoch, checkpoint, history\n",
    "    if history[-1].get('train_acc', 0) > 99.99:\n",
    "        break\n",
    "    if epoch == 0:\n",
    "        m = net.module\n",
    "        for layer in [m.layer1, m.layer2, m.layer3]: #, m.layer4]:\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].twist = True\n",
    "        print(\"twist on\")\n",
    "        print('testing on random initial weights:')\n",
    "        test(loss_func)\n",
    "    if epoch < 0:\n",
    "        lr = lr_schedule(epoch, lr)\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epoch - checkpoint['epoch'] >= 30:\n",
    "        lr = checkpoint['lr'] * 0.1\n",
    "        print('\\nlearning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
    "        print('loading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        checkpoint['epoch'] = epoch\n",
    "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
    "        opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "    train(loss_func, opt)\n",
    "    test(loss_func)\n",
    "    epoch += 1\n",
    "print('finish at lr = {}, acc = {}'.format(lr, checkpoint['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00010978416685247794 0.10759923607110977 torch.Size([64, 3, 3, 3])\n",
      "\n",
      "-0.0008422630489803851 0.018080590292811394 0.0032747811637818813\n",
      "-0.001270839711651206 0.021102633327245712 0.003429204924032092\n",
      "-0.0014117248356342316 0.023197108879685402 0.00417900737375021\n",
      "\n",
      "-0.004404743667691946 0.0251794271171093 0.024030175060033798\n",
      "-0.0007541141239926219 0.0175800658762455 0.0034878645092248917\n",
      "-0.0008282261551357806 0.016995558515191078 0.003441042033955455\n",
      "\n",
      "-0.0027684776578098536 0.015212113969027996 0.017009109258651733\n",
      "\n",
      "-0.00012686442642007023 0.004946237895637751 0.012030222453176975\n"
     ]
    }
   ],
   "source": [
    "m = net.module\n",
    "print(m.conv1.weight.mean().item(), m.conv1.weight.std().item(), m.conv1.weight.shape)\n",
    "for layer in [m.layer1, m.layer2, m.layer3, m.layer4]:\n",
    "    print()\n",
    "    for i in range(len(layer)):\n",
    "        print(layer[i].conv.weight.mean().item(), layer[i].conv.weight.std().item(), layer[i].convx.weight.std().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7B52zeW88ef"
   },
   "source": [
    "### Results: \n",
    "\n",
    "Training with lr=[0.1, 0.01, 0.001], downgrading if plateaued for 20 epochs.\n",
    "\n",
    "Baseline (classic ResNet)\n",
    "\n",
    "* ResNet50: 94.29 Bottleneck 32, [3,4,6,3], twist=[F,F,F,F]\n",
    "\n",
    "With \"twist\":\n",
    "* 94.52 BasicBlock 64, [2,2,2,2], twist=[T,T,T,T], rotation_aug=10\n",
    "\n",
    "* 94.15 Bottleneck 32, [3,4,6,3], twist=[T,T,T,T]\n",
    "* 94.59 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.28 Bottleneck 32, [3,4,6,3], twist=[T,F,F,F]\n",
    "* 94.84 Bottleneck 32, [8,8,8,3], twist=[T,T,T,F]\n",
    "* 94.22 Bottleneck 32, [8,8,16,3], twist=[T,T,T,F]\n",
    "\n",
    "When training with lr= x e^(-x) -- rising linearly, then falling off exponentially -- it converges faster, in about 100 epochs\n",
    "\n",
    "* 93.98 ResNet50 Bottleneck 16\n",
    "* 94.53 (train_acc=96.13) , BasicBlock 64, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 93.19 Bottleneck 16, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.06 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.72 Bottleneck 64, [3,4,6,3], twist=[T,T,T,F]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 27])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    " [2.8104, 2.6607, 2.0765, 1.7392, 1.5272, 1.3853, 1.1833, 1.1261, 0.8286,\n",
    "        0.7381, 0.6732, 0.5530, 0.4448, 0.3987, 0.3280, 0.2829, 0.2381, 0.2129,\n",
    "        0.1887, 0.1716, 0.1538, 0.1227, 0.1100, 0.0991, 0.0793, 0.0575, 0.0470],\n",
    " [2.5849, 2.4312, 2.0399, 1.4542, 1.3491, 1.2283, 1.0040, 0.7782, 0.7403,\n",
    "        0.6404, 0.4244, 0.4096, 0.3201, 0.2533, 0.2170, 0.1899, 0.1393, 0.1194,\n",
    "        0.1111, 0.0810, 0.0702, 0.0599, 0.0491, 0.0427, 0.0316, 0.0281, 0.0211],\n",
    " [2.3489, 2.1176, 1.8689, 1.3103, 1.1440, 1.1354, 0.8471, 0.7960, 0.6468,\n",
    "        0.4969, 0.4144, 0.3684, 0.2797, 0.2498, 0.1899, 0.1561, 0.1421, 0.1118,\n",
    "        0.0815, 0.0706, 0.0602, 0.0536, 0.0389, 0.0341, 0.0209, 0.0138, 0.0093],\n",
    " [2.3597, 2.0739, 1.8342, 1.2758, 1.1094, 1.0668, 0.9078, 0.8299, 0.6184,\n",
    "        0.5731, 0.4374, 0.4200, 0.2464, 0.2361, 0.2333, 0.1684, 0.1497, 0.1077,\n",
    "        0.0909, 0.0606, 0.0542, 0.0475, 0.0420, 0.0290, 0.0157, 0.0121, 0.0053],\n",
    " [2.4256, 2.0896, 1.8365, 1.3078, 1.1393, 1.1217, 1.0316, 0.8114, 0.6341,\n",
    "        0.5491, 0.4557, 0.3869, 0.2779, 0.2309, 0.1898, 0.1831, 0.1766, 0.1271,\n",
    "        0.0781, 0.0603, 0.0509, 0.0406, 0.0363, 0.0292, 0.0185, 0.0120, 0.0060],\n",
    " [2.4849, 2.0532, 1.8571, 1.3805, 1.1323, 1.1066, 1.0433, 0.8157, 0.6629,\n",
    "        0.5746, 0.4429, 0.3829, 0.2724, 0.2594, 0.1690, 0.1577, 0.1473, 0.1279,\n",
    "        0.0809, 0.0602, 0.0458, 0.0376, 0.0331, 0.0263, 0.0196, 0.0110, 0.0061],\n",
    " [2.4709, 2.0658, 1.8636, 1.4249, 1.1889, 1.1478, 0.9837, 0.7942, 0.6960,\n",
    "        0.5268, 0.4503, 0.4186, 0.2792, 0.2359, 0.1945, 0.1636, 0.1547, 0.1163,\n",
    "        0.0766, 0.0685, 0.0539, 0.0411, 0.0342, 0.0238, 0.0154, 0.0120, 0.0039],\n",
    " [2.5026, 2.0597, 1.8135, 1.4707, 1.2017, 1.0554, 0.9883, 0.8208, 0.7253,\n",
    "        0.5223, 0.4767, 0.4090, 0.3169, 0.2375, 0.1904, 0.1654, 0.1372, 0.1143,\n",
    "        0.0842, 0.0540, 0.0447, 0.0322, 0.0279, 0.0192, 0.0152, 0.0099, 0.0069],\n",
    " [2.5069, 2.0750, 1.7927, 1.5006, 1.2215, 1.1006, 0.9533, 0.8030, 0.7206,\n",
    "        0.5561, 0.4817, 0.4372, 0.3454, 0.2270, 0.1583, 0.1296, 0.1175, 0.0945,\n",
    "        0.0714, 0.0528, 0.0451, 0.0355, 0.0255, 0.0217, 0.0172, 0.0125, 0.0101],\n",
    " [2.5199, 2.0277, 1.7792, 1.4563, 1.2592, 1.0659, 0.9456, 0.8021, 0.7113,\n",
    "        0.5484, 0.4876, 0.4590, 0.3279, 0.1928, 0.1862, 0.1847, 0.1571, 0.1049,\n",
    "        0.0790, 0.0580, 0.0396, 0.0352, 0.0295, 0.0217, 0.0183, 0.0103, 0.0057],\n",
    " [2.3893, 1.9121, 1.6831, 1.3817, 1.1941, 1.0182, 0.9111, 0.7532, 0.6538,\n",
    "        0.5459, 0.4672, 0.4358, 0.3111, 0.1830, 0.1812, 0.1646, 0.1533, 0.0977,\n",
    "        0.0770, 0.0575, 0.0403, 0.0324, 0.0275, 0.0221, 0.0167, 0.0096, 0.0064],\n",
    " [2.2690, 1.8165, 1.5975, 1.3160, 1.1309, 0.9703, 0.8707, 0.7099, 0.6191,\n",
    "        0.5291, 0.4453, 0.4059, 0.2917, 0.1800, 0.1698, 0.1525, 0.1403, 0.0821,\n",
    "        0.0720, 0.0552, 0.0420, 0.0310, 0.0275, 0.0211, 0.0167, 0.0098, 0.0062],\n",
    " [2.1601, 1.7282, 1.5242, 1.2564, 1.0730, 0.9203, 0.8337, 0.6863, 0.5854,\n",
    "        0.5140, 0.4252, 0.3822, 0.2758, 0.1659, 0.1575, 0.1521, 0.1334, 0.0783,\n",
    "        0.0741, 0.0565, 0.0421, 0.0305, 0.0250, 0.0212, 0.0163, 0.0090, 0.0055],\n",
    " [2.0608, 1.6436, 1.4555, 1.2012, 1.0342, 0.8702, 0.7983, 0.6782, 0.5550,\n",
    "        0.4913, 0.4171, 0.3638, 0.2737, 0.1557, 0.1502, 0.1329, 0.1208, 0.0756,\n",
    "        0.0704, 0.0500, 0.0414, 0.0295, 0.0238, 0.0203, 0.0151, 0.0089, 0.0044],\n",
    " [2.0513, 1.6338, 1.4452, 1.1936, 1.0283, 0.8667, 0.7949, 0.6695, 0.5496,\n",
    "        0.4944, 0.4150, 0.3623, 0.2676, 0.1548, 0.1484, 0.1310, 0.1200, 0.0749,\n",
    "        0.0701, 0.0505, 0.0418, 0.0292, 0.0238, 0.0202, 0.0152, 0.0090, 0.0041]\n",
    "]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': 10.0, 'epoch': 0, 'loss': 230.31848001480103},\n",
       " {'epoch': 0,\n",
       "  'train_loss': 754.6636430025101,\n",
       "  'train_acc': 28.956,\n",
       "  'loss': 177.68029403686523,\n",
       "  'acc': 35.7},\n",
       " {'epoch': 1,\n",
       "  'train_loss': 582.1219648122787,\n",
       "  'train_acc': 45.178,\n",
       "  'loss': 166.2290700674057,\n",
       "  'acc': 49.44},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 487.1171388030052,\n",
       "  'train_acc': 55.164,\n",
       "  'loss': 141.82904732227325,\n",
       "  'acc': 55.11},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 438.89170759916306,\n",
       "  'train_acc': 59.926,\n",
       "  'loss': 126.66632717847824,\n",
       "  'acc': 55.96},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 402.5308494567871,\n",
       "  'train_acc': 63.694,\n",
       "  'loss': 142.63620400428772,\n",
       "  'acc': 57.95},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 371.8175357580185,\n",
       "  'train_acc': 66.636,\n",
       "  'loss': 113.34880620241165,\n",
       "  'acc': 62.46},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 346.9781383275986,\n",
       "  'train_acc': 68.852,\n",
       "  'loss': 82.87301754951477,\n",
       "  'acc': 71.34},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 329.3637217283249,\n",
       "  'train_acc': 70.536,\n",
       "  'loss': 95.5984001159668,\n",
       "  'acc': 69.68},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 316.4112443327904,\n",
       "  'train_acc': 71.912,\n",
       "  'loss': 111.94636160135269,\n",
       "  'acc': 65.53},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 309.1336905658245,\n",
       "  'train_acc': 72.488,\n",
       "  'loss': 85.85902404785156,\n",
       "  'acc': 70.85},\n",
       " {'epoch': 10,\n",
       "  'train_loss': 302.88826566934586,\n",
       "  'train_acc': 73.134,\n",
       "  'loss': 92.3339295387268,\n",
       "  'acc': 70.27},\n",
       " {'epoch': 11,\n",
       "  'train_loss': 295.1024594902992,\n",
       "  'train_acc': 73.974,\n",
       "  'loss': 68.32917627692223,\n",
       "  'acc': 76.86},\n",
       " {'epoch': 12,\n",
       "  'train_loss': 287.4972376227379,\n",
       "  'train_acc': 74.72,\n",
       "  'loss': 65.69839265942574,\n",
       "  'acc': 77.35},\n",
       " {'epoch': 13,\n",
       "  'train_loss': 281.67764607071877,\n",
       "  'train_acc': 75.3,\n",
       "  'loss': 75.40403607487679,\n",
       "  'acc': 75.09},\n",
       " {'epoch': 14,\n",
       "  'train_loss': 280.0892382860184,\n",
       "  'train_acc': 75.198,\n",
       "  'loss': 78.22349175810814,\n",
       "  'acc': 73.27},\n",
       " {'epoch': 15,\n",
       "  'train_loss': 271.89180263876915,\n",
       "  'train_acc': 75.85,\n",
       "  'loss': 94.26444393396378,\n",
       "  'acc': 71.44},\n",
       " {'epoch': 16,\n",
       "  'train_loss': 264.6638306379318,\n",
       "  'train_acc': 76.582,\n",
       "  'loss': 78.96824687719345,\n",
       "  'acc': 73.0},\n",
       " {'epoch': 17,\n",
       "  'train_loss': 258.00521370768547,\n",
       "  'train_acc': 77.042,\n",
       "  'loss': 81.85121387243271,\n",
       "  'acc': 74.07},\n",
       " {'epoch': 18,\n",
       "  'train_loss': 251.63884815573692,\n",
       "  'train_acc': 77.78,\n",
       "  'loss': 94.05693364143372,\n",
       "  'acc': 70.42},\n",
       " {'epoch': 19,\n",
       "  'train_loss': 245.31524714827538,\n",
       "  'train_acc': 78.31,\n",
       "  'loss': 71.65650033950806,\n",
       "  'acc': 76.22},\n",
       " {'epoch': 20,\n",
       "  'train_loss': 240.43961921334267,\n",
       "  'train_acc': 78.954,\n",
       "  'loss': 67.78455439209938,\n",
       "  'acc': 78.77},\n",
       " {'epoch': 21,\n",
       "  'train_loss': 234.90850976109505,\n",
       "  'train_acc': 79.226,\n",
       "  'loss': 75.9235465824604,\n",
       "  'acc': 75.34},\n",
       " {'epoch': 22,\n",
       "  'train_loss': 229.67679852247238,\n",
       "  'train_acc': 79.652,\n",
       "  'loss': 62.795541524887085,\n",
       "  'acc': 79.91},\n",
       " {'epoch': 23,\n",
       "  'train_loss': 223.76602506637573,\n",
       "  'train_acc': 80.206,\n",
       "  'loss': 60.756801038980484,\n",
       "  'acc': 79.22},\n",
       " {'epoch': 24,\n",
       "  'train_loss': 216.51522994041443,\n",
       "  'train_acc': 80.79,\n",
       "  'loss': 64.70525580644608,\n",
       "  'acc': 78.71},\n",
       " {'epoch': 25,\n",
       "  'train_loss': 212.19084921479225,\n",
       "  'train_acc': 81.286,\n",
       "  'loss': 60.04075738787651,\n",
       "  'acc': 79.87},\n",
       " {'epoch': 26,\n",
       "  'train_loss': 204.96553593873978,\n",
       "  'train_acc': 81.864,\n",
       "  'loss': 57.327406108379364,\n",
       "  'acc': 81.77},\n",
       " {'epoch': 27,\n",
       "  'train_loss': 199.871346950531,\n",
       "  'train_acc': 82.402,\n",
       "  'loss': 60.44831410050392,\n",
       "  'acc': 79.72},\n",
       " {'epoch': 28,\n",
       "  'train_loss': 194.64086565375328,\n",
       "  'train_acc': 82.806,\n",
       "  'loss': 50.15753731131554,\n",
       "  'acc': 83.13},\n",
       " {'epoch': 29,\n",
       "  'train_loss': 188.3463372439146,\n",
       "  'train_acc': 83.22,\n",
       "  'loss': 58.99103108048439,\n",
       "  'acc': 80.12},\n",
       " {'epoch': 30,\n",
       "  'train_loss': 180.94015794992447,\n",
       "  'train_acc': 83.938,\n",
       "  'loss': 38.15060582756996,\n",
       "  'acc': 86.92},\n",
       " {'epoch': 31,\n",
       "  'train_loss': 175.48647716641426,\n",
       "  'train_acc': 84.448,\n",
       "  'loss': 42.540227845311165,\n",
       "  'acc': 85.94},\n",
       " {'epoch': 32,\n",
       "  'train_loss': 170.44659201800823,\n",
       "  'train_acc': 84.812,\n",
       "  'loss': 44.465014934539795,\n",
       "  'acc': 85.29},\n",
       " {'epoch': 33,\n",
       "  'train_loss': 165.44948069751263,\n",
       "  'train_acc': 85.374,\n",
       "  'loss': 43.63767430186272,\n",
       "  'acc': 85.15},\n",
       " {'epoch': 34,\n",
       "  'train_loss': 158.86690957844257,\n",
       "  'train_acc': 85.898,\n",
       "  'loss': 53.85048881173134,\n",
       "  'acc': 83.04},\n",
       " {'epoch': 35,\n",
       "  'train_loss': 154.87540428340435,\n",
       "  'train_acc': 86.246,\n",
       "  'loss': 39.88396769762039,\n",
       "  'acc': 86.62},\n",
       " {'epoch': 36,\n",
       "  'train_loss': 149.15163958072662,\n",
       "  'train_acc': 86.86,\n",
       "  'loss': 35.54605996608734,\n",
       "  'acc': 87.68},\n",
       " {'epoch': 37,\n",
       "  'train_loss': 142.92334270477295,\n",
       "  'train_acc': 87.412,\n",
       "  'loss': 38.27856756746769,\n",
       "  'acc': 86.99},\n",
       " {'epoch': 38,\n",
       "  'train_loss': 138.7407031059265,\n",
       "  'train_acc': 87.728,\n",
       "  'loss': 35.043267503380775,\n",
       "  'acc': 88.47},\n",
       " {'epoch': 39,\n",
       "  'train_loss': 136.36564065515995,\n",
       "  'train_acc': 87.846,\n",
       "  'loss': 35.78306883573532,\n",
       "  'acc': 88.3},\n",
       " {'epoch': 40,\n",
       "  'train_loss': 128.22218723595142,\n",
       "  'train_acc': 88.572,\n",
       "  'loss': 30.021601274609566,\n",
       "  'acc': 89.9},\n",
       " {'epoch': 41,\n",
       "  'train_loss': 123.65468882024288,\n",
       "  'train_acc': 89.05,\n",
       "  'loss': 33.20254376530647,\n",
       "  'acc': 89.01},\n",
       " {'epoch': 42,\n",
       "  'train_loss': 119.40172852575779,\n",
       "  'train_acc': 89.37,\n",
       "  'loss': 34.89013160765171,\n",
       "  'acc': 88.71},\n",
       " {'epoch': 43,\n",
       "  'train_loss': 114.83898785710335,\n",
       "  'train_acc': 89.764,\n",
       "  'loss': 34.3774391785264,\n",
       "  'acc': 88.84},\n",
       " {'epoch': 44,\n",
       "  'train_loss': 108.44792911410332,\n",
       "  'train_acc': 90.27,\n",
       "  'loss': 28.022722877562046,\n",
       "  'acc': 90.72},\n",
       " {'epoch': 45,\n",
       "  'train_loss': 106.37337556481361,\n",
       "  'train_acc': 90.58,\n",
       "  'loss': 26.42863517254591,\n",
       "  'acc': 91.35},\n",
       " {'epoch': 46,\n",
       "  'train_loss': 102.38227233290672,\n",
       "  'train_acc': 90.858,\n",
       "  'loss': 30.987299293279648,\n",
       "  'acc': 90.07},\n",
       " {'epoch': 47,\n",
       "  'train_loss': 97.59058793634176,\n",
       "  'train_acc': 91.362,\n",
       "  'loss': 25.522725239396095,\n",
       "  'acc': 91.65},\n",
       " {'epoch': 48,\n",
       "  'train_loss': 92.82399659603834,\n",
       "  'train_acc': 91.736,\n",
       "  'loss': 25.45881725847721,\n",
       "  'acc': 91.74},\n",
       " {'epoch': 49,\n",
       "  'train_loss': 91.53302105516195,\n",
       "  'train_acc': 91.816,\n",
       "  'loss': 28.90867704898119,\n",
       "  'acc': 90.81},\n",
       " {'epoch': 50,\n",
       "  'train_loss': 84.91942705214024,\n",
       "  'train_acc': 92.452,\n",
       "  'loss': 25.496378526091576,\n",
       "  'acc': 92.16},\n",
       " {'epoch': 51,\n",
       "  'train_loss': 81.54751116782427,\n",
       "  'train_acc': 92.772,\n",
       "  'loss': 24.12046457082033,\n",
       "  'acc': 92.23},\n",
       " {'epoch': 52,\n",
       "  'train_loss': 78.19618439674377,\n",
       "  'train_acc': 93.026,\n",
       "  'loss': 24.495963007211685,\n",
       "  'acc': 92.12},\n",
       " {'epoch': 53,\n",
       "  'train_loss': 75.00710986182094,\n",
       "  'train_acc': 93.37,\n",
       "  'loss': 23.212439574301243,\n",
       "  'acc': 92.73},\n",
       " {'epoch': 54,\n",
       "  'train_loss': 73.11572906374931,\n",
       "  'train_acc': 93.434,\n",
       "  'loss': 25.023869276046753,\n",
       "  'acc': 92.63},\n",
       " {'epoch': 55,\n",
       "  'train_loss': 68.57173947244883,\n",
       "  'train_acc': 93.93,\n",
       "  'loss': 21.574408560991287,\n",
       "  'acc': 93.31},\n",
       " {'epoch': 56,\n",
       "  'train_loss': 66.48064863309264,\n",
       "  'train_acc': 94.144,\n",
       "  'loss': 21.594503670930862,\n",
       "  'acc': 93.16},\n",
       " {'epoch': 57,\n",
       "  'train_loss': 63.750387877225876,\n",
       "  'train_acc': 94.18,\n",
       "  'loss': 21.48833091557026,\n",
       "  'acc': 93.44},\n",
       " {'epoch': 58,\n",
       "  'train_loss': 60.735757905989885,\n",
       "  'train_acc': 94.62,\n",
       "  'loss': 24.419834703207016,\n",
       "  'acc': 92.78},\n",
       " {'epoch': 59,\n",
       "  'train_loss': 58.03293234668672,\n",
       "  'train_acc': 94.908,\n",
       "  'loss': 21.755030311644077,\n",
       "  'acc': 93.46},\n",
       " {'epoch': 60,\n",
       "  'train_loss': 53.86719523742795,\n",
       "  'train_acc': 95.222,\n",
       "  'loss': 23.144079133868217,\n",
       "  'acc': 93.08},\n",
       " {'epoch': 61,\n",
       "  'train_loss': 53.972158446908,\n",
       "  'train_acc': 95.132,\n",
       "  'loss': 19.754779256880283,\n",
       "  'acc': 93.99},\n",
       " {'epoch': 62,\n",
       "  'train_loss': 51.23151122033596,\n",
       "  'train_acc': 95.452,\n",
       "  'loss': 20.359240047633648,\n",
       "  'acc': 93.85},\n",
       " {'epoch': 63,\n",
       "  'train_loss': 47.32641971856356,\n",
       "  'train_acc': 95.848,\n",
       "  'loss': 19.53351991251111,\n",
       "  'acc': 93.97},\n",
       " {'epoch': 64,\n",
       "  'train_loss': 46.20965064689517,\n",
       "  'train_acc': 95.926,\n",
       "  'loss': 21.242604319006205,\n",
       "  'acc': 93.87},\n",
       " {'epoch': 65,\n",
       "  'train_loss': 44.43762354180217,\n",
       "  'train_acc': 96.126,\n",
       "  'loss': 20.38332138210535,\n",
       "  'acc': 94.06},\n",
       " {'epoch': 66,\n",
       "  'train_loss': 41.08667369186878,\n",
       "  'train_acc': 96.404,\n",
       "  'loss': 22.153285827487707,\n",
       "  'acc': 93.81},\n",
       " {'epoch': 67,\n",
       "  'train_loss': 40.79927361011505,\n",
       "  'train_acc': 96.37,\n",
       "  'loss': 20.84742197394371,\n",
       "  'acc': 93.98},\n",
       " {'epoch': 68,\n",
       "  'train_loss': 39.64173465967178,\n",
       "  'train_acc': 96.536,\n",
       "  'loss': 20.524369411170483,\n",
       "  'acc': 94.03},\n",
       " {'epoch': 69,\n",
       "  'train_loss': 37.92313813790679,\n",
       "  'train_acc': 96.704,\n",
       "  'loss': 20.480248659849167,\n",
       "  'acc': 93.91},\n",
       " {'epoch': 70,\n",
       "  'train_loss': 36.48559933900833,\n",
       "  'train_acc': 96.822,\n",
       "  'loss': 19.963090676814318,\n",
       "  'acc': 94.29},\n",
       " {'epoch': 71,\n",
       "  'train_loss': 34.31297986395657,\n",
       "  'train_acc': 97.03,\n",
       "  'loss': 19.80246415734291,\n",
       "  'acc': 94.44},\n",
       " {'epoch': 72,\n",
       "  'train_loss': 35.036303075030446,\n",
       "  'train_acc': 96.95,\n",
       "  'loss': 20.067306831479073,\n",
       "  'acc': 94.31},\n",
       " {'epoch': 73,\n",
       "  'train_loss': 33.17550626397133,\n",
       "  'train_acc': 97.162,\n",
       "  'loss': 20.037591498345137,\n",
       "  'acc': 94.35},\n",
       " {'epoch': 74,\n",
       "  'train_loss': 31.534218225628138,\n",
       "  'train_acc': 97.244,\n",
       "  'loss': 19.855324421077967,\n",
       "  'acc': 94.3},\n",
       " {'epoch': 75,\n",
       "  'train_loss': 32.01982982456684,\n",
       "  'train_acc': 97.188,\n",
       "  'loss': 19.95951084420085,\n",
       "  'acc': 94.46},\n",
       " {'epoch': 76,\n",
       "  'train_loss': 29.761041399091482,\n",
       "  'train_acc': 97.396,\n",
       "  'loss': 20.00375633686781,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 77,\n",
       "  'train_loss': 29.598727371543646,\n",
       "  'train_acc': 97.458,\n",
       "  'loss': 20.209928046911955,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 78,\n",
       "  'train_loss': 28.85201194509864,\n",
       "  'train_acc': 97.5,\n",
       "  'loss': 19.417937647551298,\n",
       "  'acc': 94.64},\n",
       " {'epoch': 79,\n",
       "  'train_loss': 27.52188055217266,\n",
       "  'train_acc': 97.638,\n",
       "  'loss': 20.07254132628441,\n",
       "  'acc': 94.51},\n",
       " {'epoch': 80,\n",
       "  'train_loss': 26.949739089235663,\n",
       "  'train_acc': 97.666,\n",
       "  'loss': 20.523633308708668,\n",
       "  'acc': 94.3},\n",
       " {'epoch': 81,\n",
       "  'train_loss': 27.287329031154513,\n",
       "  'train_acc': 97.64,\n",
       "  'loss': 20.47282698005438,\n",
       "  'acc': 94.45},\n",
       " {'epoch': 82,\n",
       "  'train_loss': 26.864581767469645,\n",
       "  'train_acc': 97.656,\n",
       "  'loss': 20.356502521783113,\n",
       "  'acc': 94.41},\n",
       " {'epoch': 83,\n",
       "  'train_loss': 24.70988541841507,\n",
       "  'train_acc': 97.872,\n",
       "  'loss': 20.502027813345194,\n",
       "  'acc': 94.32},\n",
       " {'epoch': 84,\n",
       "  'train_loss': 25.37227918021381,\n",
       "  'train_acc': 97.898,\n",
       "  'loss': 20.41370415315032,\n",
       "  'acc': 94.43},\n",
       " {'epoch': 85,\n",
       "  'train_loss': 24.717292906716466,\n",
       "  'train_acc': 97.888,\n",
       "  'loss': 19.961603667587042,\n",
       "  'acc': 94.62},\n",
       " {'epoch': 86,\n",
       "  'train_loss': 23.330789199098945,\n",
       "  'train_acc': 98.056,\n",
       "  'loss': 20.451539799571037,\n",
       "  'acc': 94.49},\n",
       " {'epoch': 87,\n",
       "  'train_loss': 23.76170135103166,\n",
       "  'train_acc': 97.966,\n",
       "  'loss': 20.12364847585559,\n",
       "  'acc': 94.53},\n",
       " {'epoch': 88,\n",
       "  'train_loss': 23.489305537194014,\n",
       "  'train_acc': 97.952,\n",
       "  'loss': 20.716168001294136,\n",
       "  'acc': 94.6},\n",
       " {'epoch': 89,\n",
       "  'train_loss': 22.971767948940396,\n",
       "  'train_acc': 98.042,\n",
       "  'loss': 20.09196463599801,\n",
       "  'acc': 94.57},\n",
       " {'epoch': 90,\n",
       "  'train_loss': 21.879804531112313,\n",
       "  'train_acc': 98.13,\n",
       "  'loss': 20.74242278933525,\n",
       "  'acc': 94.42},\n",
       " {'epoch': 91,\n",
       "  'train_loss': 22.351364836096764,\n",
       "  'train_acc': 98.128,\n",
       "  'loss': 20.309488777071238,\n",
       "  'acc': 94.44},\n",
       " {'epoch': 92,\n",
       "  'train_loss': 22.70437694899738,\n",
       "  'train_acc': 98.02,\n",
       "  'loss': 20.474807541817427,\n",
       "  'acc': 94.52},\n",
       " {'epoch': 93,\n",
       "  'train_loss': 21.98940627090633,\n",
       "  'train_acc': 98.144,\n",
       "  'loss': 20.276823595166206,\n",
       "  'acc': 94.54},\n",
       " {'epoch': 94,\n",
       "  'train_loss': 23.13129918463528,\n",
       "  'train_acc': 98.008,\n",
       "  'loss': 20.933124992996454,\n",
       "  'acc': 94.51},\n",
       " {'epoch': 95,\n",
       "  'train_loss': 21.944106662645936,\n",
       "  'train_acc': 98.132,\n",
       "  'loss': 20.379710882902145,\n",
       "  'acc': 94.64},\n",
       " {'epoch': 96,\n",
       "  'train_loss': 22.127670703455806,\n",
       "  'train_acc': 98.104,\n",
       "  'loss': 20.34023194387555,\n",
       "  'acc': 94.58},\n",
       " {'epoch': 97,\n",
       "  'train_loss': 21.292805939912796,\n",
       "  'train_acc': 98.212,\n",
       "  'loss': 20.245772939175367,\n",
       "  'acc': 94.67},\n",
       " {'epoch': 98,\n",
       "  'train_loss': 21.899103125557303,\n",
       "  'train_acc': 98.062,\n",
       "  'loss': 20.1459485180676,\n",
       "  'acc': 94.66},\n",
       " {'epoch': 99,\n",
       "  'train_loss': 22.581811985000968,\n",
       "  'train_acc': 98.082,\n",
       "  'loss': 20.259088438004255,\n",
       "  'acc': 94.61},\n",
       " {'epoch': 100,\n",
       "  'train_loss': 22.166775664314628,\n",
       "  'train_acc': 98.116,\n",
       "  'loss': 20.203341081738472,\n",
       "  'acc': 94.53},\n",
       " {'epoch': 101,\n",
       "  'train_loss': 21.575932003557682,\n",
       "  'train_acc': 98.154,\n",
       "  'loss': 20.720015674829483,\n",
       "  'acc': 94.38},\n",
       " {'epoch': 102,\n",
       "  'train_loss': 21.44084707275033,\n",
       "  'train_acc': 98.112,\n",
       "  'loss': 19.995714470744133,\n",
       "  'acc': 94.67},\n",
       " {'epoch': 103,\n",
       "  'train_loss': 20.660856567323208,\n",
       "  'train_acc': 98.242,\n",
       "  'loss': 20.289707750082016,\n",
       "  'acc': 94.64},\n",
       " {'epoch': 104,\n",
       "  'train_loss': 21.513843407854438,\n",
       "  'train_acc': 98.2,\n",
       "  'loss': 20.230567198246717,\n",
       "  'acc': 94.69},\n",
       " {'epoch': 105,\n",
       "  'train_loss': 22.182401878759265,\n",
       "  'train_acc': 98.142,\n",
       "  'loss': 20.23087851330638,\n",
       "  'acc': 94.68},\n",
       " {'epoch': 106,\n",
       "  'train_loss': 21.77024850808084,\n",
       "  'train_acc': 98.146,\n",
       "  'loss': 20.081487104296684,\n",
       "  'acc': 94.72},\n",
       " {'epoch': 107,\n",
       "  'train_loss': 20.370955986902118,\n",
       "  'train_acc': 98.296,\n",
       "  'loss': 20.24106700718403,\n",
       "  'acc': 94.67},\n",
       " {'epoch': 108,\n",
       "  'train_loss': 21.607679257169366,\n",
       "  'train_acc': 98.176,\n",
       "  'loss': 20.315171789377928,\n",
       "  'acc': 94.63},\n",
       " {'epoch': 109,\n",
       "  'train_loss': 21.645886505022645,\n",
       "  'train_acc': 98.19,\n",
       "  'loss': 20.813805993646383,\n",
       "  'acc': 94.56},\n",
       " {'epoch': 110,\n",
       "  'train_loss': 20.99921192973852,\n",
       "  'train_acc': 98.264,\n",
       "  'loss': 20.50289072841406,\n",
       "  'acc': 94.68},\n",
       " {'epoch': 111,\n",
       "  'train_loss': 20.661783035844564,\n",
       "  'train_acc': 98.236,\n",
       "  'loss': 20.414362780749798,\n",
       "  'acc': 94.71},\n",
       " {'epoch': 112,\n",
       "  'train_loss': 20.90364129282534,\n",
       "  'train_acc': 98.212,\n",
       "  'loss': 20.484193336218596,\n",
       "  'acc': 94.57},\n",
       " {'epoch': 113,\n",
       "  'train_loss': 21.956725258380175,\n",
       "  'train_acc': 98.106,\n",
       "  'loss': 20.704868871718645,\n",
       "  'acc': 94.6},\n",
       " {'epoch': 114,\n",
       "  'train_loss': 20.60666592232883,\n",
       "  'train_acc': 98.244,\n",
       "  'loss': 20.192240923643112,\n",
       "  'acc': 94.68},\n",
       " {'epoch': 115,\n",
       "  'train_loss': 20.535031436011195,\n",
       "  'train_acc': 98.326,\n",
       "  'loss': 20.386975217610598,\n",
       "  'acc': 94.71},\n",
       " {'epoch': 116, 'acc': 94.72},\n",
       " {'epoch': 116,\n",
       "  'train_loss': 20.362896433100104,\n",
       "  'train_acc': 98.264,\n",
       "  'loss': 19.973237723112106,\n",
       "  'acc': 94.8},\n",
       " {'epoch': 117,\n",
       "  'train_loss': 21.46388584189117,\n",
       "  'train_acc': 98.182,\n",
       "  'loss': 20.030115962028503,\n",
       "  'acc': 94.79},\n",
       " {'epoch': 118,\n",
       "  'train_loss': 20.753612684085965,\n",
       "  'train_acc': 98.158,\n",
       "  'loss': 20.251006454229355,\n",
       "  'acc': 94.7},\n",
       " {'epoch': 119,\n",
       "  'train_loss': 19.95198694244027,\n",
       "  'train_acc': 98.33,\n",
       "  'loss': 20.087826397269964,\n",
       "  'acc': 94.77},\n",
       " {'epoch': 120,\n",
       "  'train_loss': 20.72102808393538,\n",
       "  'train_acc': 98.246,\n",
       "  'loss': 20.18049632012844,\n",
       "  'acc': 94.74},\n",
       " {'epoch': 121,\n",
       "  'train_loss': 20.436268098652363,\n",
       "  'train_acc': 98.276,\n",
       "  'loss': 20.031006831675768,\n",
       "  'acc': 94.64},\n",
       " {'epoch': 122,\n",
       "  'train_loss': 20.222248420119286,\n",
       "  'train_acc': 98.342,\n",
       "  'loss': 20.101380709558725,\n",
       "  'acc': 94.67}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10_with_PDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

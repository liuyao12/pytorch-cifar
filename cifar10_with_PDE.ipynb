{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/liuyao12/pytorch-cifar/blob/master/cifar10_with_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf6nUgErY6Bh"
   },
   "source": [
    "# ResNet with a \"twist\"\n",
    "\n",
    "* As far as I'm aware, a simple and novel architecture of ConvNets (Convolutional Neural Networks) that is readily applicable to any existing ResNet backbone.\n",
    "\n",
    "* The key idea would be hard to come by or justify without viewing ResNet as a partial differential equation (like the heat equation). Traditionally, the standard toolkit for machine learning typically includes basics of multi-variable calculus, linear algebra, and statistics, and not so much PDE. This partly explains why ResNet comes on the scene relatively late (2015), and why this enhanced version of ResNet has not been \"reinvented\" by the DL community.\n",
    "\n",
    "* Code based off of https://github.com/kuangliu/pytorch-cifar\n",
    "\n",
    "* Questions and comments shall be greatly appreciated [@liuyao12](https://twitter.com/liuyao12) or liuyao@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPeChzrK7iYC"
   },
   "source": [
    "A quick summary of ConvNets from a Partial Differential Equations (PDE) point of view. For details, see my [blog post on Observable](https://observablehq.com/@liuyao12/neural-networks-and-partial-differential-equations).\n",
    "\n",
    "neural network | heat equation\n",
    ":----:|:-------:\n",
    "input layer | initial condition\n",
    "feed forward | solving the equation\n",
    "hidden layers | solution at intermediate times\n",
    "output layer | solution at final time\n",
    "convolution with 3×3 kernel | differential operator of order ≤ 2\n",
    "weights | coefficients\n",
    "boundary handling (padding) | boundary condition\n",
    "multiple channels/filters/feature maps | system of (coupled) PDEs\n",
    "e.g. 16×16×3×3 kernel | 16×16 matrix of differential operators\n",
    "16×16×1×1 kernel | 16×16 matrix of constants\n",
    "groups=2 (in Conv2d) | matrix is block diagonal (direct sum of 2 blocks)\n",
    "\n",
    "\n",
    "Basically, classical ConvNets (ResNets) are **linear PDEs with constant coefficients**, and here I'm simply trying to make it **variable coefficients**, with the variables being polynomials of degree ≤ 1, which should (in theory) enable the neural net to learn more ways to deform the input than diffusion and translation (e.g., rotation and scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lmUdhteH5N9s",
    "outputId": "ed22c545-89ff-4e1c-f44a-5609bfbd010e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "Testing on a random input:\n",
      "INPUT  torch.Size([1, 3, 224, 224])\n",
      "OUTPUT torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "ResNet in PyTorch, forked from https://github.com/kuangliu/pytorch-cifar\n",
    "Reference:\n",
    "    Kaiming He 何恺明, Xiangyu Zhang 张祥雨, Shaoqing Ren 任少卿, Jian Sun 孙剑 (Microsoft Research Asia)\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.match = stride == 1 and in_channels == self.expansion * channels\n",
    "        self.twist = False\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.conv1x = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv1y = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if not self.match:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x1 = self.conv1(x)\n",
    "        if self.twist:\n",
    "            _, c, h, w = tuple(x1.shape)\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv1x.weight.data = (self.conv1x.weight - self.conv1x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv1y.weight.data = (self.conv1y.weight - self.conv1y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv1y.weight.data = self.conv1x.weight.transpose(2,3).flip(2)\n",
    "            if self.XX is None:\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "                # print(\"twist initialized, self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "            x1 = self.conv1(x) + self.XX * self.conv1x(x) + self.YY * self.conv1y(x)\n",
    "            # print(\"twist initialized, outside self.XX\", self.XX.shape, self.XX.mean().item())\n",
    "        \n",
    "        x2 = F.relu(self.bn2(x1))\n",
    "        if self.twist:\n",
    "            # symmetrize the x-kernel (forcing it to be a 1st-order differential operator, aka a vector field)\n",
    "            self.conv2x.weight.data = (self.conv2x.weight - self.conv2x.weight.flip(2).flip(3)) / 2\n",
    "            # copy the x-kernel to be the y-kernel\n",
    "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(2)\n",
    "            x3 = self.conv2(x2) + self.XX * self.conv2x(x2) + self.YY * self.conv2y(x2)\n",
    "        else:\n",
    "            x3 = self.conv2(x2)\n",
    "        x3 += self.shortcut(x)\n",
    "        return x3\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.twist = False\n",
    "        self.channels = channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2x = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv2y = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.conv3 = nn.Conv2d(channels, self.expansion * channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = F.relu(self.bn1(x1))\n",
    "        if self.twist: \n",
    "            # symmetrize the kernels (force it to be a 1st-order diff op, i.e. a vector field)\n",
    "            tmp = (self.conv2x.weight - self.conv2x.weight.flip(2)) / 2\n",
    "            self.conv2x.weight.data = (tmp - tmp.flip(3)) / 2\n",
    "            # self.conv2y.weight.data = (self.conv2y.weight - self.conv2y.weight.flip(2).flip(3)) / 2\n",
    "            # make y-vector perpendicular to x-vector\n",
    "            self.conv2y.weight.data = self.conv2x.weight.transpose(2,3).flip(3)\n",
    "        x2 = self.conv2(x1)\n",
    "        if self.twist:\n",
    "            if self.XX is None: # initialize self.XY\n",
    "                _, c, h, w = tuple(x2.shape)\n",
    "                self.XX = torch.from_numpy(np.indices((h,w), dtype='float32')[1] / w - 0.5).to(x.device)\n",
    "                self.YY = torch.from_numpy(np.indices((h,w), dtype='float32')[0] / h - 0.5).to(x.device)\n",
    "            x2 += self.XX * self.conv2x(x1) + self.YY * self.conv2y(x1)\n",
    "        x3 = F.relu(self.bn2(x2))\n",
    "        x4 = self.conv3(x3)\n",
    "        x4 = self.bn3(x4)\n",
    "        x4 += self.shortcut(x)\n",
    "        x4 = F.relu(x4)\n",
    "        return x4\n",
    "\n",
    "\n",
    "class PDEBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, channels, stride=1):\n",
    "        super(PDEBlock, self).__init__()\n",
    "        self.twist = False\n",
    "        self.iterations = 2\n",
    "        self.expand = in_channels != channels or stride != 1\n",
    "        if self.expand:\n",
    "            self.conv0 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=stride, padding=1, bias=False)\n",
    "            self.bn0 = nn.BatchNorm2d(channels)\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, groups=4, padding=1, bias=False)\n",
    "        self.XX, self.YY = None, None\n",
    "        self.convx = nn.Conv2d(channels, channels, kernel_size=3, stride=1, groups=4, padding=1, bias=False)\n",
    "        self.convy = nn.Conv2d(channels, channels, kernel_size=3, stride=1, groups=4, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand:\n",
    "            x = self.bn0(self.conv0(x))\n",
    "        if self.XX is None:\n",
    "            _, _, h, w = tuple(x.shape)\n",
    "            self.XX = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[3]/w-0.5).to(x.device)\n",
    "            self.YY = torch.from_numpy(np.indices((1,1,h,w), dtype='float32')[2]/h-0.5).to(x.device)\n",
    "        \n",
    "        if self.twist:\n",
    "            # symmetrize kernels\n",
    "            self.convx.weight.data = (self.convx.weight - self.convx.weight.flip(2).flip(3)) / 2\n",
    "            # self.convy.weight.data = (self.convy.weight - self.convy.weight.flip(2).flip(3)) / 2\n",
    "            self.convy.weight.data = self.convx.weight.transpose(2,3).flip(2)\n",
    "        for i in range(self.iterations):\n",
    "            x1 = self.conv(x)\n",
    "            if self.twist:\n",
    "                x1 += self.XX * self.convx(x) + self.YY * self.convy(x)\n",
    "            x = x + x1 / self.iterations\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        channels = [self.in_channels * i for i in [1, 2, 4, 4]]\n",
    "        self.num_blocks = num_blocks\n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "        self.layer1 = self._make_layer(block, channels[0], num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, channels[1], num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, channels[2], num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, channels[3], num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(channels[2] * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, channels, num_blocks, stride):\n",
    "        if num_blocks == 0:\n",
    "            layers = [nn.Conv2d(self.in_channels, channels, kernel_size=1, stride=stride, padding=1, bias=False),\n",
    "                      nn.BatchNorm2d(channels)]\n",
    "            self.in_channels = channels * block.expansion\n",
    "        else:\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            layers = []\n",
    "            for idx, stride in enumerate(strides):\n",
    "                # twist = twist and idx < 3\n",
    "                layers.append(block(self.in_channels, channels, stride))\n",
    "                self.in_channels = channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "net = ResNet34()\n",
    "# net = ResNet(PDEBlock, [3,3,3])\n",
    "epoch = 0 \n",
    "lr = 0.1\n",
    "checkpoint = {'acc': 0, 'epoch': 0}\n",
    "history = [{'acc': 0, 'epoch': 0}]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device =', device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "net.to(device)\n",
    "print('Testing on a random input:')\n",
    "test = torch.randn(1,3,224,224).to(device)\n",
    "print('INPUT ', test.shape)\n",
    "print('OUTPUT', net(test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ozKUGgKwcruw",
    "outputId": "b786d887-d97a-4ee0-fa48-4315cd09769f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.3,0.3), scale=(0.8,1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainset = torchvision.datasets.ImageFolder(root='./data/imagenette2/train', transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testset = torchvision.datasets.ImageFolder(root='./data/imagenette2/val', transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1RzI4cWWHVlg",
    "outputId": "2aaf2933-e636-48b5-8e6b-6c7690c31c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twist on\n",
      "testing on random initial weights:\n",
      "test  loss: 2.305 | acc: 10.80  ( 424/3925) (up by 10.80)\n",
      "Epoch 0 (lr=0.1000)\n",
      "train loss: 2.294 | acc: 14.162 (1341/9469)\n",
      "test  loss: 2.292 | acc: 16.38  ( 643/3925) (up by 5.58)\n",
      "Epoch 1 (lr=0.1000)\n",
      "train loss: 2.125 | acc: 21.291 (2016/9469)\n",
      "test  loss: 2.119 | acc: 22.01  ( 864/3925) (up by 5.63)\n",
      "Epoch 2 (lr=0.1000)\n",
      "train loss: 2.050 | acc: 24.543 (2324/9469)\n",
      "test  loss: 2.044 | acc: 24.92  ( 978/3925) (up by 2.90)\n",
      "Epoch 3 (lr=0.1000)\n",
      "train loss: 1.984 | acc: 27.585 (2612/9469)\n",
      "test  loss: 3.112 | acc: 17.38  ( 682/3925)\n",
      "Epoch 4 (lr=0.1000)\n",
      "train loss: 1.974 | acc: 29.443 (2788/9469)\n",
      "test  loss: 2.181 | acc: 18.88  ( 741/3925)\n",
      "Epoch 5 (lr=0.1000)\n",
      "train loss: 1.816 | acc: 36.392 (3446/9469)\n",
      "test  loss: 1.833 | acc: 36.25  ( 1423/3925) (up by 11.34)\n",
      "Epoch 6 (lr=0.1000)\n",
      "train loss: 1.728 | acc: 39.159 (3708/9469)\n",
      "test  loss: 4.603 | acc: 20.15  ( 791/3925)\n",
      "Epoch 7 (lr=0.1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/www/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/www/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/www/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/www/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-889552641431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-889552641431>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loss_func, opt)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "def train(loss_func, opt):\n",
    "    global history\n",
    "    print('Epoch {} (lr={:.4f})'.format(epoch, lr))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (x, y) in enumerate(trainloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = net(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = pred.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "    print('train loss: {:.3f} | acc: {:.3f} ({}/{})'.format(\n",
    "        train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    history.append({'epoch': epoch, 'train_loss': train_loss, 'train_acc': 100. * correct / total})\n",
    "    \n",
    "def test(loss_func):\n",
    "    global checkpoint, history\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(testloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = net(x)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = pred.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    acc = 100. * correct / total\n",
    "    history[-1]['loss'] = test_loss\n",
    "    history[-1]['acc'] = acc\n",
    "    if acc > checkpoint['acc']:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{}) (up by {:.2f})'.format(\n",
    "               test_loss / (batch_idx + 1), 100. * correct / total, correct, total,\n",
    "               acc - checkpoint['acc']))\n",
    "        # print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'lr': lr,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        checkpoint = state\n",
    "        # if not os.path.isdir('checkpoint'):\n",
    "        #     os.mkdir('checkpoint')\n",
    "        # torch.save(state, './checkpoint/ckpt.pth')\n",
    "    else:\n",
    "        print('test  loss: {:.3f} | acc: {:.2f}  ( {}/{})'.format(\n",
    "            test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)  # 5e-4\n",
    "\n",
    "\n",
    "from math import exp, e\n",
    "\n",
    "def lr_schedule(x, lr):\n",
    "    x0 = 10\n",
    "    y0 = 0.1\n",
    "    return 0.001 + exp(- x / x0) * x * e * y0 / x0\n",
    "#     if x < x0:\n",
    "#         return 0.001 if x == 0 else lr + y0 / x0\n",
    "#     elif x < 100:\n",
    "#         return 0.001 + (lr - 0.001) * 0.95\n",
    "\n",
    "for _ in range(10):\n",
    "    global epoch, checkpoint, history\n",
    "    if history[-1].get('train_acc', 0) > 99.99:\n",
    "        break\n",
    "    if epoch == 0:\n",
    "        m = net.module\n",
    "        for layer in [m.layer1]: #, m.layer2, m.layer3]: #, m.layer4]:\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].twist = True\n",
    "        print(\"twist on\")\n",
    "        print('testing on random initial weights:')\n",
    "        test(loss_func)\n",
    "    if epoch < 0:\n",
    "        lr = lr_schedule(epoch, lr)\n",
    "        for param_group in opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epoch - checkpoint['epoch'] >= 30:\n",
    "        print('\\nloading state_dict from Epoch {} (acc = {})'.format(checkpoint['epoch'], checkpoint['acc']))\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        m = net.module\n",
    "        for layer in [m.layer1, m.layer2]: #, m.layer3]: #, m.layer4]:\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].twist = True\n",
    "        test(loss_func)\n",
    "        lr = checkpoint['lr'] * 0.1\n",
    "        print('\\nlearning rate downgraded to {} at epoch {}'.format(lr, epoch))\n",
    "        checkpoint['epoch'] = epoch\n",
    "        history.append({'epoch': checkpoint['epoch'], 'acc': checkpoint['acc']})\n",
    "        opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "    train(loss_func, opt)\n",
    "    test(loss_func)\n",
    "    epoch += 1\n",
    "print('finish at lr = {}, acc = {}'.format(lr, checkpoint['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0WrCG8iuIB_",
    "outputId": "c5117046-7752-4f8b-de48-21bd325d4fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0017278349259868264 0.24676665663719177 torch.Size([8, 3, 3, 3])\n",
      "\n",
      "-0.003532852977514267 0.1745581179857254 0.034341562539339066\n",
      "-0.006805328652262688 0.14749710261821747 0.03434890881180763\n",
      "0.007838931865990162 0.15485425293445587 0.022962460294365883\n",
      "\n",
      "-0.006107522640377283 0.10852225124835968 0.018574967980384827\n",
      "-0.004374963231384754 0.12109889090061188 0.02475770376622677\n",
      "0.008549327962100506 0.11630651354789734 0.02603377401828766\n",
      "\n",
      "-0.00048096803948283195 0.08133669197559357 0.015090527012944221\n",
      "-0.0058867610059678555 0.09106168895959854 0.016228066757321358\n",
      "0.0011466664727777243 0.05562036111950874 0.00782526470720768\n"
     ]
    }
   ],
   "source": [
    "m = net.module\n",
    "print(m.conv1.weight.mean().item(), m.conv1.weight.std().item(), m.conv1.weight.shape)\n",
    "for layer in [m.layer1, m.layer2, m.layer3]: #, m.layer4]:\n",
    "    print()\n",
    "    for i in range(len(layer)):\n",
    "        print(layer[i].conv.weight.mean().item(), layer[i].conv.weight.std().item(), layer[i].convx.weight.std().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7B52zeW88ef"
   },
   "source": [
    "### Results: \n",
    "\n",
    "Training with lr=[0.1, 0.01, 0.001], downgrading if plateaued for 20 epochs.\n",
    "\n",
    "Baseline (classic ResNet)\n",
    "\n",
    "* ResNet50: 94.29 Bottleneck 32, [3,4,6,3], twist=[F,F,F,F]\n",
    "\n",
    "With \"twist\":\n",
    "* 94.52 BasicBlock 64, [2,2,2,2], twist=[T,T,T,T], rotation_aug=10\n",
    "\n",
    "* 94.15 Bottleneck 32, [3,4,6,3], twist=[T,T,T,T]\n",
    "* 94.59 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.28 Bottleneck 32, [3,4,6,3], twist=[T,F,F,F]\n",
    "* 94.84 Bottleneck 32, [8,8,8,3], twist=[T,T,T,F]\n",
    "* 94.22 Bottleneck 32, [8,8,16,3], twist=[T,T,T,F]\n",
    "\n",
    "When training with lr= x e^(-x) -- rising linearly, then falling off exponentially -- it converges faster, in about 100 epochs\n",
    "\n",
    "* 93.98 ResNet50 Bottleneck 16\n",
    "* 94.53 (train_acc=96.13) , BasicBlock 64, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 93.19 Bottleneck 16, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.06 Bottleneck 32, [3,4,6,3], twist=[T,T,T,F]\n",
    "* 94.72 Bottleneck 64, [3,4,6,3], twist=[T,T,T,F]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQ90kBl6uICE",
    "outputId": "cb8a3557-6c80-4f92-fee8-918da5ad2c66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    " [2.8104, 2.6607, 2.0765, 1.7392, 1.5272, 1.3853, 1.1833, 1.1261, 0.8286,\n",
    "        0.7381, 0.6732, 0.5530, 0.4448, 0.3987, 0.3280, 0.2829, 0.2381, 0.2129,\n",
    "        0.1887, 0.1716, 0.1538, 0.1227, 0.1100, 0.0991, 0.0793, 0.0575, 0.0470],\n",
    " [2.5849, 2.4312, 2.0399, 1.4542, 1.3491, 1.2283, 1.0040, 0.7782, 0.7403,\n",
    "        0.6404, 0.4244, 0.4096, 0.3201, 0.2533, 0.2170, 0.1899, 0.1393, 0.1194,\n",
    "        0.1111, 0.0810, 0.0702, 0.0599, 0.0491, 0.0427, 0.0316, 0.0281, 0.0211],\n",
    " [2.3489, 2.1176, 1.8689, 1.3103, 1.1440, 1.1354, 0.8471, 0.7960, 0.6468,\n",
    "        0.4969, 0.4144, 0.3684, 0.2797, 0.2498, 0.1899, 0.1561, 0.1421, 0.1118,\n",
    "        0.0815, 0.0706, 0.0602, 0.0536, 0.0389, 0.0341, 0.0209, 0.0138, 0.0093],\n",
    " [2.3597, 2.0739, 1.8342, 1.2758, 1.1094, 1.0668, 0.9078, 0.8299, 0.6184,\n",
    "        0.5731, 0.4374, 0.4200, 0.2464, 0.2361, 0.2333, 0.1684, 0.1497, 0.1077,\n",
    "        0.0909, 0.0606, 0.0542, 0.0475, 0.0420, 0.0290, 0.0157, 0.0121, 0.0053],\n",
    " [2.4256, 2.0896, 1.8365, 1.3078, 1.1393, 1.1217, 1.0316, 0.8114, 0.6341,\n",
    "        0.5491, 0.4557, 0.3869, 0.2779, 0.2309, 0.1898, 0.1831, 0.1766, 0.1271,\n",
    "        0.0781, 0.0603, 0.0509, 0.0406, 0.0363, 0.0292, 0.0185, 0.0120, 0.0060],\n",
    " [2.4849, 2.0532, 1.8571, 1.3805, 1.1323, 1.1066, 1.0433, 0.8157, 0.6629,\n",
    "        0.5746, 0.4429, 0.3829, 0.2724, 0.2594, 0.1690, 0.1577, 0.1473, 0.1279,\n",
    "        0.0809, 0.0602, 0.0458, 0.0376, 0.0331, 0.0263, 0.0196, 0.0110, 0.0061],\n",
    " [2.4709, 2.0658, 1.8636, 1.4249, 1.1889, 1.1478, 0.9837, 0.7942, 0.6960,\n",
    "        0.5268, 0.4503, 0.4186, 0.2792, 0.2359, 0.1945, 0.1636, 0.1547, 0.1163,\n",
    "        0.0766, 0.0685, 0.0539, 0.0411, 0.0342, 0.0238, 0.0154, 0.0120, 0.0039],\n",
    " [2.5026, 2.0597, 1.8135, 1.4707, 1.2017, 1.0554, 0.9883, 0.8208, 0.7253,\n",
    "        0.5223, 0.4767, 0.4090, 0.3169, 0.2375, 0.1904, 0.1654, 0.1372, 0.1143,\n",
    "        0.0842, 0.0540, 0.0447, 0.0322, 0.0279, 0.0192, 0.0152, 0.0099, 0.0069],\n",
    " [2.5069, 2.0750, 1.7927, 1.5006, 1.2215, 1.1006, 0.9533, 0.8030, 0.7206,\n",
    "        0.5561, 0.4817, 0.4372, 0.3454, 0.2270, 0.1583, 0.1296, 0.1175, 0.0945,\n",
    "        0.0714, 0.0528, 0.0451, 0.0355, 0.0255, 0.0217, 0.0172, 0.0125, 0.0101],\n",
    " [2.5199, 2.0277, 1.7792, 1.4563, 1.2592, 1.0659, 0.9456, 0.8021, 0.7113,\n",
    "        0.5484, 0.4876, 0.4590, 0.3279, 0.1928, 0.1862, 0.1847, 0.1571, 0.1049,\n",
    "        0.0790, 0.0580, 0.0396, 0.0352, 0.0295, 0.0217, 0.0183, 0.0103, 0.0057],\n",
    " [2.3893, 1.9121, 1.6831, 1.3817, 1.1941, 1.0182, 0.9111, 0.7532, 0.6538,\n",
    "        0.5459, 0.4672, 0.4358, 0.3111, 0.1830, 0.1812, 0.1646, 0.1533, 0.0977,\n",
    "        0.0770, 0.0575, 0.0403, 0.0324, 0.0275, 0.0221, 0.0167, 0.0096, 0.0064],\n",
    " [2.2690, 1.8165, 1.5975, 1.3160, 1.1309, 0.9703, 0.8707, 0.7099, 0.6191,\n",
    "        0.5291, 0.4453, 0.4059, 0.2917, 0.1800, 0.1698, 0.1525, 0.1403, 0.0821,\n",
    "        0.0720, 0.0552, 0.0420, 0.0310, 0.0275, 0.0211, 0.0167, 0.0098, 0.0062],\n",
    " [2.1601, 1.7282, 1.5242, 1.2564, 1.0730, 0.9203, 0.8337, 0.6863, 0.5854,\n",
    "        0.5140, 0.4252, 0.3822, 0.2758, 0.1659, 0.1575, 0.1521, 0.1334, 0.0783,\n",
    "        0.0741, 0.0565, 0.0421, 0.0305, 0.0250, 0.0212, 0.0163, 0.0090, 0.0055],\n",
    " [2.0608, 1.6436, 1.4555, 1.2012, 1.0342, 0.8702, 0.7983, 0.6782, 0.5550,\n",
    "        0.4913, 0.4171, 0.3638, 0.2737, 0.1557, 0.1502, 0.1329, 0.1208, 0.0756,\n",
    "        0.0704, 0.0500, 0.0414, 0.0295, 0.0238, 0.0203, 0.0151, 0.0089, 0.0044],\n",
    " [2.0513, 1.6338, 1.4452, 1.1936, 1.0283, 0.8667, 0.7949, 0.6695, 0.5496,\n",
    "        0.4944, 0.4150, 0.3623, 0.2676, 0.1548, 0.1484, 0.1310, 0.1200, 0.0749,\n",
    "        0.0701, 0.0505, 0.0418, 0.0292, 0.0238, 0.0202, 0.0152, 0.0090, 0.0041]\n",
    "]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4S2q8OxuICH",
    "outputId": "5f9559d5-dcd2-4726-c31a-9ce53ba8c69e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': 7.94, 'epoch': 0, 'loss': 230.29033517837524},\n",
       " {'epoch': 0,\n",
       "  'train_loss': 738.2270909547806,\n",
       "  'train_acc': 28.212,\n",
       "  'loss': 178.15970933437347,\n",
       "  'acc': 35.03},\n",
       " {'epoch': 1,\n",
       "  'train_loss': 636.0984075069427,\n",
       "  'train_acc': 39.642,\n",
       "  'loss': 171.37144315242767,\n",
       "  'acc': 39.55},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 579.8472344875336,\n",
       "  'train_acc': 45.41,\n",
       "  'loss': 189.14156591892242,\n",
       "  'acc': 44.0},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 551.47753739357,\n",
       "  'train_acc': 48.558,\n",
       "  'loss': 140.14471018314362,\n",
       "  'acc': 52.36},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 531.3351120948792,\n",
       "  'train_acc': 50.55,\n",
       "  'loss': 194.9198615550995,\n",
       "  'acc': 41.94},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 514.1015273332596,\n",
       "  'train_acc': 52.394,\n",
       "  'loss': 174.3349907398224,\n",
       "  'acc': 45.35},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 500.8305673599243,\n",
       "  'train_acc': 53.652,\n",
       "  'loss': 155.5255310535431,\n",
       "  'acc': 50.97},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 490.45059484243393,\n",
       "  'train_acc': 54.622,\n",
       "  'loss': 181.0910484790802,\n",
       "  'acc': 47.19},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 479.7907404303551,\n",
       "  'train_acc': 56.322,\n",
       "  'loss': 177.98825025558472,\n",
       "  'acc': 49.82},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 471.8980811238289,\n",
       "  'train_acc': 57.056,\n",
       "  'loss': 132.81695175170898,\n",
       "  'acc': 54.59},\n",
       " {'epoch': 10,\n",
       "  'train_loss': 463.8471734523773,\n",
       "  'train_acc': 57.824,\n",
       "  'loss': 142.9321995973587,\n",
       "  'acc': 55.65},\n",
       " {'epoch': 11,\n",
       "  'train_loss': 458.0706177353859,\n",
       "  'train_acc': 58.396,\n",
       "  'loss': 203.06889259815216,\n",
       "  'acc': 48.29},\n",
       " {'epoch': 12,\n",
       "  'train_loss': 451.53625416755676,\n",
       "  'train_acc': 58.982,\n",
       "  'loss': 130.6719849705696,\n",
       "  'acc': 60.76},\n",
       " {'epoch': 13,\n",
       "  'train_loss': 446.4423971772194,\n",
       "  'train_acc': 59.84,\n",
       "  'loss': 142.5071257352829,\n",
       "  'acc': 54.8},\n",
       " {'epoch': 14,\n",
       "  'train_loss': 444.2825377583504,\n",
       "  'train_acc': 59.764,\n",
       "  'loss': 115.22107070684433,\n",
       "  'acc': 61.55},\n",
       " {'epoch': 15,\n",
       "  'train_loss': 440.4220908880234,\n",
       "  'train_acc': 60.116,\n",
       "  'loss': 154.2152839899063,\n",
       "  'acc': 53.09},\n",
       " {'epoch': 16,\n",
       "  'train_loss': 440.02454632520676,\n",
       "  'train_acc': 60.134,\n",
       "  'loss': 114.73185539245605,\n",
       "  'acc': 61.15},\n",
       " {'epoch': 17,\n",
       "  'train_loss': 435.1574607491493,\n",
       "  'train_acc': 60.778,\n",
       "  'loss': 122.80402731895447,\n",
       "  'acc': 60.4},\n",
       " {'epoch': 18,\n",
       "  'train_loss': 432.9562475681305,\n",
       "  'train_acc': 61.016,\n",
       "  'loss': 128.84126156568527,\n",
       "  'acc': 57.26},\n",
       " {'epoch': 19,\n",
       "  'train_loss': 432.1044429540634,\n",
       "  'train_acc': 61.176,\n",
       "  'loss': 144.05410373210907,\n",
       "  'acc': 55.7},\n",
       " {'epoch': 20,\n",
       "  'train_loss': 430.7617779970169,\n",
       "  'train_acc': 61.236,\n",
       "  'loss': 136.0675951242447,\n",
       "  'acc': 56.09},\n",
       " {'epoch': 21,\n",
       "  'train_loss': 428.9211485981941,\n",
       "  'train_acc': 61.372,\n",
       "  'loss': 104.14077472686768,\n",
       "  'acc': 64.5},\n",
       " {'epoch': 22,\n",
       "  'train_loss': 427.9161514043808,\n",
       "  'train_acc': 61.224,\n",
       "  'loss': 107.75971710681915,\n",
       "  'acc': 62.62},\n",
       " {'epoch': 23,\n",
       "  'train_loss': 429.0370543003082,\n",
       "  'train_acc': 61.344,\n",
       "  'loss': 106.46120828390121,\n",
       "  'acc': 63.34},\n",
       " {'epoch': 24,\n",
       "  'train_loss': 423.0295475721359,\n",
       "  'train_acc': 61.85,\n",
       "  'loss': 123.90346109867096,\n",
       "  'acc': 60.01},\n",
       " {'epoch': 25,\n",
       "  'train_loss': 421.8919516801834,\n",
       "  'train_acc': 62.01,\n",
       "  'loss': 121.23485201597214,\n",
       "  'acc': 60.93},\n",
       " {'epoch': 26,\n",
       "  'train_loss': 424.1937192082405,\n",
       "  'train_acc': 61.85,\n",
       "  'loss': 159.2300170660019,\n",
       "  'acc': 52.47},\n",
       " {'epoch': 27,\n",
       "  'train_loss': 419.28503155708313,\n",
       "  'train_acc': 62.302,\n",
       "  'loss': 130.1863590478897,\n",
       "  'acc': 60.81},\n",
       " {'epoch': 28,\n",
       "  'train_loss': 421.4518259167671,\n",
       "  'train_acc': 62.092,\n",
       "  'loss': 103.45227521657944,\n",
       "  'acc': 65.87},\n",
       " {'epoch': 29,\n",
       "  'train_loss': 417.72643488645554,\n",
       "  'train_acc': 62.662,\n",
       "  'loss': 107.30797588825226,\n",
       "  'acc': 65.22},\n",
       " {'epoch': 30,\n",
       "  'train_loss': 417.8565636873245,\n",
       "  'train_acc': 62.276,\n",
       "  'loss': 114.32171994447708,\n",
       "  'acc': 61.9},\n",
       " {'epoch': 31,\n",
       "  'train_loss': 419.7934036850929,\n",
       "  'train_acc': 62.476,\n",
       "  'loss': 103.35448086261749,\n",
       "  'acc': 66.23},\n",
       " {'epoch': 32,\n",
       "  'train_loss': 421.1802458167076,\n",
       "  'train_acc': 62.102,\n",
       "  'loss': 166.53725469112396,\n",
       "  'acc': 51.82},\n",
       " {'epoch': 33,\n",
       "  'train_loss': 420.2717648744583,\n",
       "  'train_acc': 62.14,\n",
       "  'loss': 143.2384340763092,\n",
       "  'acc': 56.15},\n",
       " {'epoch': 34,\n",
       "  'train_loss': 417.03478622436523,\n",
       "  'train_acc': 62.502,\n",
       "  'loss': 160.06623232364655,\n",
       "  'acc': 52.04},\n",
       " {'epoch': 35,\n",
       "  'train_loss': 417.006206035614,\n",
       "  'train_acc': 62.622,\n",
       "  'loss': 100.60840022563934,\n",
       "  'acc': 65.95},\n",
       " {'epoch': 36,\n",
       "  'train_loss': 416.5966856479645,\n",
       "  'train_acc': 62.802,\n",
       "  'loss': 125.6378487944603,\n",
       "  'acc': 59.37},\n",
       " {'epoch': 37,\n",
       "  'train_loss': 415.4964094758034,\n",
       "  'train_acc': 62.476,\n",
       "  'loss': 138.92346322536469,\n",
       "  'acc': 57.65},\n",
       " {'epoch': 38,\n",
       "  'train_loss': 413.95806699991226,\n",
       "  'train_acc': 62.97,\n",
       "  'loss': 111.07996189594269,\n",
       "  'acc': 64.1},\n",
       " {'epoch': 39,\n",
       "  'train_loss': 414.96757662296295,\n",
       "  'train_acc': 62.706,\n",
       "  'loss': 127.83512616157532,\n",
       "  'acc': 60.18},\n",
       " {'epoch': 40,\n",
       "  'train_loss': 413.0586014389992,\n",
       "  'train_acc': 62.828,\n",
       "  'loss': 141.92954170703888,\n",
       "  'acc': 57.81},\n",
       " {'epoch': 41,\n",
       "  'train_loss': 412.8932564854622,\n",
       "  'train_acc': 63.024,\n",
       "  'loss': 140.50568318367004,\n",
       "  'acc': 60.42},\n",
       " {'epoch': 42,\n",
       "  'train_loss': 413.82101875543594,\n",
       "  'train_acc': 63.026,\n",
       "  'loss': 114.80342000722885,\n",
       "  'acc': 63.61},\n",
       " {'epoch': 43,\n",
       "  'train_loss': 412.1435295343399,\n",
       "  'train_acc': 62.956,\n",
       "  'loss': 120.30436187982559,\n",
       "  'acc': 62.07},\n",
       " {'epoch': 44,\n",
       "  'train_loss': 414.55801302194595,\n",
       "  'train_acc': 62.978,\n",
       "  'loss': 125.41475230455399,\n",
       "  'acc': 59.8},\n",
       " {'epoch': 45,\n",
       "  'train_loss': 413.5053839087486,\n",
       "  'train_acc': 62.676,\n",
       "  'loss': 190.6724443435669,\n",
       "  'acc': 47.99},\n",
       " {'epoch': 46,\n",
       "  'train_loss': 412.4328973889351,\n",
       "  'train_acc': 63.084,\n",
       "  'loss': 112.78260564804077,\n",
       "  'acc': 63.73},\n",
       " {'epoch': 47,\n",
       "  'train_loss': 413.0850810408592,\n",
       "  'train_acc': 62.77,\n",
       "  'loss': 150.0651512145996,\n",
       "  'acc': 58.0},\n",
       " {'epoch': 48,\n",
       "  'train_loss': 411.9250520467758,\n",
       "  'train_acc': 63.16,\n",
       "  'loss': 132.98118591308594,\n",
       "  'acc': 57.66},\n",
       " {'epoch': 49,\n",
       "  'train_loss': 410.1146569252014,\n",
       "  'train_acc': 63.012,\n",
       "  'loss': 115.78604704141617,\n",
       "  'acc': 62.77},\n",
       " {'epoch': 50,\n",
       "  'train_loss': 411.6327002644539,\n",
       "  'train_acc': 62.856,\n",
       "  'loss': 143.3079478740692,\n",
       "  'acc': 54.84},\n",
       " {'epoch': 51,\n",
       "  'train_loss': 408.7979579567909,\n",
       "  'train_acc': 63.3,\n",
       "  'loss': 148.39080649614334,\n",
       "  'acc': 56.69},\n",
       " {'epoch': 52,\n",
       "  'train_loss': 412.1635176539421,\n",
       "  'train_acc': 63.082,\n",
       "  'loss': 97.69706243276596,\n",
       "  'acc': 66.86},\n",
       " {'epoch': 53,\n",
       "  'train_loss': 409.4167826771736,\n",
       "  'train_acc': 63.254,\n",
       "  'loss': 121.84178549051285,\n",
       "  'acc': 60.24},\n",
       " {'epoch': 54,\n",
       "  'train_loss': 411.3081858754158,\n",
       "  'train_acc': 63.096,\n",
       "  'loss': 116.41163623332977,\n",
       "  'acc': 63.72},\n",
       " {'epoch': 55,\n",
       "  'train_loss': 412.1312629580498,\n",
       "  'train_acc': 62.916,\n",
       "  'loss': 123.50228673219681,\n",
       "  'acc': 61.08},\n",
       " {'epoch': 56,\n",
       "  'train_loss': 407.4055666923523,\n",
       "  'train_acc': 63.394,\n",
       "  'loss': 129.785799741745,\n",
       "  'acc': 59.02},\n",
       " {'epoch': 57,\n",
       "  'train_loss': 407.3787354230881,\n",
       "  'train_acc': 63.588,\n",
       "  'loss': 209.45217430591583,\n",
       "  'acc': 46.09},\n",
       " {'epoch': 58,\n",
       "  'train_loss': 409.74256056547165,\n",
       "  'train_acc': 63.212,\n",
       "  'loss': 105.55689305067062,\n",
       "  'acc': 63.9},\n",
       " {'epoch': 59,\n",
       "  'train_loss': 407.5036550760269,\n",
       "  'train_acc': 63.462,\n",
       "  'loss': 148.43070149421692,\n",
       "  'acc': 55.86},\n",
       " {'epoch': 60,\n",
       "  'train_loss': 409.5428103208542,\n",
       "  'train_acc': 63.188,\n",
       "  'loss': 133.32401663064957,\n",
       "  'acc': 58.56},\n",
       " {'epoch': 61,\n",
       "  'train_loss': 409.5941159129143,\n",
       "  'train_acc': 63.41,\n",
       "  'loss': 124.66500186920166,\n",
       "  'acc': 60.0},\n",
       " {'epoch': 62,\n",
       "  'train_loss': 406.27338898181915,\n",
       "  'train_acc': 63.46,\n",
       "  'loss': 105.51278293132782,\n",
       "  'acc': 64.99},\n",
       " {'epoch': 63,\n",
       "  'train_loss': 406.0270575284958,\n",
       "  'train_acc': 63.554,\n",
       "  'loss': 127.86127698421478,\n",
       "  'acc': 59.2},\n",
       " {'epoch': 64,\n",
       "  'train_loss': 408.03316646814346,\n",
       "  'train_acc': 63.246,\n",
       "  'loss': 128.33413702249527,\n",
       "  'acc': 59.04},\n",
       " {'epoch': 65,\n",
       "  'train_loss': 410.4170306324959,\n",
       "  'train_acc': 63.146,\n",
       "  'loss': 132.38275718688965,\n",
       "  'acc': 60.86},\n",
       " {'epoch': 66,\n",
       "  'train_loss': 409.7235361337662,\n",
       "  'train_acc': 63.408,\n",
       "  'loss': 127.36349594593048,\n",
       "  'acc': 59.97},\n",
       " {'epoch': 67,\n",
       "  'train_loss': 403.35653245449066,\n",
       "  'train_acc': 63.648,\n",
       "  'loss': 105.55532902479172,\n",
       "  'acc': 63.93},\n",
       " {'epoch': 68,\n",
       "  'train_loss': 406.1975855231285,\n",
       "  'train_acc': 63.77,\n",
       "  'loss': 123.11526453495026,\n",
       "  'acc': 62.66},\n",
       " {'epoch': 69,\n",
       "  'train_loss': 405.65017408132553,\n",
       "  'train_acc': 63.644,\n",
       "  'loss': 133.0424771308899,\n",
       "  'acc': 60.15},\n",
       " {'epoch': 70,\n",
       "  'train_loss': 403.5761843919754,\n",
       "  'train_acc': 63.624,\n",
       "  'loss': 129.58693265914917,\n",
       "  'acc': 60.47},\n",
       " {'epoch': 71,\n",
       "  'train_loss': 407.38002932071686,\n",
       "  'train_acc': 63.724,\n",
       "  'loss': 129.9150927066803,\n",
       "  'acc': 59.87},\n",
       " {'epoch': 72,\n",
       "  'train_loss': 407.0946271419525,\n",
       "  'train_acc': 63.462,\n",
       "  'loss': 153.03065264225006,\n",
       "  'acc': 54.41},\n",
       " {'epoch': 73,\n",
       "  'train_loss': 405.472450196743,\n",
       "  'train_acc': 63.534,\n",
       "  'loss': 137.9898825287819,\n",
       "  'acc': 56.59},\n",
       " {'epoch': 74,\n",
       "  'train_loss': 403.7599168419838,\n",
       "  'train_acc': 63.874,\n",
       "  'loss': 166.56068980693817,\n",
       "  'acc': 53.69},\n",
       " {'epoch': 75,\n",
       "  'train_loss': 403.4383270740509,\n",
       "  'train_acc': 64.126,\n",
       "  'loss': 138.28345888853073,\n",
       "  'acc': 59.48},\n",
       " {'epoch': 76,\n",
       "  'train_loss': 402.9052257537842,\n",
       "  'train_acc': 64.094,\n",
       "  'loss': 105.18832641839981,\n",
       "  'acc': 64.39},\n",
       " {'epoch': 77,\n",
       "  'train_loss': 405.40900641679764,\n",
       "  'train_acc': 63.658,\n",
       "  'loss': 108.40590459108353,\n",
       "  'acc': 63.72},\n",
       " {'epoch': 78,\n",
       "  'train_loss': 405.451856136322,\n",
       "  'train_acc': 63.586,\n",
       "  'loss': 98.81379741430283,\n",
       "  'acc': 65.99},\n",
       " {'epoch': 79,\n",
       "  'train_loss': 407.25385171175003,\n",
       "  'train_acc': 63.412,\n",
       "  'loss': 115.2299056649208,\n",
       "  'acc': 61.41},\n",
       " {'epoch': 80,\n",
       "  'train_loss': 403.7665764093399,\n",
       "  'train_acc': 63.996,\n",
       "  'loss': 99.57981473207474,\n",
       "  'acc': 64.95},\n",
       " {'epoch': 81,\n",
       "  'train_loss': 404.25278210639954,\n",
       "  'train_acc': 63.952,\n",
       "  'loss': 107.0367306470871,\n",
       "  'acc': 62.97},\n",
       " {'epoch': 82, 'acc': 66.86},\n",
       " {'epoch': 82,\n",
       "  'train_loss': 341.30568194389343,\n",
       "  'train_acc': 69.578,\n",
       "  'loss': 70.85937589406967,\n",
       "  'acc': 75.82},\n",
       " {'epoch': 83,\n",
       "  'train_loss': 323.0655946135521,\n",
       "  'train_acc': 71.144,\n",
       "  'loss': 76.09151989221573,\n",
       "  'acc': 73.81},\n",
       " {'epoch': 84,\n",
       "  'train_loss': 318.0264060497284,\n",
       "  'train_acc': 71.518,\n",
       "  'loss': 68.7353709936142,\n",
       "  'acc': 76.13},\n",
       " {'epoch': 85,\n",
       "  'train_loss': 313.34346359968185,\n",
       "  'train_acc': 72.058,\n",
       "  'loss': 68.24282163381577,\n",
       "  'acc': 76.6},\n",
       " {'epoch': 86,\n",
       "  'train_loss': 310.4363912343979,\n",
       "  'train_acc': 72.322,\n",
       "  'loss': 68.02798795700073,\n",
       "  'acc': 76.37},\n",
       " {'epoch': 87,\n",
       "  'train_loss': 309.015665769577,\n",
       "  'train_acc': 72.69,\n",
       "  'loss': 68.42187815904617,\n",
       "  'acc': 76.43},\n",
       " {'epoch': 88,\n",
       "  'train_loss': 308.50392961502075,\n",
       "  'train_acc': 72.29,\n",
       "  'loss': 68.51713815331459,\n",
       "  'acc': 76.51},\n",
       " {'epoch': 89,\n",
       "  'train_loss': 307.98132878541946,\n",
       "  'train_acc': 72.54,\n",
       "  'loss': 69.07318365573883,\n",
       "  'acc': 76.07},\n",
       " {'epoch': 90,\n",
       "  'train_loss': 305.4764956831932,\n",
       "  'train_acc': 72.54,\n",
       "  'loss': 69.65474358201027,\n",
       "  'acc': 76.25},\n",
       " {'epoch': 91,\n",
       "  'train_loss': 306.0297721028328,\n",
       "  'train_acc': 72.786,\n",
       "  'loss': 70.76044496893883,\n",
       "  'acc': 75.98},\n",
       " {'epoch': 92,\n",
       "  'train_loss': 305.59649246931076,\n",
       "  'train_acc': 72.582,\n",
       "  'loss': 67.50886824727058,\n",
       "  'acc': 76.98},\n",
       " {'epoch': 93,\n",
       "  'train_loss': 306.18385887145996,\n",
       "  'train_acc': 72.582,\n",
       "  'loss': 74.09482738375664,\n",
       "  'acc': 75.4},\n",
       " {'epoch': 94,\n",
       "  'train_loss': 302.7964052557945,\n",
       "  'train_acc': 72.892,\n",
       "  'loss': 64.88788518309593,\n",
       "  'acc': 77.42},\n",
       " {'epoch': 95,\n",
       "  'train_loss': 303.11920577287674,\n",
       "  'train_acc': 72.956,\n",
       "  'loss': 64.67348656058311,\n",
       "  'acc': 77.43},\n",
       " {'epoch': 96,\n",
       "  'train_loss': 304.83571577072144,\n",
       "  'train_acc': 72.722,\n",
       "  'loss': 69.0525486767292,\n",
       "  'acc': 76.4},\n",
       " {'epoch': 97,\n",
       "  'train_loss': 304.5652720928192,\n",
       "  'train_acc': 72.724,\n",
       "  'loss': 66.8873929977417,\n",
       "  'acc': 76.83},\n",
       " {'epoch': 98,\n",
       "  'train_loss': 304.1015141606331,\n",
       "  'train_acc': 72.744,\n",
       "  'loss': 73.32081136107445,\n",
       "  'acc': 75.19},\n",
       " {'epoch': 99,\n",
       "  'train_loss': 303.20975267887115,\n",
       "  'train_acc': 73.064,\n",
       "  'loss': 66.30652901530266,\n",
       "  'acc': 77.26},\n",
       " {'epoch': 100,\n",
       "  'train_loss': 301.3923295736313,\n",
       "  'train_acc': 72.988,\n",
       "  'loss': 71.7441700398922,\n",
       "  'acc': 75.74},\n",
       " {'epoch': 101,\n",
       "  'train_loss': 302.99360942840576,\n",
       "  'train_acc': 72.84,\n",
       "  'loss': 65.95468884706497,\n",
       "  'acc': 77.74},\n",
       " {'epoch': 102,\n",
       "  'train_loss': 300.67807722091675,\n",
       "  'train_acc': 73.208,\n",
       "  'loss': 67.65919950604439,\n",
       "  'acc': 76.79},\n",
       " {'epoch': 103,\n",
       "  'train_loss': 300.1403497457504,\n",
       "  'train_acc': 73.028,\n",
       "  'loss': 73.01064929366112,\n",
       "  'acc': 75.11},\n",
       " {'epoch': 104,\n",
       "  'train_loss': 302.03587514162064,\n",
       "  'train_acc': 72.94,\n",
       "  'loss': 67.76309850811958,\n",
       "  'acc': 77.03},\n",
       " {'epoch': 105,\n",
       "  'train_loss': 300.95794290304184,\n",
       "  'train_acc': 72.744,\n",
       "  'loss': 77.46328231692314,\n",
       "  'acc': 74.22},\n",
       " {'epoch': 106,\n",
       "  'train_loss': 304.0719608068466,\n",
       "  'train_acc': 72.916,\n",
       "  'loss': 80.91326248645782,\n",
       "  'acc': 73.46},\n",
       " {'epoch': 107,\n",
       "  'train_loss': 300.1443775296211,\n",
       "  'train_acc': 73.114,\n",
       "  'loss': 72.12405371665955,\n",
       "  'acc': 76.06},\n",
       " {'epoch': 108,\n",
       "  'train_loss': 300.4667286872864,\n",
       "  'train_acc': 73.11,\n",
       "  'loss': 79.51378545165062,\n",
       "  'acc': 73.25},\n",
       " {'epoch': 109,\n",
       "  'train_loss': 304.15664476156235,\n",
       "  'train_acc': 73.028,\n",
       "  'loss': 67.33174070715904,\n",
       "  'acc': 76.61},\n",
       " {'epoch': 110,\n",
       "  'train_loss': 302.24218863248825,\n",
       "  'train_acc': 72.952,\n",
       "  'loss': 70.55705714225769,\n",
       "  'acc': 75.81},\n",
       " {'epoch': 111,\n",
       "  'train_loss': 299.5750063061714,\n",
       "  'train_acc': 73.154,\n",
       "  'loss': 68.54655230045319,\n",
       "  'acc': 76.47},\n",
       " {'epoch': 112,\n",
       "  'train_loss': 300.38951313495636,\n",
       "  'train_acc': 73.148,\n",
       "  'loss': 69.18350738286972,\n",
       "  'acc': 75.85},\n",
       " {'epoch': 113,\n",
       "  'train_loss': 300.8732361793518,\n",
       "  'train_acc': 73.108,\n",
       "  'loss': 68.56674724817276,\n",
       "  'acc': 76.94},\n",
       " {'epoch': 114,\n",
       "  'train_loss': 301.431194961071,\n",
       "  'train_acc': 73.002,\n",
       "  'loss': 68.97510465979576,\n",
       "  'acc': 76.11},\n",
       " {'epoch': 115,\n",
       "  'train_loss': 302.0315058231354,\n",
       "  'train_acc': 72.928,\n",
       "  'loss': 66.88496342301369,\n",
       "  'acc': 77.17},\n",
       " {'epoch': 116,\n",
       "  'train_loss': 298.39301282167435,\n",
       "  'train_acc': 73.318,\n",
       "  'loss': 68.19531387090683,\n",
       "  'acc': 76.16},\n",
       " {'epoch': 117,\n",
       "  'train_loss': 298.59616672992706,\n",
       "  'train_acc': 73.314,\n",
       "  'loss': 69.09078922867775,\n",
       "  'acc': 76.77},\n",
       " {'epoch': 118,\n",
       "  'train_loss': 300.3592058420181,\n",
       "  'train_acc': 73.23,\n",
       "  'loss': 72.94537597894669,\n",
       "  'acc': 75.4},\n",
       " {'epoch': 119,\n",
       "  'train_loss': 301.18347960710526,\n",
       "  'train_acc': 73.19,\n",
       "  'loss': 69.93656679987907,\n",
       "  'acc': 76.4},\n",
       " {'epoch': 120,\n",
       "  'train_loss': 300.1714989542961,\n",
       "  'train_acc': 73.306,\n",
       "  'loss': 65.87855976819992,\n",
       "  'acc': 77.34},\n",
       " {'epoch': 121,\n",
       "  'train_loss': 300.03070655465126,\n",
       "  'train_acc': 73.16,\n",
       "  'loss': 69.82995611429214,\n",
       "  'acc': 76.04},\n",
       " {'epoch': 122,\n",
       "  'train_loss': 298.3669774532318,\n",
       "  'train_acc': 73.434,\n",
       "  'loss': 66.71660131216049,\n",
       "  'acc': 77.66},\n",
       " {'epoch': 123,\n",
       "  'train_loss': 299.514428794384,\n",
       "  'train_acc': 73.058,\n",
       "  'loss': 69.91433817148209,\n",
       "  'acc': 76.88},\n",
       " {'epoch': 124,\n",
       "  'train_loss': 298.8887467980385,\n",
       "  'train_acc': 73.256,\n",
       "  'loss': 76.55654853582382,\n",
       "  'acc': 74.75},\n",
       " {'epoch': 125,\n",
       "  'train_loss': 301.9997052550316,\n",
       "  'train_acc': 73.044,\n",
       "  'loss': 69.8816745877266,\n",
       "  'acc': 76.29},\n",
       " {'epoch': 126,\n",
       "  'train_loss': 300.2515594959259,\n",
       "  'train_acc': 73.418,\n",
       "  'loss': 69.85700660943985,\n",
       "  'acc': 76.17},\n",
       " {'epoch': 127,\n",
       "  'train_loss': 298.11909145116806,\n",
       "  'train_acc': 73.132,\n",
       "  'loss': 66.9811861217022,\n",
       "  'acc': 76.99},\n",
       " {'epoch': 128,\n",
       "  'train_loss': 299.374679684639,\n",
       "  'train_acc': 73.388,\n",
       "  'loss': 76.47848150134087,\n",
       "  'acc': 74.31},\n",
       " {'epoch': 129,\n",
       "  'train_loss': 297.776747405529,\n",
       "  'train_acc': 73.288,\n",
       "  'loss': 78.43918368220329,\n",
       "  'acc': 75.6},\n",
       " {'epoch': 130,\n",
       "  'train_loss': 299.6937589645386,\n",
       "  'train_acc': 73.104,\n",
       "  'loss': 66.27224388718605,\n",
       "  'acc': 77.71},\n",
       " {'epoch': 131, 'acc': 77.74},\n",
       " {'epoch': 131,\n",
       "  'train_loss': 281.0980233848095,\n",
       "  'train_acc': 74.95,\n",
       "  'loss': 59.20959493517876,\n",
       "  'acc': 79.85},\n",
       " {'epoch': 132,\n",
       "  'train_loss': 275.76111924648285,\n",
       "  'train_acc': 75.318,\n",
       "  'loss': 59.68773651123047,\n",
       "  'acc': 79.81},\n",
       " {'epoch': 133,\n",
       "  'train_loss': 273.6493226289749,\n",
       "  'train_acc': 75.614,\n",
       "  'loss': 57.929075449705124,\n",
       "  'acc': 80.01},\n",
       " {'epoch': 134,\n",
       "  'train_loss': 271.02974301576614,\n",
       "  'train_acc': 75.878,\n",
       "  'loss': 58.81447824835777,\n",
       "  'acc': 79.93},\n",
       " {'epoch': 135,\n",
       "  'train_loss': 274.3743485510349,\n",
       "  'train_acc': 75.564,\n",
       "  'loss': 58.97482657432556,\n",
       "  'acc': 79.78},\n",
       " {'epoch': 136,\n",
       "  'train_loss': 271.48386082053185,\n",
       "  'train_acc': 75.836,\n",
       "  'loss': 58.7103188931942,\n",
       "  'acc': 79.78},\n",
       " {'epoch': 137,\n",
       "  'train_loss': 269.98551070690155,\n",
       "  'train_acc': 75.908,\n",
       "  'loss': 58.1236826479435,\n",
       "  'acc': 80.01},\n",
       " {'epoch': 138,\n",
       "  'train_loss': 269.9339208304882,\n",
       "  'train_acc': 75.876,\n",
       "  'loss': 58.42501065135002,\n",
       "  'acc': 79.9},\n",
       " {'epoch': 139,\n",
       "  'train_loss': 269.0286301970482,\n",
       "  'train_acc': 75.832,\n",
       "  'loss': 58.324206709861755,\n",
       "  'acc': 79.94},\n",
       " {'epoch': 140,\n",
       "  'train_loss': 268.3639247119427,\n",
       "  'train_acc': 75.794,\n",
       "  'loss': 58.59351545572281,\n",
       "  'acc': 79.76},\n",
       " {'epoch': 141,\n",
       "  'train_loss': 269.4728896021843,\n",
       "  'train_acc': 75.95,\n",
       "  'loss': 59.19020810723305,\n",
       "  'acc': 79.98},\n",
       " {'epoch': 142,\n",
       "  'train_loss': 269.8977611362934,\n",
       "  'train_acc': 75.85,\n",
       "  'loss': 59.889429211616516,\n",
       "  'acc': 79.74},\n",
       " {'epoch': 143,\n",
       "  'train_loss': 269.74713107943535,\n",
       "  'train_acc': 75.892,\n",
       "  'loss': 58.090080827474594,\n",
       "  'acc': 79.98},\n",
       " {'epoch': 144,\n",
       "  'train_loss': 267.5892887413502,\n",
       "  'train_acc': 76.046,\n",
       "  'loss': 59.31444716453552,\n",
       "  'acc': 79.9},\n",
       " {'epoch': 145,\n",
       "  'train_loss': 268.84269693493843,\n",
       "  'train_acc': 76.096,\n",
       "  'loss': 57.969635128974915,\n",
       "  'acc': 80.02},\n",
       " {'epoch': 146,\n",
       "  'train_loss': 267.506984770298,\n",
       "  'train_acc': 75.93,\n",
       "  'loss': 59.320496916770935,\n",
       "  'acc': 79.81},\n",
       " {'epoch': 147,\n",
       "  'train_loss': 267.5109095275402,\n",
       "  'train_acc': 76.042,\n",
       "  'loss': 58.07165679335594,\n",
       "  'acc': 80.27},\n",
       " {'epoch': 148,\n",
       "  'train_loss': 269.89616337418556,\n",
       "  'train_acc': 76.022,\n",
       "  'loss': 58.82784694433212,\n",
       "  'acc': 79.77},\n",
       " {'epoch': 149,\n",
       "  'train_loss': 266.49223840236664,\n",
       "  'train_acc': 76.212,\n",
       "  'loss': 58.63921934366226,\n",
       "  'acc': 80.02},\n",
       " {'epoch': 150,\n",
       "  'train_loss': 266.4732029736042,\n",
       "  'train_acc': 76.176,\n",
       "  'loss': 58.29392093420029,\n",
       "  'acc': 79.98},\n",
       " {'epoch': 151,\n",
       "  'train_loss': 266.75071105360985,\n",
       "  'train_acc': 76.194,\n",
       "  'loss': 57.8159216940403,\n",
       "  'acc': 80.29},\n",
       " {'epoch': 152,\n",
       "  'train_loss': 266.94867807626724,\n",
       "  'train_acc': 76.394,\n",
       "  'loss': 58.28093123435974,\n",
       "  'acc': 80.19},\n",
       " {'epoch': 153,\n",
       "  'train_loss': 266.7815593481064,\n",
       "  'train_acc': 76.22,\n",
       "  'loss': 57.364650309085846,\n",
       "  'acc': 80.26},\n",
       " {'epoch': 154,\n",
       "  'train_loss': 267.8735108971596,\n",
       "  'train_acc': 75.912,\n",
       "  'loss': 58.33171024918556,\n",
       "  'acc': 79.81},\n",
       " {'epoch': 155,\n",
       "  'train_loss': 263.93079575896263,\n",
       "  'train_acc': 76.554,\n",
       "  'loss': 57.3284772336483,\n",
       "  'acc': 80.39},\n",
       " {'epoch': 156,\n",
       "  'train_loss': 265.0512619614601,\n",
       "  'train_acc': 76.312,\n",
       "  'loss': 58.238706946372986,\n",
       "  'acc': 80.23},\n",
       " {'epoch': 157,\n",
       "  'train_loss': 265.0148691236973,\n",
       "  'train_acc': 76.24,\n",
       "  'loss': 57.86584120988846,\n",
       "  'acc': 80.26},\n",
       " {'epoch': 158,\n",
       "  'train_loss': 265.0536479353905,\n",
       "  'train_acc': 76.42,\n",
       "  'loss': 60.026555210351944,\n",
       "  'acc': 79.37},\n",
       " {'epoch': 159,\n",
       "  'train_loss': 264.1837839484215,\n",
       "  'train_acc': 76.254,\n",
       "  'loss': 58.137457460165024,\n",
       "  'acc': 80.1},\n",
       " {'epoch': 160,\n",
       "  'train_loss': 265.02279180288315,\n",
       "  'train_acc': 76.254,\n",
       "  'loss': 58.544277250766754,\n",
       "  'acc': 79.96},\n",
       " {'epoch': 161,\n",
       "  'train_loss': 266.7304507493973,\n",
       "  'train_acc': 76.268,\n",
       "  'loss': 57.17674484848976,\n",
       "  'acc': 80.39},\n",
       " {'epoch': 162,\n",
       "  'train_loss': 266.3200762271881,\n",
       "  'train_acc': 76.13,\n",
       "  'loss': 57.93152463436127,\n",
       "  'acc': 80.06},\n",
       " {'epoch': 163,\n",
       "  'train_loss': 265.07780730724335,\n",
       "  'train_acc': 76.426,\n",
       "  'loss': 57.5532623231411,\n",
       "  'acc': 80.22},\n",
       " {'epoch': 164,\n",
       "  'train_loss': 264.88562217354774,\n",
       "  'train_acc': 76.44,\n",
       "  'loss': 58.14411589503288,\n",
       "  'acc': 80.18},\n",
       " {'epoch': 165,\n",
       "  'train_loss': 266.0876041352749,\n",
       "  'train_acc': 76.142,\n",
       "  'loss': 58.070870131254196,\n",
       "  'acc': 79.96},\n",
       " {'epoch': 166,\n",
       "  'train_loss': 263.7225883603096,\n",
       "  'train_acc': 76.282,\n",
       "  'loss': 58.532532304525375,\n",
       "  'acc': 80.03},\n",
       " {'epoch': 167,\n",
       "  'train_loss': 262.34213081002235,\n",
       "  'train_acc': 76.486,\n",
       "  'loss': 57.740189641714096,\n",
       "  'acc': 80.29},\n",
       " {'epoch': 168,\n",
       "  'train_loss': 265.07593953609467,\n",
       "  'train_acc': 76.344,\n",
       "  'loss': 58.45136830210686,\n",
       "  'acc': 80.05},\n",
       " {'epoch': 169,\n",
       "  'train_loss': 264.09475713968277,\n",
       "  'train_acc': 76.388,\n",
       "  'loss': 58.18940615653992,\n",
       "  'acc': 80.03},\n",
       " {'epoch': 170,\n",
       "  'train_loss': 264.1595677435398,\n",
       "  'train_acc': 76.52,\n",
       "  'loss': 57.8523166179657,\n",
       "  'acc': 80.1},\n",
       " {'epoch': 171,\n",
       "  'train_loss': 266.0715954899788,\n",
       "  'train_acc': 76.188,\n",
       "  'loss': 56.85892194509506,\n",
       "  'acc': 80.59},\n",
       " {'epoch': 172,\n",
       "  'train_loss': 265.2687287032604,\n",
       "  'train_acc': 76.286,\n",
       "  'loss': 58.67953649163246,\n",
       "  'acc': 80.11},\n",
       " {'epoch': 173,\n",
       "  'train_loss': 264.9365519285202,\n",
       "  'train_acc': 76.468,\n",
       "  'loss': 57.220567494630814,\n",
       "  'acc': 80.5},\n",
       " {'epoch': 174,\n",
       "  'train_loss': 264.5711087882519,\n",
       "  'train_acc': 76.444,\n",
       "  'loss': 56.785913199186325,\n",
       "  'acc': 80.9},\n",
       " {'epoch': 175,\n",
       "  'train_loss': 264.4851759970188,\n",
       "  'train_acc': 76.406,\n",
       "  'loss': 58.3190053999424,\n",
       "  'acc': 80.21},\n",
       " {'epoch': 176,\n",
       "  'train_loss': 264.40929302573204,\n",
       "  'train_acc': 76.488,\n",
       "  'loss': 57.91888028383255,\n",
       "  'acc': 80.2},\n",
       " {'epoch': 177,\n",
       "  'train_loss': 266.0420820116997,\n",
       "  'train_acc': 76.31,\n",
       "  'loss': 58.73254242539406,\n",
       "  'acc': 79.95},\n",
       " {'epoch': 178,\n",
       "  'train_loss': 263.0709593594074,\n",
       "  'train_acc': 76.478,\n",
       "  'loss': 58.146648079156876,\n",
       "  'acc': 80.08},\n",
       " {'epoch': 179,\n",
       "  'train_loss': 263.3756232559681,\n",
       "  'train_acc': 76.668,\n",
       "  'loss': 57.74849423766136,\n",
       "  'acc': 80.37},\n",
       " {'epoch': 180,\n",
       "  'train_loss': 263.0649580359459,\n",
       "  'train_acc': 76.51,\n",
       "  'loss': 58.933698028326035,\n",
       "  'acc': 79.87},\n",
       " {'epoch': 181,\n",
       "  'train_loss': 264.9622946381569,\n",
       "  'train_acc': 76.246,\n",
       "  'loss': 57.654997766017914,\n",
       "  'acc': 80.33},\n",
       " {'epoch': 182,\n",
       "  'train_loss': 263.7770550251007,\n",
       "  'train_acc': 76.432,\n",
       "  'loss': 58.95629531145096,\n",
       "  'acc': 79.99},\n",
       " {'epoch': 183,\n",
       "  'train_loss': 262.6033642590046,\n",
       "  'train_acc': 76.544,\n",
       "  'loss': 58.0608246922493,\n",
       "  'acc': 80.34},\n",
       " {'epoch': 184,\n",
       "  'train_loss': 264.96285995841026,\n",
       "  'train_acc': 76.28,\n",
       "  'loss': 57.18045124411583,\n",
       "  'acc': 80.17},\n",
       " {'epoch': 185,\n",
       "  'train_loss': 262.1331040561199,\n",
       "  'train_acc': 76.586,\n",
       "  'loss': 57.73327887058258,\n",
       "  'acc': 80.3},\n",
       " {'epoch': 186,\n",
       "  'train_loss': 263.440776437521,\n",
       "  'train_acc': 76.458,\n",
       "  'loss': 58.18868711590767,\n",
       "  'acc': 80.0},\n",
       " {'epoch': 187,\n",
       "  'train_loss': 263.31840708851814,\n",
       "  'train_acc': 76.42,\n",
       "  'loss': 57.3547705411911,\n",
       "  'acc': 80.37},\n",
       " {'epoch': 188,\n",
       "  'train_loss': 263.3002748787403,\n",
       "  'train_acc': 76.466,\n",
       "  'loss': 57.64246216416359,\n",
       "  'acc': 80.28},\n",
       " {'epoch': 189,\n",
       "  'train_loss': 262.64828154444695,\n",
       "  'train_acc': 76.532,\n",
       "  'loss': 56.97037094831467,\n",
       "  'acc': 80.43},\n",
       " {'epoch': 190,\n",
       "  'train_loss': 261.6013467311859,\n",
       "  'train_acc': 76.668,\n",
       "  'loss': 57.576532274484634,\n",
       "  'acc': 80.18},\n",
       " {'epoch': 191,\n",
       "  'train_loss': 261.5025030374527,\n",
       "  'train_acc': 76.802,\n",
       "  'loss': 59.393750458955765,\n",
       "  'acc': 79.67},\n",
       " {'epoch': 192,\n",
       "  'train_loss': 263.37633925676346,\n",
       "  'train_acc': 76.318,\n",
       "  'loss': 56.513240694999695,\n",
       "  'acc': 80.45},\n",
       " {'epoch': 193,\n",
       "  'train_loss': 263.6852242052555,\n",
       "  'train_acc': 76.488,\n",
       "  'loss': 58.401859521865845,\n",
       "  'acc': 79.92},\n",
       " {'epoch': 194,\n",
       "  'train_loss': 264.3729575574398,\n",
       "  'train_acc': 76.18,\n",
       "  'loss': 60.175452172756195,\n",
       "  'acc': 79.65},\n",
       " {'epoch': 195,\n",
       "  'train_loss': 261.17351111769676,\n",
       "  'train_acc': 76.922,\n",
       "  'loss': 57.18999871611595,\n",
       "  'acc': 80.26},\n",
       " {'epoch': 196,\n",
       "  'train_loss': 262.8090097308159,\n",
       "  'train_acc': 76.654,\n",
       "  'loss': 57.8780132830143,\n",
       "  'acc': 80.21},\n",
       " {'epoch': 197,\n",
       "  'train_loss': 264.0828652083874,\n",
       "  'train_acc': 76.458,\n",
       "  'loss': 55.60094937682152,\n",
       "  'acc': 80.57},\n",
       " {'epoch': 198,\n",
       "  'train_loss': 260.65166372060776,\n",
       "  'train_acc': 76.692,\n",
       "  'loss': 57.95630580186844,\n",
       "  'acc': 79.81},\n",
       " {'epoch': 199,\n",
       "  'train_loss': 262.38046419620514,\n",
       "  'train_acc': 76.702,\n",
       "  'loss': 57.08645725250244,\n",
       "  'acc': 80.46}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynzOgqVMuICK",
    "outputId": "1404db2c-2218-4b94-bc9a-117c2a497a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(lr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cifar10_with_PDE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
